---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 30m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.16.5
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
        namespace: flux-system
  values:
    toolbox:
      enabled: true

    monitoring:
      enabled: true
      createPrometheusRules: true

    ingress:
      ingressClassName: internal
      dashboard:
        host:
          name: &host rook.${SECRET_DOMAIN}
          path: "/"
        tls:
          - hosts:
              - *host

    configOverride: |
      [global]
      bdev_enable_discard = true
      bdev_async_discard = true

    cephClusterSpec:
      network:
        provider: host

      resources:
        mgr:
          requests:
            cpu: "125m"
            memory: "512Mi"
          limits:
            memory: "2Gi"
        mon:
          requests:
            cpu: "49m"
            memory: "512Mi"
          limits:
            memory: "1Gi"
        osd:
          requests:
            cpu: "442m"
            memory: "1Gi"
          limits:
            memory: "6Gi"
        mgr-sidecar:
          requests:
            cpu: "49m"
            memory: "128Mi"
          limits:
            memory: "256Mi"
        crashcollector:
          requests:
            cpu: "15m"
            memory: "64Mi"
          limits:
            memory: "64Mi"
        logcollector:
          requests:
            cpu: "100m"
            memory: "100Mi"
          limits:
            memory: "1Gi"
        cleanup:
          requests:
            cpu: "250m"
            memory: "100Mi"
          limits:
            memory: "1Gi"

      crashCollector:
        disable: false

      dashboard:
        enabled: true
        urlPrefix: /
        ssl: false

      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          osdsPerDevice: "1"
        nodes:
          - name: "worker-01"
            devices:
              - name: "/dev/disk/by-id/nvme-AirDisk_512GB_SSD_NFQ0044006866P70GX"
          - name: "worker-02"
            devices:
              - name: "/dev/disk/by-id/nvme-AirDisk_512GB_SSD_NFQ0044007344P70GX"
          - name: "worker-03"
            devices:
              - name: "/dev/disk/by-id/nvme-AirDisk_512GB_SSD_NFQ0044010702P70GX"

      cleanupPolicy:
        # set to "yes-really-destroy-data" to enable the cleanup job to delete all data when deleting the cluster
        confirmation: ""

    cephBlockPoolsVolumeSnapshotClass:
      enabled: true

    # cephFileSystems:
    #   - name: ceph-filesystem
    #     spec:
    #       metadataServer:
    #         resources:
    #           requests:
    #             cpu: "35m"
    #             memory: "1Gi"
    #           limits:
    #             memory: "3Gi"

    # cephFileSystemVolumeSnapshotClass:
    #   enabled: true

    # cephObjectStores:
    #   - name: ceph-objectstore
    #     spec:
    #       metadataPool:
    #         failureDomain: host
    #         replicated:
    #           size: 3
    #       dataPool:
    #         failureDomain: host
    #         erasureCoded:
    #           dataChunks: 2
    #           codingChunks: 1
    #       preservePoolsOnDelete: true
    #       gateway:
    #         port: 80
    #         resources:
    #           requests:
    #             cpu: 100m
    #             memory: 256Mi
    #           limits:
    #             memory: 2Gi
    #         instances: 1
    #       healthCheck:
    #         bucket:
    #           interval: 60s
    #     storageClass:
    #       enabled: true
    #       name: ceph-bucket
    #       reclaimPolicy: Delete
    #       parameters:
    #         region: eu-central-1
