{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home Lab","text":"<p>Welcome to the documentation for the cluster, a Kubernetes home lab built on Talos Linux and managed entirely through GitOps with ArgoCD. This repository defines the complete infrastructure-as-code for a mixed-architecture cluster running on Raspberry Pi, Lenovo ThinkPad, and Acemagician mini-PC hardware.</p> <p>Quick Start</p> <p>New here? Head to the Getting Started guide to understand the repository layout and dive into the architecture.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart LR\n    Internet((Internet))\n    CF[Cloudflare]\n    Tunnel[cloudflared\\nTunnel]\n    Nginx[nginx\\nReverse Proxy]\n    EE[Envoy External\\n192.168.0.239]\n    EI[Envoy Internal\\n192.168.0.238]\n    Apps[Applications]\n    TS[Tailscale\\nVPN]\n    User((User))\n\n    Internet --&gt;|\"*.example.com\"| CF\n    CF --&gt;|Proxied traffic| Tunnel\n    Tunnel --&gt; Nginx\n    Nginx --&gt; EE\n    EE --&gt; Apps\n\n    User --&gt;|VPN| TS\n    TS --&gt; EI\n    EI --&gt; Apps</code></pre>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"Section Description Getting Started Repository overview, prerequisites, and architecture Infrastructure Hardware, Talos Linux, cluster bootstrap, node management Networking Cilium CNI, Envoy Gateway, DNS, Cloudflare Tunnel, Tailscale GitOps ArgoCD setup, ApplicationSets, sync policies Storage Rook Ceph, OpenEBS, backup and restore Security Authelia, LLDAP, External Secrets, SOPS, cert-manager Monitoring Prometheus, Grafana, Loki, Fluent Bit Applications Media stack, home automation, self-hosted apps, databases Operations Justfile recipes, Talos commands, troubleshooting, upgrades CI/CD GitHub Actions, Docker builds, Renovate Development App template patterns, adding new apps Reference IP allocation table, full app catalog"},{"location":"#hardware-summary","title":"Hardware Summary","text":""},{"location":"#compute","title":"Compute","text":"Device Quantity Role Architecture Raspberry Pi 4 4 Control plane / workers ARM64 Lenovo ThinkPad T440p 2 Workers AMD64 Acemagician AM06 3 Workers (NVMe / Ceph) AMD64 Raspberry Pi 3B+ 1 Worker ARM64 Raspberry Pi 2B+ 4 Auxiliary / monitoring ARMv7"},{"location":"#storage","title":"Storage","text":"Device Details Synology NAS 4-bay, 8 TB total Boot drives 128 GB SSD per node Ceph OSD drives 512 GB NVMe x3 (Acemagician nodes)"},{"location":"#network-power","title":"Network &amp; Power","text":"Device Details TP-Link 24-port PoE switch Core switch, powers Pi nodes via PoE NanoPi R5C Network appliance / router helper Ubiquiti U7-Pro Primary Wi-Fi access point Ubiquiti U6-Lite Secondary Wi-Fi access point Eaton 500VA UPS Battery backup for core infrastructure"},{"location":"#other","title":"Other","text":"Device Details BambuLab A1 Combo 3D printer for cases, mounts, and brackets"},{"location":"#key-technologies","title":"Key Technologies","text":"Layer Technology Purpose Operating System Talos Linux v1.12.4 Immutable, API-driven Kubernetes OS GitOps ArgoCD Declarative continuous delivery CNI Cilium eBPF networking with L2 announcements, DSR, Maglev Ingress Envoy Gateway Two-gateway architecture (external, internal) DNS Cloudflare + external-dns Automated DNS record management Tunnel cloudflared Secure external access without port forwarding VPN Tailscale Remote access to internal services Storage Rook Ceph + OpenEBS Distributed and local persistent storage Secrets SOPS + age + 1Password Connect Encrypted secrets in Git, synced via External Secrets Auth Authelia + LLDAP SSO and lightweight LDAP directory Monitoring Prometheus + Grafana + Loki Metrics, dashboards, and log aggregation Domain example.com Managed via Cloudflare"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>home-ops/\n\u251c\u2500\u2500 pitower/\n\u2502   \u251c\u2500\u2500 kubernetes/\n\u2502   \u2502   \u251c\u2500\u2500 apps/           # Application manifests by category\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ai/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 banking/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cert-manager/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cloudnative-pg/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 home-automation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kube-system/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 media/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 monitoring/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 networking/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 openebs/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 rook-ceph/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 security/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 selfhosted/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 system/\n\u2502   \u2502   \u251c\u2500\u2500 argocd/         # ArgoCD app and ApplicationSet definitions\n\u2502   \u2502   \u2514\u2500\u2500 bootstrap/      # Cluster bootstrap resources\n\u2502   \u2514\u2500\u2500 talos/\n\u2502       \u251c\u2500\u2500 clusterconfig/  # Generated Talos machine configs\n\u2502       \u251c\u2500\u2500 extensions/     # Talos system extension definitions\n\u2502       \u251c\u2500\u2500 patches/        # Machine config patches (general + per-node)\n\u2502       \u2514\u2500\u2500 justfile        # Task runner recipes for cluster operations\n\u2514\u2500\u2500 docs/                   # This documentation site\n</code></pre>"},{"location":"applications/","title":"Applications","text":"<p>The cluster runs a wide range of self-hosted applications, organized by category. Most applications are deployed using the bjw-s app-template Helm chart (v4.6.2) and connect to one of two Envoy Gateways via Gateway API <code>HTTPRoute</code> resources.</p>"},{"location":"applications/#application-categories","title":"Application Categories","text":"Category Apps Namespace Description Media Stack 7 <code>media</code> Jellyfin media server, *arr apps for library management, and download clients Home Automation 5 <code>home-automation</code> Home Assistant, Zigbee/Matter/Thread device management, MQTT broker Self-Hosted 12 <code>selfhosted</code> Dashboards, productivity tools, DNS utilities, and more Databases -- <code>cloudnative-pg</code> CloudNative-PG PostgreSQL operator, clusters, and credential management AI 1 <code>ai</code> kagent -- Kubernetes-native AI agent framework Banking 2 <code>banking</code> Firefly III personal finance manager and data importer"},{"location":"applications/#gateway-routing-pattern","title":"Gateway Routing Pattern","text":"<p>All applications expose their web interfaces through <code>HTTPRoute</code> resources that attach to one of two Envoy Gateways:</p> Gateway IP Target Domain Use Case <code>envoy-external</code> 192.168.0.239 <code>external.example.com</code> Public services via Cloudflare tunnel <code>envoy-internal</code> 192.168.0.238 <code>internal.example.com</code> LAN / Tailscale VPN only Typical HTTPRoute attachment<pre><code>route:\n  app:\n    enabled: true\n    hostnames:\n      - app-name.example.com\n    parentRefs:\n      - name: envoy-external  # or envoy-internal\n        namespace: networking\n        sectionName: https\n</code></pre> <p>App Template Helm Chart</p> <p>The bjw-s app-template chart provides a standardized way to define controllers, services, routes, and persistence. Each application's <code>values.yaml</code> follows the same structure, making it straightforward to add new services. See Development &gt; App Template for the full pattern.</p>"},{"location":"applications/#helm-chart-versions","title":"Helm Chart Versions","text":"Chart Version Source bjw-s app-template v4.6.2 <code>ghcr.io/bjw-s-labs/helm</code> CloudNative-PG Latest <code>cloudnative-pg.io</code> kagent 0.6.19 <code>kagent.dev</code>"},{"location":"applications/#common-patterns","title":"Common Patterns","text":""},{"location":"applications/#stakater-reloader","title":"Stakater Reloader","text":"<p>Most controllers carry the annotation <code>reloader.stakater.com/auto: \"true\"</code>, which triggers automatic pod restarts when referenced ConfigMaps or Secrets change.</p>"},{"location":"applications/#security-contexts","title":"Security Contexts","text":"<p>Applications follow a least-privilege model where possible:</p> <pre><code>securityContext:\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n      - ALL\n</code></pre>"},{"location":"applications/#nfs-media-storage","title":"NFS Media Storage","text":"<p>Media applications mount the Synology NAS via NFS:</p> <pre><code>persistence:\n  media:\n    type: nfs\n    server: data\n    path: /volume1/media\n    globalMounts:\n      - path: /data/nas-media\n</code></pre>"},{"location":"applications/#external-secrets","title":"External Secrets","text":"<p>Applications that need database credentials or API keys use <code>ExternalSecret</code> resources backed by the <code>cnpg-secrets</code> ClusterSecretStore or 1Password Connect.</p>"},{"location":"applications/databases/","title":"Databases","text":"<p>The cluster uses CloudNative-PG to run PostgreSQL databases natively in Kubernetes. The operator manages the full lifecycle of PostgreSQL clusters -- provisioning, high availability, failover, backups, and monitoring.</p>"},{"location":"applications/databases/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    subgraph CNPG[\"cloudnative-pg namespace\"]\n        OP[CloudNative-PG Operator]\n        CSS[ClusterSecretStore&lt;br/&gt;cnpg-secrets]\n        ESA[external-secrets-pg&lt;br/&gt;ServiceAccount]\n\n        subgraph Clusters[\"PostgreSQL Clusters\"]\n            PG[postgres&lt;br/&gt;General purpose]\n            TD[tandoor&lt;br/&gt;Recipes]\n            AB[autobrr&lt;br/&gt;IRC monitoring]\n            BS[backstage&lt;br/&gt;Developer portal]\n            HH[house-hunter&lt;br/&gt;Property search]\n            LL[lldap&lt;br/&gt;LDAP directory]\n            FF[firefly&lt;br/&gt;Finance manager]\n            MF[miniflux&lt;br/&gt;RSS reader]\n        end\n    end\n\n    subgraph Apps[\"Application Namespaces\"]\n        A1[Tandoor&lt;br/&gt;selfhosted]\n        A2[Autobrr&lt;br/&gt;media]\n        A3[House Hunter&lt;br/&gt;selfhosted]\n        A4[LLDAP&lt;br/&gt;security]\n        A5[Firefly III&lt;br/&gt;banking]\n        A6[Miniflux&lt;br/&gt;selfhosted]\n    end\n\n    OP --&gt;|Manages| Clusters\n    CSS --&gt;|Reads credentials| Clusters\n    ESA --&gt;|Authenticates| CSS\n\n    TD -.-&gt;|ExternalSecret| A1\n    AB -.-&gt;|ExternalSecret| A2\n    HH -.-&gt;|ExternalSecret| A3\n    LL -.-&gt;|ExternalSecret| A4\n    FF -.-&gt;|ExternalSecret| A5\n    MF -.-&gt;|ExternalSecret| A6\n\n    classDef operator fill:#7c3aed,stroke:#5b21b6,color:#fff\n    classDef cluster fill:#00b894,stroke:#00a381,color:#fff\n    class OP operator\n    class PG,TD,AB,BS,HH,LL,FF,MF cluster</code></pre>"},{"location":"applications/databases/#operator","title":"Operator","text":"<p>The CloudNative-PG operator is deployed via Helm chart in the <code>cloudnative-pg</code> namespace:</p> Operator values<pre><code>crds:\n  create: true\nmonitoring:\n  podMonitorEnabled: true\n  grafanaDashboard:\n    create: true\n</code></pre> <p>The operator watches for <code>Cluster</code> CRDs and manages the PostgreSQL pods, replication, and failover automatically.</p>"},{"location":"applications/databases/#postgresql-clusters","title":"PostgreSQL Clusters","text":"<p>All clusters run PostgreSQL 16 (<code>ghcr.io/cloudnative-pg/postgresql:16</code>) with 2 instances each for high availability. Storage is backed by <code>openebs-hostpath</code> for local NVMe performance.</p> Cluster Database Owner Storage Consumers <code>postgres</code> (default) (default) 1 Gi General purpose, Authelia <code>tandoor</code> <code>tandoor</code> <code>tandoor</code> 1 Gi Tandoor Recipes <code>autobrr</code> <code>tandoor</code>* <code>tandoor</code>* 1 Gi Autobrr <code>backstage</code> (default) (default) 1 Gi Backstage (superuser enabled) <code>house-hunter</code> <code>house_hunter</code> <code>house_hunter</code> 1 Gi House Hunter <code>lldap</code> <code>lldap</code> <code>lldap</code> 100 Mi LLDAP <code>firefly</code> <code>firefly</code> <code>firefly</code> 100 Mi Firefly III <code>miniflux</code> <code>miniflux</code> <code>miniflux</code> 100 Mi Miniflux <p>Cluster Configuration</p> <p>The <code>postgres</code> cluster has custom parameters for higher connection limits (<code>max_connections: 600</code>) and larger shared buffers (<code>shared_buffers: 512MB</code>) since it serves as the general-purpose database for multiple consumers.</p>"},{"location":"applications/databases/#example-cluster-definition","title":"Example Cluster Definition","text":"cluster.yaml (miniflux example)<pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: miniflux\n  namespace: cloudnative-pg\nspec:\n  instances: 2\n  imageName: ghcr.io/cloudnative-pg/postgresql:16\n  enableSuperuserAccess: true\n  bootstrap:\n    initdb:\n      database: miniflux\n      owner: miniflux\n  storage:\n    size: 100Mi\n    storageClass: openebs-hostpath\n  monitoring:\n    enablePodMonitor: true\n</code></pre>"},{"location":"applications/databases/#clustersecretstore","title":"ClusterSecretStore","text":"<p>The <code>cnpg-secrets</code> ClusterSecretStore allows applications in any namespace to access PostgreSQL credentials using <code>ExternalSecret</code> resources:</p> clustersecretstore.yaml<pre><code>apiVersion: external-secrets.io/v1\nkind: ClusterSecretStore\nmetadata:\n  name: cnpg-secrets\nspec:\n  provider:\n    kubernetes:\n      remoteNamespace: cloudnative-pg\n      server:\n        caProvider:\n          type: \"ConfigMap\"\n          name: \"kube-root-ca.crt\"\n          namespace: cloudnative-pg\n          key: \"ca.crt\"\n      auth:\n        serviceAccount:\n          name: external-secrets-pg\n          namespace: cloudnative-pg\n</code></pre>"},{"location":"applications/databases/#how-it-works","title":"How It Works","text":"<ol> <li>CloudNative-PG creates a Secret for each cluster (e.g., <code>miniflux-app</code>) containing the connection URI, username, password, and host</li> <li>The <code>cnpg-secrets</code> ClusterSecretStore reads these Secrets via a dedicated ServiceAccount (<code>external-secrets-pg</code>)</li> <li>Applications create <code>ExternalSecret</code> resources that reference <code>cnpg-secrets</code> and project the database credentials into their namespace</li> <li>The application reads credentials from the projected Secret (typically via <code>envFrom</code>)</li> </ol> <pre><code>sequenceDiagram\n    participant CNPG as CloudNative-PG\n    participant Secret as PG Secret&lt;br/&gt;(cloudnative-pg ns)\n    participant CSS as ClusterSecretStore&lt;br/&gt;cnpg-secrets\n    participant ES as ExternalSecret&lt;br/&gt;(app namespace)\n    participant App as Application Pod\n\n    CNPG-&gt;&gt;Secret: Creates credentials\n    ES-&gt;&gt;CSS: Requests credentials\n    CSS-&gt;&gt;Secret: Reads via ServiceAccount\n    CSS--&gt;&gt;ES: Returns credentials\n    ES-&gt;&gt;App: Projects as Secret\n    App-&gt;&gt;App: Reads envFrom</code></pre>"},{"location":"applications/databases/#application-externalsecret-pattern","title":"Application ExternalSecret Pattern","text":"<p>Each application that needs database access creates an <code>ExternalSecret</code> like this:</p> Example ExternalSecret for miniflux<pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: miniflux-db-secret\n  namespace: selfhosted\nspec:\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: cnpg-secrets\n  target:\n    name: miniflux-db-secret\n  data:\n    - secretKey: DATABASE_URL\n      remoteRef:\n        key: miniflux-app\n        property: uri\n</code></pre>"},{"location":"applications/databases/#rbac","title":"RBAC","text":"<p>A dedicated ServiceAccount and RBAC configuration grants the External Secrets Operator read access to Secrets in the <code>cloudnative-pg</code> namespace:</p> rbac.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-secrets-pg\n  namespace: cloudnative-pg\n</code></pre>"},{"location":"applications/databases/#monitoring","title":"Monitoring","text":"<p>All PostgreSQL clusters have <code>enablePodMonitor: true</code>, which creates Prometheus PodMonitor resources for metrics collection. The operator also creates a Grafana dashboard for monitoring cluster health, replication lag, and query performance.</p>"},{"location":"applications/databases/#storage","title":"Storage","text":"<p>All clusters use <code>openebs-hostpath</code> as the storage class, which provisions local NVMe-backed volumes on the Acemagician nodes. This provides the lowest latency for database I/O while relying on PostgreSQL's built-in streaming replication (2 instances) for data durability.</p> <p>Backup Strategy</p> <p>With <code>openebs-hostpath</code> storage and no external backup target configured, data durability relies on PostgreSQL's 2-instance replication. Consider adding a backup schedule with an S3-compatible target for disaster recovery.</p>"},{"location":"applications/home-automation/","title":"Home Automation","text":"<p>The home automation stack runs in the <code>home-automation</code> namespace and provides smart home control, Zigbee device management, Matter/Thread protocol support, and MQTT messaging. All components are pinned to <code>worker-04</code>, which has the required USB hardware attached.</p>"},{"location":"applications/home-automation/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    subgraph Devices\n        ZD[Zigbee Devices]\n        MD[Matter Devices]\n        TD[Thread Devices]\n    end\n\n    subgraph Coordinators[\"Hardware Coordinators\"]\n        SLZB[SLZB-06&lt;br/&gt;Zigbee Coordinator&lt;br/&gt;tcp://slzb-06:6638]\n    end\n\n    subgraph Cluster[\"home-automation namespace\"]\n        Z2M[Zigbee2MQTT&lt;br/&gt;Port 8080]\n        MOS[Mosquitto&lt;br/&gt;MQTT Broker&lt;br/&gt;Port 1883]\n        HA[Home Assistant&lt;br/&gt;Port 8123]\n        MS[Matter Server&lt;br/&gt;Port 5580]\n        OTBR[OTBR&lt;br/&gt;Thread Border Router&lt;br/&gt;Port 80]\n    end\n\n    ZD &lt;--&gt;|Zigbee protocol| SLZB\n    SLZB &lt;--&gt;|TCP| Z2M\n    Z2M --&gt;|Publish device state| MOS\n    MOS --&gt;|Subscribe to updates| HA\n    HA &lt;--&gt;|WebSocket| MS\n    MD &lt;--&gt;|Matter protocol| MS\n    TD &lt;--&gt;|Thread protocol| OTBR\n    OTBR --&gt; HA\n\n    classDef broker fill:#f59e0b,stroke:#d97706,color:#000\n    classDef hub fill:#7c3aed,stroke:#5b21b6,color:#fff\n    class MOS broker\n    class HA hub</code></pre>"},{"location":"applications/home-automation/#application-summary","title":"Application Summary","text":"App Purpose Gateway URL LoadBalancer IP Home Assistant Smart home hub <code>envoy-external</code> <code>ha.example.com</code> <code>192.168.0.227</code> Zigbee2MQTT Zigbee device bridge <code>envoy-internal</code> <code>zigbee.example.com</code> -- Mosquitto MQTT broker LoadBalancer <code>mosquitto.example.com</code> <code>192.168.0.226</code> Matter Server Matter protocol server <code>envoy-internal</code> <code>matter.example.com</code> <code>192.168.0.228</code> OTBR Thread border router <code>envoy-internal</code> <code>otbr.example.com</code> <code>192.168.0.230</code>"},{"location":"applications/home-automation/#node-pinning","title":"Node Pinning","text":"<p>All home automation components are pinned to <code>worker-04</code> (an Acemagician AM06 node) using <code>nodeSelector</code>:</p> <pre><code>nodeSelector:\n  kubernetes.io/hostname: worker-04\n</code></pre> <p>This is required because:</p> <ul> <li>The SLZB-06 Zigbee coordinator is accessible via TCP from this node's network segment</li> <li>Matter Server requires <code>hostNetwork: true</code> for mDNS discovery and must be co-located with Home Assistant</li> <li>OTBR needs access to <code>/dev</code> for Thread radio hardware</li> </ul>"},{"location":"applications/home-automation/#protocol-stack","title":"Protocol Stack","text":"Protocol Coordinator Bridge Integration Zigbee SLZB-06 (network) Zigbee2MQTT MQTT discovery in Home Assistant Matter -- Matter Server WebSocket to Home Assistant Thread OTBR OTBR REST API Home Assistant Thread integration MQTT -- Mosquitto Native Home Assistant integration <p>SLZB-06 Coordinator</p> <p>The Zigbee coordinator is an SLZB-06 network-attached coordinator, accessed over TCP (<code>tcp://slzb-06:6638</code>) rather than USB serial. This removes the need for USB passthrough into the container.</p>"},{"location":"applications/home-automation/home-assistant/","title":"Home Assistant","text":"<p>Home Assistant is the central smart home hub, integrating Zigbee devices (via Zigbee2MQTT and MQTT), Matter devices (via Matter Server), and Thread devices (via OTBR).</p>"},{"location":"applications/home-automation/home-assistant/#deployment-details","title":"Deployment Details","text":"Setting Value Image <code>ghcr.io/home-operations/home-assistant:2026.1.3</code> Namespace <code>home-automation</code> Gateway <code>envoy-external</code> (main UI), <code>envoy-internal</code> (code-server) URLs <code>ha.example.com</code> (UI), <code>ha-code.example.com</code> (code editor) Node <code>worker-04</code> (pinned) LoadBalancer IP <code>192.168.0.227</code>"},{"location":"applications/home-automation/home-assistant/#containers","title":"Containers","text":"<p>Home Assistant runs as a multi-container pod with two containers:</p>"},{"location":"applications/home-automation/home-assistant/#main-application","title":"Main Application","text":"<p>The core Home Assistant container provides the web UI and automation engine:</p> <pre><code>containers:\n  app:\n    image:\n      repository: ghcr.io/home-operations/home-assistant\n      tag: 2026.1.3\n    resources:\n      requests:\n        cpu: 47m\n        memory: 256Mi\n      limits:\n        memory: 2Gi\n    securityContext:\n      privileged: true\n</code></pre> <p>Privileged Mode</p> <p>Home Assistant runs in privileged mode to support device discovery, mDNS, and direct hardware access. This is a known trade-off for full home automation functionality.</p>"},{"location":"applications/home-automation/home-assistant/#code-server-sidecar","title":"Code Server Sidecar","text":"<p>A code-server sidecar provides a VS Code editor for editing Home Assistant's YAML configuration files directly in the browser:</p> <pre><code>containers:\n  code-server:\n    image:\n      repository: ghcr.io/coder/code-server\n      tag: \"4.108.2\"\n    args:\n      - --auth\n      - none\n      - --disable-telemetry\n      - --disable-update-check\n      - --user-data-dir\n      - /config/.code-server\n      - --extensions-dir\n      - /config/.code-server\n      - --port\n      - \"12321\"\n      - /config\n    env:\n      HASS_SERVER: http://localhost:8123\n</code></pre>"},{"location":"applications/home-automation/home-assistant/#routing","title":"Routing","text":"<p>Home Assistant has two HTTPRoutes:</p> Route Hostname Gateway Port Purpose <code>app</code> <code>ha.example.com</code> <code>envoy-external</code> 8123 Main UI (public access) <code>code</code> <code>ha-code.example.com</code> <code>envoy-internal</code> 12321 Code editor (LAN only) Route configuration<pre><code>route:\n  app:\n    hostnames:\n      - ha.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\n    rules:\n      - backendRefs:\n          - identifier: app\n            port: 8123\n  code:\n    hostnames:\n      - ha-code.example.com\n    parentRefs:\n      - name: envoy-internal\n        namespace: networking\n        sectionName: https\n    rules:\n      - backendRefs:\n          - identifier: app\n            port: 12321\n</code></pre>"},{"location":"applications/home-automation/home-assistant/#service","title":"Service","text":"<p>The service is a LoadBalancer type with a dedicated Cilium LBIPAM IP, enabling direct LAN access and device discovery:</p> <pre><code>service:\n  app:\n    controller: home-assistant\n    type: LoadBalancer\n    annotations:\n      lbipam.cilium.io/ips: 192.168.0.227\n    ports:\n      http:\n        port: 8123\n      code-server:\n        port: 12321\n</code></pre>"},{"location":"applications/home-automation/home-assistant/#storage","title":"Storage","text":"Mount Source Purpose <code>/config</code> PVC <code>home-assistant-config</code> All HA configuration, automations, database <code>/tmp</code> <code>emptyDir</code> (subpath <code>hass-tmp</code>) Temporary files <p>Both the main container and code-server sidecar share the <code>/config</code> PVC, allowing the code editor to modify configuration files that Home Assistant reads.</p>"},{"location":"applications/home-automation/home-assistant/#secrets","title":"Secrets","text":"<p>Sensitive configuration values (API keys, tokens) are injected from the <code>home-assistant-secret</code> ExternalSecret:</p> <pre><code>envFrom:\n  - secretRef:\n      name: home-assistant-secret\n</code></pre>"},{"location":"applications/home-automation/home-assistant/#integration-points","title":"Integration Points","text":"Integration Protocol Endpoint Zigbee2MQTT MQTT via Mosquitto <code>mqtt://mosquitto.home-automation.svc.cluster.local:1883</code> Matter Server WebSocket <code>ws://localhost:5580</code> (co-located on same node) OTBR REST API <code>http://otbr.home-automation.svc.cluster.local:8081</code>"},{"location":"applications/home-automation/mosquitto/","title":"Mosquitto","text":"<p>Eclipse Mosquitto is the MQTT broker that serves as the message bus between Zigbee2MQTT and Home Assistant. It handles device state updates, command messages, and Home Assistant MQTT discovery.</p>"},{"location":"applications/home-automation/mosquitto/#deployment-details","title":"Deployment Details","text":"Setting Value Image <code>docker.io/library/eclipse-mosquitto:2.0.22</code> Namespace <code>home-automation</code> Port 1883 (MQTT) LoadBalancer IP <code>192.168.0.226</code> DNS <code>mosquitto.example.com</code> <p>No HTTPRoute</p> <p>Mosquitto does not use an Envoy Gateway HTTPRoute because MQTT is a TCP protocol, not HTTP. Instead, it is exposed directly as a <code>LoadBalancer</code> service with a dedicated Cilium LBIPAM IP address.</p>"},{"location":"applications/home-automation/mosquitto/#service-configuration","title":"Service Configuration","text":"LoadBalancer service<pre><code>service:\n  app:\n    controller: mosquitto\n    type: LoadBalancer\n    annotations:\n      external-dns.alpha.kubernetes.io/hostname: mosquitto.example.com\n      lbipam.cilium.io/ips: 192.168.0.226\n    ports:\n      http:\n        port: 1883\n</code></pre> <p>The <code>external-dns</code> annotation creates a DNS record pointing <code>mosquitto.example.com</code> to the LoadBalancer IP, allowing external MQTT clients (such as IoT devices on the LAN) to connect.</p>"},{"location":"applications/home-automation/mosquitto/#mosquitto-configuration","title":"Mosquitto Configuration","text":"<p>Mosquitto is configured via a ConfigMap mounted into the container:</p> Config mount<pre><code>persistence:\n  config-file:\n    type: configMap\n    name: mosquitto-configmap\n    advancedMounts:\n      mosquitto:\n        app:\n          - path: /mosquitto/config/mosquitto.conf\n            subPath: mosquitto.conf\n</code></pre>"},{"location":"applications/home-automation/mosquitto/#listener-configuration","title":"Listener Configuration","text":"<p>Mosquitto listens on port 1883 for unencrypted MQTT connections. Since all communication happens within the cluster or on the LAN, TLS termination is not required at the broker level.</p>"},{"location":"applications/home-automation/mosquitto/#authentication","title":"Authentication","text":"<p>Authentication Setup</p> <p>Mosquitto authentication is configured through the <code>mosquitto.conf</code> file in the ConfigMap. Consult the ConfigMap resource for the current authentication settings.</p>"},{"location":"applications/home-automation/mosquitto/#storage","title":"Storage","text":"Mount Source Purpose <code>/data</code> PVC <code>mosquitto</code> Persistent message store and retained messages <code>/mosquitto/config/mosquitto.conf</code> ConfigMap <code>mosquitto-configmap</code> Broker configuration"},{"location":"applications/home-automation/mosquitto/#security-context","title":"Security Context","text":"<p>Mosquitto runs with a fully hardened security context:</p> <pre><code>pod:\n  securityContext:\n    runAsUser: 568\n    runAsGroup: 568\n    runAsNonRoot: true\n    fsGroup: 568\n    fsGroupChangePolicy: OnRootMismatch\n\ncontainers:\n  app:\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop: [\"ALL\"]\n</code></pre>"},{"location":"applications/home-automation/mosquitto/#message-flow","title":"Message Flow","text":"<pre><code>flowchart LR\n    Z2M[Zigbee2MQTT] --&gt;|Publish device state| MOS[Mosquitto&lt;br/&gt;1883]\n    MOS --&gt;|Subscribe to updates| HA[Home Assistant]\n    HA --&gt;|Publish commands| MOS\n    MOS --&gt;|Forward commands| Z2M\n    IOT[Other MQTT Devices] &lt;--&gt;|Publish / Subscribe| MOS</code></pre>"},{"location":"applications/home-automation/mosquitto/#connected-clients","title":"Connected Clients","text":"Client Direction Topics Zigbee2MQTT Publish + Subscribe <code>zigbee2mqtt/#</code>, <code>homeassistant/#</code> Home Assistant Publish + Subscribe <code>homeassistant/#</code>, <code>zigbee2mqtt/+/set</code> LAN IoT devices Publish + Subscribe Various custom topics"},{"location":"applications/home-automation/zigbee2mqtt/","title":"Zigbee2MQTT","text":"<p>Zigbee2MQTT bridges Zigbee devices to MQTT, allowing Home Assistant and other consumers to interact with Zigbee sensors, switches, and lights through the Mosquitto MQTT broker.</p>"},{"location":"applications/home-automation/zigbee2mqtt/#deployment-details","title":"Deployment Details","text":"Setting Value Image <code>ghcr.io/koenkk/zigbee2mqtt:2.8.0</code> Namespace <code>home-automation</code> Gateway <code>envoy-internal</code> URL <code>zigbee.example.com</code> Node <code>worker-04</code> (pinned)"},{"location":"applications/home-automation/zigbee2mqtt/#zigbee-coordinator","title":"Zigbee Coordinator","text":"<p>Zigbee2MQTT connects to an SLZB-06 network-attached Zigbee coordinator over TCP:</p> <pre><code>env:\n  ZIGBEE2MQTT_CONFIG_SERIAL_PORT: tcp://slzb-06:6638\n  ZIGBEE2MQTT_CONFIG_SERIAL_ADAPTER: zstack\n  ZIGBEE2MQTT_CONFIG_SERIAL_BAUDRATE: 115200\n</code></pre> <p>Network vs USB</p> <p>The SLZB-06 is a network-based coordinator, so there is no need for USB device passthrough (<code>/dev/ttyACM0</code>). This simplifies the Kubernetes deployment and removes host device dependencies. The commented-out USB hostPath mount in <code>values.yaml</code> is kept for reference.</p>"},{"location":"applications/home-automation/zigbee2mqtt/#mqtt-integration","title":"MQTT Integration","text":"<p>Zigbee2MQTT publishes device state to and receives commands from the Mosquitto MQTT broker:</p> MQTT configuration<pre><code>env:\n  ZIGBEE2MQTT_CONFIG_MQTT_SERVER: mqtt://mosquitto.home-automation.svc.cluster.local\n  ZIGBEE2MQTT_CONFIG_MQTT_VERSION: 5\n  ZIGBEE2MQTT_CONFIG_MQTT_KEEPALIVE: 60\n  ZIGBEE2MQTT_CONFIG_MQTT_REJECT_UNAUTHORIZED: false\n  ZIGBEE2MQTT_CONFIG_MQTT_INCLUDE_DEVICE_INFORMATION: true\n</code></pre>"},{"location":"applications/home-automation/zigbee2mqtt/#home-assistant-discovery","title":"Home Assistant Discovery","text":"<p>Zigbee2MQTT is configured to publish Home Assistant MQTT discovery messages, enabling automatic device registration:</p> <pre><code>env:\n  ZIGBEE2MQTT_CONFIG_HOMEASSISTANT_ENABLED: true\n  ZIGBEE2MQTT_CONFIG_HOMEASSISTANT_DISCOVERY_TOPIC: homeassistant\n  ZIGBEE2MQTT_CONFIG_HOMEASSISTANT_STATUS_TOPIC: homeassistant/status\n  ZIGBEE2MQTT_CONFIG_HOMEASSISTANT_EXPERIMENTAL_EVENT_ENTITIES: true\n</code></pre>"},{"location":"applications/home-automation/zigbee2mqtt/#containers","title":"Containers","text":"<p>Like Home Assistant, Zigbee2MQTT runs as a multi-container pod with a code-server sidecar:</p>"},{"location":"applications/home-automation/zigbee2mqtt/#main-application","title":"Main Application","text":"<pre><code>containers:\n  app:\n    image:\n      repository: ghcr.io/koenkk/zigbee2mqtt\n      tag: 2.8.0\n    securityContext:\n      privileged: true\n</code></pre>"},{"location":"applications/home-automation/zigbee2mqtt/#code-server-sidecar","title":"Code Server Sidecar","text":"<p>The code-server sidecar at port 12321 provides direct access to Zigbee2MQTT's data files:</p> <pre><code>containers:\n  code-server:\n    image:\n      repository: ghcr.io/coder/code-server\n      tag: 4.108.2\n    args:\n      - --auth\n      - none\n      - --port\n      - \"12321\"\n      - /data\n</code></pre>"},{"location":"applications/home-automation/zigbee2mqtt/#routing","title":"Routing","text":"<p>The HTTPRoute uses path-based routing to split traffic between the Zigbee2MQTT frontend and code-server:</p> Path-based routing<pre><code>route:\n  app:\n    hostnames:\n      - zigbee.example.com\n    parentRefs:\n      - name: envoy-internal\n        namespace: networking\n        sectionName: https\n    rules:\n      - matches:\n          - path:\n              type: PathPrefix\n              value: /code\n        backendRefs:\n          - identifier: app\n            port: 12321\n        filters:\n          - type: URLRewrite\n            urlRewrite:\n              path:\n                type: ReplacePrefixMatch\n                replacePrefixMatch: /\n      - backendRefs:\n          - identifier: app\n            port: 8080\n</code></pre> Path Backend Description <code>/code/*</code> code-server (12321) VS Code editor (path rewritten to <code>/</code>) <code>/*</code> Zigbee2MQTT (8080) Zigbee2MQTT web frontend"},{"location":"applications/home-automation/zigbee2mqtt/#storage","title":"Storage","text":"Mount Source Purpose <code>/data</code> PVC <code>zigbee2mqtt</code> Configuration, device database, coordinator backup <p>Both the main container and code-server share the <code>/data</code> PVC.</p>"},{"location":"applications/home-automation/zigbee2mqtt/#key-configuration","title":"Key Configuration","text":"Setting Value Purpose <code>TZ</code> <code>Europe/Zurich</code> Timezone for timestamps <code>PERMIT_JOIN</code> <code>true</code> Allow new devices to join the network <code>FRONTEND_ENABLED</code> <code>true</code> Enable the web-based dashboard <code>FRONTEND_PORT</code> <code>8080</code> Port for the web UI <code>FRONTEND_URL</code> <code>https://zigbee.example.com</code> External URL for WebSocket connections <code>LOG_LEVEL</code> <code>info</code> Logging verbosity <code>DEVICE_OPTIONS_RETAIN</code> <code>true</code> Retain last device state in MQTT"},{"location":"applications/media-stack/","title":"Media Stack","text":"<p>The media stack provides automated media acquisition, organization, and playback. It consists of seven applications working together in the <code>media</code> namespace.</p>"},{"location":"applications/media-stack/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Indexers\n        PR[Prowlarr&lt;br/&gt;Indexer Manager]\n    end\n\n    subgraph Management\n        SO[Sonarr&lt;br/&gt;TV Shows]\n        RA[Radarr&lt;br/&gt;Movies]\n        AB[Autobrr&lt;br/&gt;IRC Announces]\n    end\n\n    subgraph Download[\"Download Clients\"]\n        QB[qBittorrent&lt;br/&gt;Torrents]\n        SAB[SABnzbd&lt;br/&gt;Usenet]\n    end\n\n    subgraph Storage\n        NAS[(Synology NAS&lt;br/&gt;/volume1/media)]\n    end\n\n    subgraph Playback\n        JF[Jellyfin&lt;br/&gt;Media Server]\n    end\n\n    PR --&gt;|Manages indexers for| SO\n    PR --&gt;|Manages indexers for| RA\n    AB --&gt;|Grabs from IRC| QB\n    SO --&gt;|Sends to| QB\n    SO --&gt;|Sends to| SAB\n    RA --&gt;|Sends to| QB\n    RA --&gt;|Sends to| SAB\n    QB --&gt;|Downloads to| NAS\n    SAB --&gt;|Downloads to| NAS\n    SO --&gt;|Imports from| NAS\n    RA --&gt;|Imports from| NAS\n    JF --&gt;|Reads from| NAS</code></pre>"},{"location":"applications/media-stack/#application-summary","title":"Application Summary","text":"App Purpose Image Gateway URL Jellyfin Media server with GPU transcoding <code>ghcr.io/jellyfin/jellyfin</code> <code>envoy-external</code> <code>jellyfin.example.com</code> Sonarr TV show management <code>ghcr.io/home-operations/sonarr</code> <code>envoy-internal</code> <code>sonarr.example.com</code> Radarr Movie management <code>ghcr.io/home-operations/radarr</code> <code>envoy-internal</code> <code>radarr.example.com</code> Prowlarr Indexer management <code>ghcr.io/home-operations/prowlarr</code> <code>envoy-internal</code> <code>prowlarr.example.com</code> Autobrr IRC announce monitoring <code>ghcr.io/autobrr/autobrr</code> <code>envoy-internal</code> <code>autobrr.example.com</code> qBittorrent Torrent client <code>ghcr.io/home-operations/qbittorrent</code> <code>envoy-internal</code> <code>qbittorrent.example.com</code> SABnzbd Usenet client <code>ghcr.io/home-operations/sabnzbd</code> <code>envoy-internal</code> <code>sabnzbd.example.com</code>"},{"location":"applications/media-stack/#data-flow","title":"Data Flow","text":"<ol> <li>Prowlarr manages indexer configurations and syncs them to Sonarr and Radarr</li> <li>Autobrr monitors IRC announce channels for new releases and pushes matching torrents to qBittorrent</li> <li>Sonarr (TV) and Radarr (movies) search indexers for wanted content and send downloads to qBittorrent or SABnzbd</li> <li>qBittorrent and SABnzbd download content to the NAS at <code>/volume1/media/downloads/</code></li> <li>Sonarr and Radarr import completed downloads, renaming and organizing files into the library</li> <li>Jellyfin serves the organized media library for playback with hardware-accelerated transcoding</li> </ol>"},{"location":"applications/media-stack/#shared-storage","title":"Shared Storage","text":"<p>All media apps mount the same Synology NAS export:</p> <pre><code>persistence:\n  media:\n    type: nfs\n    server: data\n    path: /volume1/media\n    globalMounts:\n      - path: /data/nas-media\n</code></pre> <p>This ensures a single unified path structure across all applications, allowing Sonarr/Radarr to hardlink or move files from download directories into library directories without additional copies.</p> <p>Path Consistency</p> <p>All apps see the NAS at <code>/data/nas-media</code>. This shared mount point is critical for hardlinking to work -- both the download client and the *arr app must see the same filesystem.</p>"},{"location":"applications/media-stack/arr-stack/","title":"Arr Stack","text":"<p>The arr stack consists of four interconnected applications that automate media library management: *Sonarr** for TV shows, Radarr for movies, Prowlarr for indexer management, and Autobrr for IRC announce monitoring.</p>"},{"location":"applications/media-stack/arr-stack/#how-they-interconnect","title":"How They Interconnect","text":"<pre><code>flowchart TD\n    PR[Prowlarr&lt;br/&gt;Port 9696]\n    SO[Sonarr&lt;br/&gt;Port 8989]\n    RA[Radarr&lt;br/&gt;Port 7878]\n    AB[Autobrr&lt;br/&gt;Port 7474]\n    QB[qBittorrent]\n    SAB[SABnzbd]\n    IRC((IRC&lt;br/&gt;Announce Channels))\n\n    PR --&gt;|Sync indexers via API| SO\n    PR --&gt;|Sync indexers via API| RA\n    IRC --&gt;|New release announces| AB\n    AB --&gt;|Push matching torrents| QB\n    SO --&gt;|Search &amp; send downloads| QB\n    SO --&gt;|Search &amp; send downloads| SAB\n    RA --&gt;|Search &amp; send downloads| QB\n    RA --&gt;|Search &amp; send downloads| SAB</code></pre> <p>All *arr apps are exposed through <code>envoy-internal</code>, making them accessible only from the LAN or via Tailscale VPN.</p>"},{"location":"applications/media-stack/arr-stack/#sonarr-tv-shows","title":"Sonarr (TV Shows)","text":"<p>Sonarr monitors for new TV show episodes and manages the download-to-library pipeline.</p> Setting Value Image <code>ghcr.io/home-operations/sonarr:4.0.16.2946</code> Port 8989 URL <code>sonarr.example.com</code> Authentication External (Authelia), disabled for local addresses"},{"location":"applications/media-stack/arr-stack/#configuration","title":"Configuration","text":"Key environment variables<pre><code>env:\n  SONARR__INSTANCE_NAME: Sonarr\n  SONARR__PORT: 8989\n  SONARR__AUTHENTICATION_METHOD: External\n  SONARR__AUTHENTICATION_REQUIRED: DisabledForLocalAddresses\n  SONARR__APPLICATION_URL: \"https://sonarr.example.com\"\n</code></pre>"},{"location":"applications/media-stack/arr-stack/#storage","title":"Storage","text":"Mount Source Purpose <code>/config</code> PVC <code>sonarr-config</code> Sonarr database and settings <code>/data/nas-media</code> NFS <code>data:/volume1/media</code> Media library (shared with downloaders) <p>Sonarr runs as UID/GID 568 with <code>fsGroupChangePolicy: OnRootMismatch</code>.</p>"},{"location":"applications/media-stack/arr-stack/#radarr-movies","title":"Radarr (Movies)","text":"<p>Radarr is the movie equivalent of Sonarr -- it searches for, downloads, and organizes movies.</p> Setting Value Image <code>ghcr.io/home-operations/radarr:6.1.1.10317</code> Port 7878 URL <code>radarr.example.com</code> Authentication External (Authelia), disabled for local addresses"},{"location":"applications/media-stack/arr-stack/#configuration_1","title":"Configuration","text":"Key environment variables<pre><code>env:\n  RADARR__INSTANCE_NAME: Radarr\n  RADARR__PORT: 7878\n  RADARR__AUTHENTICATION_METHOD: External\n  RADARR__AUTHENTICATION_REQUIRED: DisabledForLocalAddresses\n  RADARR__APPLICATION_URL: \"https://radarr.example.com\"\n  RADARR__LOG_LEVEL: info\n</code></pre>"},{"location":"applications/media-stack/arr-stack/#storage_1","title":"Storage","text":"Mount Source Purpose <code>/config</code> PVC <code>radarr-config</code> Radarr database and settings <code>/data/nas-media</code> NFS <code>data:/volume1/media</code> Media library (shared with downloaders) <p>Radarr uses a hardened security context with <code>readOnlyRootFilesystem: true</code> and runs as UID/GID 2000.</p>"},{"location":"applications/media-stack/arr-stack/#prowlarr-indexer-manager","title":"Prowlarr (Indexer Manager)","text":"<p>Prowlarr centralizes indexer management and syncs configurations to Sonarr and Radarr automatically.</p> Setting Value Image <code>ghcr.io/home-operations/prowlarr:2.3.2</code> Port 9696 URL <code>prowlarr.example.com</code> Authentication External (Authelia)"},{"location":"applications/media-stack/arr-stack/#configuration_2","title":"Configuration","text":"Key environment variables<pre><code>env:\n  PROWLARR__INSTANCE_NAME: Prowlarr\n  PROWLARR__PORT: 9696\n  PROWLARR__LOG_LEVEL: info\n  PROWLARR__ANALYTICS_ENABLED: \"False\"\n  PROWLARR__AUTHENTICATION_METHOD: External\n  PROWLARR__API_KEY:\n    valueFrom:\n      secretKeyRef:\n        name: prowlarr-secret\n        key: api_key\n</code></pre> <p>API Key</p> <p>Prowlarr's API key is stored in an <code>ExternalSecret</code> (<code>prowlarr-secret</code>) and injected as an environment variable. This key is used by Sonarr and Radarr to authenticate with Prowlarr's API.</p>"},{"location":"applications/media-stack/arr-stack/#storage_2","title":"Storage","text":"<p>Prowlarr stores its configuration in a single PVC (<code>prowlarr-config</code>). It does not need access to the NFS media share since it only manages indexer metadata.</p>"},{"location":"applications/media-stack/arr-stack/#autobrr-irc-announce-monitoring","title":"Autobrr (IRC Announce Monitoring)","text":"<p>Autobrr monitors IRC announce channels for new releases and automatically pushes matching content to download clients, bypassing the indexer search delay.</p> Setting Value Image <code>ghcr.io/autobrr/autobrr:v1.72.1</code> Port 7474 URL <code>autobrr.example.com</code> Database PostgreSQL (CloudNative-PG <code>autobrr</code> cluster)"},{"location":"applications/media-stack/arr-stack/#configuration_3","title":"Configuration","text":"<p>Autobrr uses PostgreSQL for its database instead of SQLite, with credentials injected from the <code>autobrr-db-secret</code> ExternalSecret:</p> Secret references<pre><code>envFrom:\n  - secretRef:\n      name: autobrr-secret\n  - secretRef:\n      name: autobrr-db-secret\n</code></pre> <p>Autobrr is a stateless application (aside from the database) and does not require PVC storage. It runs as UID/GID 2000 with a hardened security context.</p>"},{"location":"applications/media-stack/downloaders/","title":"Downloaders","text":"<p>The media stack uses two download clients: qBittorrent for torrents and SABnzbd for Usenet. Both are managed by Sonarr and Radarr and download to the shared Synology NAS.</p>"},{"location":"applications/media-stack/downloaders/#qbittorrent","title":"qBittorrent","text":"<p>qBittorrent is the primary torrent client, handling downloads triggered by Sonarr, Radarr, and Autobrr.</p>"},{"location":"applications/media-stack/downloaders/#deployment-details","title":"Deployment Details","text":"Setting Value Image <code>ghcr.io/home-operations/qbittorrent:5.1.4</code> Port 8080 (WebUI) Gateway <code>envoy-internal</code> URL <code>qbittorrent.example.com</code>"},{"location":"applications/media-stack/downloaders/#configuration","title":"Configuration","text":"Key environment variables<pre><code>env:\n  UMASK: \"022\"\n  QBT_WEBUI_PORT: 8080\n</code></pre>"},{"location":"applications/media-stack/downloaders/#storage","title":"Storage","text":"Mount Source Purpose <code>/config</code> PVC <code>qbittorrent-config</code> qBittorrent configuration and database <code>/data/nas-media/downloads/qbittorrent</code> NFS <code>data:/volume1/media</code> Download directory (subpath) Download path configuration<pre><code>persistence:\n  config:\n    existingClaim: qbittorrent-config\n    advancedMounts:\n      qbittorrent:\n        app:\n          - path: /config\n  downloads:\n    type: nfs\n    server: data\n    path: /volume1/media\n    globalMounts:\n      - path: /data/nas-media\n        subPath: downloads/qbittorrent\n</code></pre> <p>Download Path</p> <p>qBittorrent downloads to <code>/data/nas-media/downloads/qbittorrent</code> on the NAS. Sonarr and Radarr see the same NFS mount at <code>/data/nas-media</code>, which allows them to hardlink or move completed downloads into the organized library folders without duplicating data.</p>"},{"location":"applications/media-stack/downloaders/#security-context","title":"Security Context","text":"<p>qBittorrent runs with a hardened security context:</p> <pre><code>securityContext:\n  runAsUser: 2000\n  runAsGroup: 2000\n  runAsNonRoot: true\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n      - ALL\n</code></pre>"},{"location":"applications/media-stack/downloaders/#resources","title":"Resources","text":"<pre><code>resources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    memory: 8192Mi\n</code></pre> <p>Memory Limit</p> <p>The 8 GiB memory limit accommodates qBittorrent's in-memory torrent state, which can grow significantly with large numbers of active torrents.</p>"},{"location":"applications/media-stack/downloaders/#sabnzbd","title":"SABnzbd","text":"<p>SABnzbd is the Usenet download client, providing an alternative download path to torrents for Sonarr and Radarr.</p>"},{"location":"applications/media-stack/downloaders/#deployment-details_1","title":"Deployment Details","text":"Setting Value Image <code>ghcr.io/home-operations/sabnzbd:4.5.5</code> Port 8080 (WebUI) Gateway <code>envoy-internal</code> URL <code>sabnzbd.example.com</code> Node Selector <code>kubernetes.io/arch: amd64</code>"},{"location":"applications/media-stack/downloaders/#configuration_1","title":"Configuration","text":"Key environment variables<pre><code>env:\n  SABNZBD__PORT: 8080\n  SABNZBD__HOST_WHITELIST_ENTRIES: &gt;-\n    sabnzbd,\n    sabnzbd.downloads,\n    sabnzbd.downloads.svc,\n    sabnzbd.downloads.svc.cluster,\n    sabnzbd.downloads.svc.cluster.local,\n    sabnzbd.example.com\n</code></pre> <p>Host Whitelist</p> <p>SABnzbd requires explicit host whitelist entries to allow connections from Kubernetes service names and the external hostname. Without these entries, SABnzbd rejects requests from Sonarr/Radarr and the Envoy Gateway.</p>"},{"location":"applications/media-stack/downloaders/#storage_1","title":"Storage","text":"Mount Source Purpose <code>/config</code> PVC <code>sabnzbd-config</code> SABnzbd configuration <code>/data/nas-media</code> NFS <code>data:/volume1/media</code> Media library (for post-processing) <code>/downloads</code> PVC <code>sabnzbd-downloads</code> Active download staging area <code>/tmp</code> <code>emptyDir</code> Temporary files Storage mounts<pre><code>persistence:\n  config:\n    existingClaim: sabnzbd-config\n  media:\n    type: nfs\n    server: data\n    path: /volume1/media\n    globalMounts:\n      - path: /data/nas-media\n  downloads:\n    existingClaim: sabnzbd-downloads\n</code></pre>"},{"location":"applications/media-stack/downloaders/#security-context_1","title":"Security Context","text":"<p>SABnzbd runs as UID/GID 568 with a read-only root filesystem and all capabilities dropped.</p>"},{"location":"applications/media-stack/downloaders/#integration-with-arr-apps","title":"Integration with Arr Apps","text":"<p>Both download clients are configured as \"Download Clients\" within Sonarr and Radarr. The typical flow:</p> <ol> <li>Sonarr/Radarr find a release on an indexer (managed by Prowlarr)</li> <li>The *arr app sends the <code>.torrent</code> or <code>.nzb</code> to qBittorrent or SABnzbd</li> <li>The download client fetches the content to the NAS</li> <li>Sonarr/Radarr detect the completed download and import it into the library</li> </ol> <pre><code>sequenceDiagram\n    participant S as Sonarr / Radarr\n    participant P as Prowlarr\n    participant Q as qBittorrent / SABnzbd\n    participant N as Synology NAS\n\n    S-&gt;&gt;P: Search indexers\n    P--&gt;&gt;S: Return results\n    S-&gt;&gt;Q: Send download request\n    Q-&gt;&gt;N: Download to /downloads/\n    Q--&gt;&gt;S: Notify completion\n    S-&gt;&gt;N: Import &amp; rename to /library/</code></pre> <p>Path Mapping</p> <p>If the download client and *arr app see different filesystem paths for the same data, hardlinking will fail and files will be copied instead, doubling disk usage. The shared NFS mount at <code>/data/nas-media</code> avoids this problem.</p>"},{"location":"applications/media-stack/jellyfin/","title":"Jellyfin","text":"<p>Jellyfin is the free and open-source media server that provides the playback interface for the entire media stack. It streams content organized by Sonarr and Radarr to any device with a web browser or native client.</p>"},{"location":"applications/media-stack/jellyfin/#deployment-details","title":"Deployment Details","text":"Setting Value Image <code>ghcr.io/jellyfin/jellyfin:10.11.6</code> Namespace <code>media</code> Gateway <code>envoy-external</code> URL <code>jellyfin.example.com</code> Node <code>worker-04</code> (pinned) LoadBalancer IP <code>192.168.0.229</code> <p>Gateway Access</p> <p>Jellyfin uses <code>envoy-external</code> for access via the Cloudflare tunnel. It also has a dedicated LoadBalancer IP (<code>192.168.0.229</code>) for direct LAN access, enabling native client discovery via DLNA or Jellyfin's UDP broadcast.</p>"},{"location":"applications/media-stack/jellyfin/#gpu-transcoding","title":"GPU Transcoding","text":"<p>Jellyfin is configured for hardware-accelerated transcoding using Intel Quick Sync Video (QSV) on the Acemagician AM06 nodes, which have Intel iGPUs.</p> GPU resource allocation<pre><code>resources:\n  requests:\n    cpu: 100m\n    gpu.intel.com/i915: 1\n    memory: 1024M\n  limits:\n    gpu.intel.com/i915: 1\n    memory: 8192M\n</code></pre> <p>The node selector ensures Jellyfin runs on a node with an Intel GPU:</p> <pre><code>nodeSelector:\n  intel.feature.node.kubernetes.io/gpu: \"true\"\n  kubernetes.io/arch: amd64\n  kubernetes.io/hostname: \"worker-04\"\n</code></pre> <p>Intel Device Plugins</p> <p>The <code>intel-device-plugins</code> operator runs in the cluster to expose <code>gpu.intel.com/i915</code> as a schedulable resource. This is deployed as part of the <code>kube-system</code> namespace.</p>"},{"location":"applications/media-stack/jellyfin/#storage","title":"Storage","text":"<p>Jellyfin uses three persistent storage volumes:</p> Mount Source Purpose <code>/config</code> PVC <code>jellyfin</code> Server configuration and database <code>/config/metadata</code> PVC <code>jellyfin-cache</code> Metadata and image cache <code>/data/nas-media</code> NFS <code>data:/volume1/media</code> Media library (Synology NAS) <code>/cache</code>, <code>/config/log</code>, <code>/tmp</code> <code>emptyDir</code> Temporary files, logs NFS media mount<pre><code>persistence:\n  media:\n    type: nfs\n    server: data\n    path: /volume1/media\n    advancedMounts:\n      jellyfin:\n        app:\n          - path: /data/nas-media\n</code></pre>"},{"location":"applications/media-stack/jellyfin/#configuration","title":"Configuration","text":""},{"location":"applications/media-stack/jellyfin/#environment-variables","title":"Environment Variables","text":"Variable Value Purpose <code>DOTNET_SYSTEM_IO_DISABLEFILELOCKING</code> <code>true</code> Prevents file locking issues on NFS volumes"},{"location":"applications/media-stack/jellyfin/#service","title":"Service","text":"<p>Jellyfin is also exposed as a <code>LoadBalancer</code> service with a dedicated Cilium LBIPAM IP (<code>192.168.0.229</code>), allowing direct access from the LAN without going through Envoy Gateway. This enables native client discovery via DLNA or Jellyfin's UDP broadcast.</p> LoadBalancer service<pre><code>service:\n  app:\n    controller: jellyfin\n    type: LoadBalancer\n    annotations:\n      lbipam.cilium.io/ips: \"192.168.0.229\"\n    ports:\n      http:\n        port: 8096\n</code></pre>"},{"location":"applications/media-stack/jellyfin/#route-configuration","title":"Route Configuration","text":"HTTPRoute via envoy-external<pre><code>route:\n  app:\n    hostnames:\n      - jellyfin.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\n</code></pre>"},{"location":"applications/selfhosted/","title":"Self-Hosted Applications","text":"<p>The <code>selfhosted</code> namespace contains a variety of productivity tools, dashboards, and utility services. Most are deployed using the bjw-s app-template Helm chart and connect to Envoy Gateways via HTTPRoute resources.</p>"},{"location":"applications/selfhosted/#application-catalog","title":"Application Catalog","text":"App Description Gateway URL Database Cryptgeon Encrypted secret sharing (view-once notes and files) <code>envoy-external</code> <code>secrets.example.com</code> Valkey (sidecar) Echo Server HTTP request debugging and inspection <code>envoy-external</code> <code>echo.example.com</code> -- Excalidraw Collaborative whiteboard and diagramming <code>envoy-external</code> <code>draw.example.com</code> -- Glance Customizable dashboard with feeds and widgets <code>envoy-external</code> <code>glance.example.com</code> -- Homepage Kubernetes-aware application dashboard <code>envoy-external</code> <code>home.example.com</code> -- House Hunter Property search aggregator <code>envoy-external</code> <code>house-hunter.example.com</code> PostgreSQL Miniflux Minimalist RSS/Atom feed reader <code>envoy-external</code> <code>miniflux.example.com</code> PostgreSQL n8n Workflow automation platform <code>envoy-external</code> <code>n8n.example.com</code> SQLite RRDA DNS REST API for querying DNS records over HTTP <code>envoy-external</code> <code>rrda.example.com</code> -- Sharkord Voice/video communication server <code>envoy-external</code> <code>sharkord.example.com</code> -- Tandoor Recipe management and meal planning <code>envoy-external</code> <code>tandoor.example.com</code> PostgreSQL Whoami Simple HTTP debugging endpoint <code>envoy-external</code> <code>whoami.example.com</code> --"},{"location":"applications/selfhosted/#application-details","title":"Application Details","text":""},{"location":"applications/selfhosted/#cryptgeon","title":"Cryptgeon","text":"<p>Cryptgeon provides encrypted, self-destructing notes and file sharing. Messages are encrypted client-side and stored in a Valkey (Redis-compatible) sidecar.</p> <ul> <li>Image: <code>cupcakearmy/cryptgeon:2.9.1</code></li> <li>Sidecar: <code>valkey/valkey:8.1</code> for ephemeral encrypted data storage</li> <li>Size limit: 100 MB per note</li> </ul>"},{"location":"applications/selfhosted/#echo-server","title":"Echo Server","text":"<p>HTTP Echo Server returns request headers, body, and metadata. Useful for debugging Envoy Gateway routing, TLS termination, and header injection.</p> <ul> <li>Image: <code>ghcr.io/mendhak/http-https-echo:39</code></li> </ul>"},{"location":"applications/selfhosted/#excalidraw","title":"Excalidraw","text":"<p>Excalidraw is an open-source collaborative whiteboard tool for sketching diagrams and illustrations.</p> <ul> <li>Image: <code>docker.io/excalidraw/excalidraw:latest</code></li> <li>Storage: <code>emptyDir</code> only (stateless)</li> </ul>"},{"location":"applications/selfhosted/#glance","title":"Glance","text":"<p>Glance is a self-hosted dashboard with configurable widgets for RSS feeds, weather, bookmarks, and system monitoring.</p> <ul> <li>Image: <code>docker.io/glanceapp/glance:v0.8.4</code></li> <li>Configuration: ConfigMap <code>glance-config</code> mounted as <code>/config/glance.yml</code></li> </ul>"},{"location":"applications/selfhosted/#homepage","title":"Homepage","text":"<p>Homepage is a Kubernetes-aware application dashboard that auto-discovers services and displays their status.</p> <ul> <li>Image: <code>ghcr.io/gethomepage/homepage:v1.9.0</code></li> <li>Configuration: ConfigMap <code>homepage-config</code> with multiple YAML files (bookmarks, services, settings, widgets)</li> <li>RBAC: ServiceAccount with cluster read permissions for Kubernetes service discovery</li> </ul>"},{"location":"applications/selfhosted/#house-hunter","title":"House Hunter","text":"<p>House Hunter is a custom property search aggregator backed by PostgreSQL.</p> <ul> <li>Image: <code>ghcr.io/swibrow/house-hunter:latest</code></li> <li>Database: PostgreSQL (CloudNative-PG <code>house-hunter</code> cluster)</li> </ul>"},{"location":"applications/selfhosted/#miniflux","title":"Miniflux","text":"<p>Miniflux is a minimalist, opinionated RSS feed reader with a clean interface.</p> <ul> <li>Image: <code>ghcr.io/miniflux/miniflux:2.2.17-distroless</code></li> <li>Database: PostgreSQL (CloudNative-PG <code>miniflux</code> cluster)</li> <li>Polling: Every 15 minutes using entry frequency scheduler</li> </ul>"},{"location":"applications/selfhosted/#n8n","title":"n8n","text":"<p>n8n is a workflow automation platform with a visual editor, supporting hundreds of integrations.</p> <ul> <li>Image: <code>ghcr.io/n8n-io/n8n:2.6.2</code></li> <li>Storage: PVC <code>n8n</code> at <code>/home/node/.n8n</code></li> <li>Webhook URL: <code>https://n8n-webhook.example.com</code> (separate HTTPRoute)</li> <li>Timezone: <code>Europe/Zurich</code></li> </ul> <p>Dual Routes</p> <p>n8n has two HTTPRoutes: one for the main UI (<code>n8n.example.com</code>) and one for webhook callbacks (<code>n8n-webhook.example.com</code>). Both use <code>envoy-external</code>.</p>"},{"location":"applications/selfhosted/#rrda","title":"RRDA","text":"<p>RRDA is a REST API for DNS record lookups, providing an HTTP interface to DNS queries.</p> <ul> <li>Image: <code>ghcr.io/swibrow/rrda:1.4.1</code></li> <li>Sidecar: <code>adguard/dnsproxy:v0.78.2</code> -- DNS-over-HTTPS proxy to bypass the Ubiquiti router's DNS interception on port 53</li> </ul> <p>DoH Sidecar</p> <p>The dnsproxy sidecar resolves DNS queries over HTTPS (DoH) to Cloudflare (<code>1.1.1.1</code> / <code>1.0.0.1</code>), bypassing the Ubiquiti router which intercepts all DNS traffic on port 53. See DNS Management for details on this workaround.</p>"},{"location":"applications/selfhosted/#sharkord","title":"Sharkord","text":"<p>Sharkord is a voice and video communication server that uses WebRTC for real-time media.</p> <ul> <li>Image: <code>sharkord/sharkord:latest</code></li> <li>Gateway: <code>envoy-external</code></li> <li>Storage: PVC <code>sharkord</code> at <code>/root/.config/sharkord</code></li> </ul>"},{"location":"applications/selfhosted/#tandoor","title":"Tandoor","text":"<p>Tandoor Recipes is a recipe management and meal planning application.</p> <ul> <li>Image: <code>ghcr.io/tandoorrecipes/recipes:2.4.2</code></li> <li>Sidecar: <code>nginx-unprivileged:1.27.4-alpine</code> for serving static files and media</li> <li>Database: PostgreSQL (CloudNative-PG <code>tandoor</code> cluster)</li> <li>Storage: PVC <code>tandoor-data</code> with subpaths for media, static files, and data</li> </ul>"},{"location":"applications/selfhosted/#whoami","title":"Whoami","text":"<p>Whoami is a lightweight HTTP server that returns connection and request information. Useful for testing gateway routing and TLS configuration.</p> <ul> <li>Image: <code>docker.io/traefik/whoami:v1.11.0</code></li> </ul>"},{"location":"applications/selfhosted/#gateway-distribution","title":"Gateway Distribution","text":"<pre><code>pie title Gateway Usage\n    \"envoy-external\" : 11\n    \"envoy-internal\" : 1</code></pre> <p>Internal-Only Apps</p> <p>Most selfhosted apps use <code>envoy-external</code> for convenience, relying on Cloudflare Access or Authelia for authentication.</p>"},{"location":"ci-cd/","title":"CI/CD","text":"<p>Continuous integration and delivery pipeline for the home lab.</p>"},{"location":"ci-cd/#overview","title":"Overview","text":"<p>The CI/CD pipeline combines three systems to automate code quality, image builds, dependency updates, and cluster deployment:</p> <pre><code>flowchart LR\n    Dev((Developer)) --&gt;|Push / PR| GitHub[GitHub]\n\n    subgraph CI[\"GitHub Actions\"]\n        Lint[Lint Workflow]\n        Build[Docker Build Workflow]\n    end\n\n    subgraph Updates[\"Dependency Management\"]\n        Renovate[Renovate Bot]\n    end\n\n    subgraph CD[\"Continuous Delivery\"]\n        ArgoCD[ArgoCD]\n    end\n\n    GitHub --&gt; Lint\n    GitHub --&gt; Build\n    Renovate --&gt;|PRs| GitHub\n    Build --&gt;|Push images| GHCR[ghcr.io]\n    GitHub --&gt;|main branch| ArgoCD\n    ArgoCD --&gt;|Sync| Cluster[Cluster]</code></pre>"},{"location":"ci-cd/#pipeline-components","title":"Pipeline Components","text":""},{"location":"ci-cd/#github-actions","title":"GitHub Actions","text":"<p>Two workflows automate CI tasks:</p> Workflow Trigger Purpose Lint Pull requests to <code>main</code> Code quality checks on PR changes Build Docker Images Push to <code>main</code> (docker/**) or manual dispatch Build and push multi-arch Docker images to GHCR <p>Details: GitHub Actions</p>"},{"location":"ci-cd/#docker-builds","title":"Docker Builds","text":"<p>The Docker build workflow provides:</p> <ul> <li>Automatic discovery of changed Dockerfiles</li> <li>Version extraction from <code>ARG</code> directives</li> <li>Multi-platform builds (linux/amd64, linux/arm64)</li> <li>Push to GitHub Container Registry (ghcr.io)</li> </ul> <p>Details: Docker Builds</p>"},{"location":"ci-cd/#renovate","title":"Renovate","text":"<p>Renovate Bot automatically manages dependency updates:</p> <ul> <li>Extends bjw-s presets for home-ops patterns</li> <li>Auto-merges Docker digest and patch updates</li> <li>Auto-merges GitHub Actions digest and patch updates</li> <li>Auto-merges Helm patch updates</li> <li>Groups related packages (Cilium, Rook Ceph, Talos)</li> <li>Custom Grafana dashboard version tracking</li> </ul> <p>Details: Renovate</p>"},{"location":"ci-cd/#argocd","title":"ArgoCD","text":"<p>ArgoCD provides GitOps-based continuous delivery:</p> <ul> <li>Watches the <code>main</code> branch of the repository</li> <li>Auto-discovers applications via ApplicationSets</li> <li>Syncs Kubernetes manifests to the cluster</li> <li>Provides health monitoring and rollback capabilities</li> </ul> <p>Details: ArgoCD Setup</p>"},{"location":"ci-cd/#workflow-summary","title":"Workflow Summary","text":"Event Action Result PR opened Lint workflow runs Code quality verified PR merged to <code>main</code> with docker/ changes Docker build workflow New images pushed to GHCR PR merged to <code>main</code> with app changes ArgoCD detects change Application synced to cluster Renovate detects update Creates PR Auto-merged (digest/patch) or awaits review Manual dispatch Docker build for all images All images rebuilt and pushed"},{"location":"ci-cd/#documentation-deployment","title":"Documentation Deployment","text":"<p>The documentation site itself is also CI/CD managed:</p> Workflow Trigger Purpose Deploy Docs Push to <code>main</code> (docs/** or mkdocs.yml) Build and deploy MkDocs Material site to GitHub Pages"},{"location":"ci-cd/docker-builds/","title":"Docker Builds","text":"<p>Automated Docker image builds using GitHub Actions with multi-architecture support and GitHub Container Registry (GHCR).</p>"},{"location":"ci-cd/docker-builds/#overview","title":"Overview","text":"<p>Custom Docker images live in the <code>docker/</code> directory at the repository root. Each subdirectory contains a <code>Dockerfile</code> and any supporting files for a single image.</p> <pre><code>docker/\n\u251c\u2500\u2500 app-one/\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 app-two/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 config.yaml\n\u2514\u2500\u2500 app-three/\n    \u2514\u2500\u2500 Dockerfile\n</code></pre>"},{"location":"ci-cd/docker-builds/#auto-discovery","title":"Auto-Discovery","text":"<p>The build workflow automatically discovers which images need to be built based on the trigger:</p>"},{"location":"ci-cd/docker-builds/#on-push-to-main","title":"On Push to <code>main</code>","text":"<p>When files under <code>docker/</code> are changed and pushed to <code>main</code>, the workflow uses tj-actions/changed-files to detect which subdirectories have been modified:</p> <pre><code>- id: changed\n  uses: tj-actions/changed-files@v47\n  with:\n    dir_names: \"true\"\n    dir_names_max_depth: \"2\"\n    files: docker/**\n    json: \"true\"\n    escape_json: \"false\"\n</code></pre> <p>Only the changed images are built, saving time and compute resources.</p>"},{"location":"ci-cd/docker-builds/#on-manual-dispatch","title":"On Manual Dispatch","text":"<p>When manually triggered via <code>workflow_dispatch</code>, all images under <code>docker/</code> are discovered and built:</p> <pre><code>ls -d docker/*/ | xargs -I{} basename {}\n</code></pre>"},{"location":"ci-cd/docker-builds/#version-extraction","title":"Version Extraction","text":"<p>Image versions are automatically extracted from the Dockerfile's <code>ARG</code> directives:</p> <pre><code># Example Dockerfile\nARG APP_VERSION=1.2.3\n\nFROM ubuntu:22.04\n# ...\n</code></pre> <p>The build step uses a regex to extract the version:</p> <pre><code>version=$(grep -oP 'ARG \\w+_VERSION=\\K.+' docker/${{ matrix.image }}/Dockerfile | head -1)\n</code></pre> Scenario Tag <code>ARG APP_VERSION=1.2.3</code> found <code>1.2.3</code> No <code>*_VERSION</code> ARG found Git commit SHA <p>Version ARG Naming</p> <p>Any ARG ending in <code>_VERSION</code> will be detected. Common patterns: <pre><code>ARG APP_VERSION=2.0.0\nARG TOOL_VERSION=1.5.3\nARG BASE_VERSION=3.18\n</code></pre></p>"},{"location":"ci-cd/docker-builds/#multi-platform-builds","title":"Multi-Platform Builds","text":"<p>All images are built for two architectures using Docker Buildx and QEMU emulation:</p> Platform Architecture Use Case <code>linux/amd64</code> x86_64 Intel/AMD worker nodes <code>linux/arm64</code> AArch64 Raspberry Pi nodes <pre><code>- uses: docker/setup-qemu-action@v3\n\n- uses: docker/setup-buildx-action@v3\n\n- uses: docker/build-push-action@v6\n  with:\n    context: docker/${{ matrix.image }}\n    platforms: linux/amd64,linux/arm64\n    push: true\n    tags: |\n      ghcr.io/swibrow/${{ matrix.image }}:&lt;version&gt;\n      ghcr.io/swibrow/${{ matrix.image }}:latest\n</code></pre> <p>Build Times</p> <p>ARM64 builds on AMD64 runners use QEMU emulation, which is slower than native builds. Expect ARM64 builds to take 2-5x longer than AMD64 builds.</p>"},{"location":"ci-cd/docker-builds/#registry","title":"Registry","text":"<p>All images are pushed to the GitHub Container Registry (GHCR):</p> <pre><code>ghcr.io/swibrow/&lt;image-name&gt;:&lt;tag&gt;\n</code></pre>"},{"location":"ci-cd/docker-builds/#authentication","title":"Authentication","text":"<p>The workflow authenticates using the built-in <code>GITHUB_TOKEN</code>:</p> <pre><code>- uses: docker/login-action@v3\n  with:\n    registry: ghcr.io\n    username: ${{ github.actor }}\n    password: ${{ secrets.GITHUB_TOKEN }}\n</code></pre>"},{"location":"ci-cd/docker-builds/#permissions","title":"Permissions","text":"<p>The build job requires <code>packages: write</code> permission to push images:</p> <pre><code>permissions:\n  contents: read\n  packages: write\n</code></pre>"},{"location":"ci-cd/docker-builds/#image-tags","title":"Image Tags","text":"<p>Each successful build produces two tags:</p> Tag Purpose <code>&lt;version&gt;</code> Immutable version tag extracted from Dockerfile <code>latest</code> Rolling tag pointing to the most recent build <p>Example for an image <code>rrda</code> with <code>ARG RRDA_VERSION=2.1.0</code>:</p> <pre><code>ghcr.io/swibrow/rrda:2.1.0\nghcr.io/swibrow/rrda:latest\n</code></pre>"},{"location":"ci-cd/docker-builds/#adding-a-new-docker-image","title":"Adding a New Docker Image","text":"<ol> <li> <p>Create a new directory under <code>docker/</code>:</p> <pre><code>mkdir docker/my-app\n</code></pre> </li> <li> <p>Add a <code>Dockerfile</code> with a version ARG:</p> <pre><code>ARG MY_APP_VERSION=1.0.0\n\nFROM python:3.12-slim\n# ... build steps\n</code></pre> </li> <li> <p>Commit and push to <code>main</code>:</p> <pre><code>git add docker/my-app/\ngit commit -m \"feat(docker): add my-app image\"\ngit push origin main\n</code></pre> </li> <li> <p>The build workflow will automatically detect the new directory and build the image.</p> </li> <li> <p>The image will be available at:</p> <pre><code>ghcr.io/swibrow/my-app:1.0.0\nghcr.io/swibrow/my-app:latest\n</code></pre> </li> </ol>"},{"location":"ci-cd/docker-builds/#matrix-strategy","title":"Matrix Strategy","text":"<p>The build job uses a matrix strategy to build all discovered images in parallel:</p> <pre><code>strategy:\n  matrix:\n    image: ${{ fromJson(needs.discover.outputs.images) }}\n</code></pre> <p>This means if three images change in a single push, three build jobs run concurrently, each handling one image independently.</p>"},{"location":"ci-cd/github-actions/","title":"GitHub Actions","text":"<p>GitHub Actions workflows for linting, Docker builds, and documentation deployment.</p>"},{"location":"ci-cd/github-actions/#workflows","title":"Workflows","text":"Workflow File Trigger Lint <code>.github/workflows/lint.yaml</code> Pull requests to <code>main</code> Build Docker Images <code>.github/workflows/build-docker-images.yaml</code> Push to <code>main</code> (docker/**) or manual dispatch Deploy Docs <code>.github/workflows/deploy-docs.yml</code> Push to <code>main</code> (docs/** or mkdocs.yml)"},{"location":"ci-cd/github-actions/#lint-workflow","title":"Lint Workflow","text":"<p>Runs on every pull request targeting the <code>main</code> branch. Validates code quality before merge.</p> .github/workflows/lint.yaml<pre><code>name: Linter\n\non:\n  pull_request:\n    branches: [main]\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v6\n      - run: |\n          echo \"Linting...\"\n</code></pre> <p>Extending the Linter</p> <p>This is a minimal linting skeleton. Add steps for YAML validation, Helm template checks, or shellcheck as needed.</p>"},{"location":"ci-cd/github-actions/#build-docker-images-workflow","title":"Build Docker Images Workflow","text":"<p>Automatically discovers changed Dockerfiles, extracts versions, and builds multi-architecture images. See Docker Builds for detailed documentation.</p>"},{"location":"ci-cd/github-actions/#trigger","title":"Trigger","text":"<pre><code>on:\n  push:\n    branches: [main]\n    paths:\n      - docker/**\n  workflow_dispatch:\n</code></pre> <ul> <li>Push: Only triggers when files under <code>docker/</code> change on <code>main</code></li> <li>Manual dispatch: Rebuilds all images regardless of changes</li> </ul>"},{"location":"ci-cd/github-actions/#discovery-job","title":"Discovery Job","text":"<p>The <code>discover</code> job identifies which images need to be built:</p> <pre><code>discover:\n  runs-on: ubuntu-latest\n  outputs:\n    images: ${{ steps.set-matrix.outputs.images }}\n  steps:\n    - uses: actions/checkout@v4\n\n    - id: changed\n      if: github.event_name == 'push'\n      uses: tj-actions/changed-files@v47\n      with:\n        dir_names: \"true\"\n        dir_names_max_depth: \"2\"\n        files: docker/**\n        json: \"true\"\n        escape_json: \"false\"\n\n    - id: set-matrix\n      run: |\n        if [[ \"${{ github.event_name }}\" == \"workflow_dispatch\" ]]; then\n          echo \"images=$(ls -d docker/*/ | xargs -I{} basename {} | \\\n            jq -R -s -c 'split(\"\\n\") | map(select(length &gt; 0))')\" &gt;&gt; \"$GITHUB_OUTPUT\"\n        else\n          echo \"images=$(echo '${{ steps.changed.outputs.all_changed_files }}' | \\\n            jq -c '[.[] | ltrimstr(\"docker/\")]')\" &gt;&gt; \"$GITHUB_OUTPUT\"\n        fi\n</code></pre> <p>On push: Uses <code>tj-actions/changed-files</code> to detect which <code>docker/</code> subdirectories have changed, producing a JSON array like <code>[\"app1\", \"app2\"]</code>.</p> <p>On manual dispatch: Lists all directories under <code>docker/</code>, building everything.</p>"},{"location":"ci-cd/github-actions/#build-job","title":"Build Job","text":"<p>Uses a matrix strategy to build each discovered image in parallel:</p> <pre><code>build:\n  needs: discover\n  if: needs.discover.outputs.images != '[]'\n  runs-on: ubuntu-latest\n  permissions:\n    contents: read\n    packages: write\n  strategy:\n    matrix:\n      image: ${{ fromJson(needs.discover.outputs.images) }}\n</code></pre> <p>Key build steps:</p> <ol> <li>QEMU setup -- Enables cross-architecture emulation for ARM64 builds</li> <li>Buildx setup -- Configures Docker Buildx for multi-platform builds</li> <li>Registry login -- Authenticates to ghcr.io using the GitHub token</li> <li>Version extraction -- Reads the version from <code>ARG *_VERSION=</code> in the Dockerfile</li> <li>Build and push -- Builds for <code>linux/amd64</code> and <code>linux/arm64</code>, tags with version and <code>latest</code></li> </ol>"},{"location":"ci-cd/github-actions/#version-extraction","title":"Version Extraction","text":"<pre><code>- id: version\n  run: |\n    version=$(grep -oP 'ARG \\w+_VERSION=\\K.+' docker/${{ matrix.image }}/Dockerfile | head -1)\n    echo \"tag=${version:-${{ github.sha }}}\" &gt;&gt; \"$GITHUB_OUTPUT\"\n</code></pre> <p>The version tag is extracted from the first <code>ARG *_VERSION=</code> line in the Dockerfile. If no version ARG is found, it falls back to the Git commit SHA.</p>"},{"location":"ci-cd/github-actions/#image-tags","title":"Image Tags","text":"<p>Each image is tagged twice:</p> <pre><code>ghcr.io/swibrow/&lt;image&gt;:&lt;version&gt;\nghcr.io/swibrow/&lt;image&gt;:latest\n</code></pre>"},{"location":"ci-cd/github-actions/#deploy-docs-workflow","title":"Deploy Docs Workflow","text":"<p>Builds and deploys the MkDocs Material documentation site to GitHub Pages.</p> .github/workflows/deploy-docs.yml<pre><code>name: Deploy Docs\n\non:\n  push:\n    branches: [main]\n    paths:\n      - \"docs/**\"\n      - \"mkdocs.yml\"\n      - \"requirements.txt\"\n  workflow_dispatch:\n\npermissions:\n  contents: write\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v6\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n          cache: pip\n\n      - run: pip install -r requirements.txt\n\n      - run: mkdocs gh-deploy --force\n</code></pre> <ul> <li>Triggered when docs, mkdocs config, or Python requirements change</li> <li>Uses <code>fetch-depth: 0</code> for the git-revision-date-localized plugin</li> <li>Deploys to the <code>gh-pages</code> branch</li> </ul>"},{"location":"ci-cd/renovate/","title":"Renovate","text":"<p>Automated dependency management using Renovate Bot to keep Docker images, Helm charts, and GitHub Actions up to date.</p>"},{"location":"ci-cd/renovate/#overview","title":"Overview","text":"<p>Renovate scans the repository for dependency declarations and creates pull requests when updates are available. The configuration extends community presets and adds custom rules for the cluster.</p> <pre><code>flowchart TD\n    Renovate[Renovate Bot] --&gt;|Scans repo| Repo[home-ops]\n    Repo --&gt; Docker[Docker images&lt;br/&gt;in values.yaml]\n    Repo --&gt; Helm[Helm charts&lt;br/&gt;in kustomization.yaml]\n    Repo --&gt; Actions[GitHub Actions&lt;br/&gt;in workflows]\n    Repo --&gt; Grafana[Grafana dashboards&lt;br/&gt;gnetId + revision]\n\n    Renovate --&gt;|Creates PRs| PRs[Pull Requests]\n    PRs --&gt;|Auto-merge| AutoMerge[Digest / Patch]\n    PRs --&gt;|Manual review| Manual[Minor / Major]</code></pre>"},{"location":"ci-cd/renovate/#configuration","title":"Configuration","text":"<p>The root configuration lives in <code>renovate.json5</code>:</p> renovate.json5<pre><code>{\n  $schema: \"https://docs.renovatebot.com/renovate-schema.json\",\n  extends: [\n    \"github&gt;bjw-s/renovate-config\",\n    \"github&gt;bjw-s/renovate-config:automerge-docker-digest\",\n    \"github&gt;bjw-s/renovate-config:automerge-github-actions\",\n    \"github&gt;swibrow/home-ops//.renovate/allowedVersions.json5\",\n    \"github&gt;swibrow/home-ops//.renovate/autoMerge.json5\",\n    \"github&gt;swibrow/home-ops//.renovate/clusters.json5\",\n    \"github&gt;swibrow/home-ops//.renovate/grafanaDashboards.json5\",\n    \"github&gt;swibrow/home-ops//.renovate/groups.json5\",\n    \"github&gt;swibrow/home-ops//.renovate/versioning.json5\",\n  ],\n  ignorePaths: [\".archive/**\"],\n  flux: {\n    fileMatch: [\"^kubernetes/.+\\\\.ya?ml$\"],\n  },\n  \"helm-values\": {\n    fileMatch: [\"^kubernetes/.+\\\\.ya?ml$\"],\n  },\n  kubernetes: {\n    fileMatch: [\"^kubernetes/.+\\\\.ya?ml$\"],\n  },\n}\n</code></pre>"},{"location":"ci-cd/renovate/#base-presets","title":"Base Presets","text":"Preset Purpose <code>bjw-s/renovate-config</code> Base configuration for home-ops repositories <code>bjw-s/renovate-config:automerge-docker-digest</code> Auto-merge Docker digest updates <code>bjw-s/renovate-config:automerge-github-actions</code> Auto-merge GitHub Actions updates"},{"location":"ci-cd/renovate/#custom-configurations","title":"Custom Configurations","text":"<p>Custom rules are stored in the <code>.renovate/</code> directory:</p>"},{"location":"ci-cd/renovate/#allowed-versions-renovateallowedversionsjson5","title":"Allowed Versions (<code>.renovate/allowedVersions.json5</code>)","text":"<p>Restricts which versions Renovate will propose for specific packages:</p> <pre><code>{\n  packageRules: [\n    {\n      matchDatasources: [\"docker\"],\n      matchPackageNames: [\"ghcr.io/linuxserver/calibre-web\"],\n      allowedVersions: \"&lt;1\",\n    },\n    {\n      matchDatasources: [\"docker\"],\n      matchPackageNames: [\"tomsquest/docker-radicale\"],\n      allowedVersions: \"/^[0-9]+\\\\.[0-9]+\\\\.[0-9]+(\\\\.[0-9]+)?$/\",\n    },\n  ],\n}\n</code></pre> <p>Use this to pin packages to specific version ranges or filter out pre-release versions.</p>"},{"location":"ci-cd/renovate/#auto-merge-renovateautomergejson5","title":"Auto-Merge (<code>.renovate/autoMerge.json5</code>)","text":"<p>Defines which update types are automatically merged without manual review:</p> Rule Datasource Update Types Auto-Merge Container digests and patches Docker digest, patch, pin, pinDigest Yes KPS minor and patches Helm (<code>kube-prometheus-stack</code>) minor, patch Yes Helm patches Helm patch, digest, pin, pinDigest Yes <pre><code>{\n  packageRules: [\n    {\n      description: \"Auto merge container digests and patches\",\n      matchDatasources: [\"docker\"],\n      automerge: true,\n      matchUpdateTypes: [\"digest\", \"patch\", \"pin\", \"pinDigest\"],\n    },\n    {\n      description: \"Auto merge KPS minors and patches\",\n      matchDatasources: [\"helm\"],\n      automerge: true,\n      matchUpdateTypes: [\"minor\", \"patch\"],\n      matchPackageNames: [\"kube-prometheus-stack\"],\n    },\n    {\n      description: \"Auto merge helm patches\",\n      matchDatasources: [\"helm\"],\n      automerge: true,\n      matchUpdateTypes: [\"patch\", \"digest\", \"pin\", \"pinDigest\"],\n    },\n  ],\n}\n</code></pre>"},{"location":"ci-cd/renovate/#clusters-renovateclustersjson5","title":"Clusters (<code>.renovate/clusters.json5</code>)","text":"<p>Adds a branch prefix for cluster-specific updates:</p> <pre><code>{\n  packageRules: [\n    {\n      description: \"Separate PRs for the cluster\",\n      matchFileNames: [\"**/kubernetes/pitower/**\"],\n      additionalBranchPrefix: \"cluster-\",\n    },\n  ],\n}\n</code></pre>"},{"location":"ci-cd/renovate/#groups-renovategroupsjson5","title":"Groups (<code>.renovate/groups.json5</code>)","text":"<p>Groups related packages into single PRs to avoid upgrade conflicts:</p> Group Packages Datasources Flux <code>flux*</code>, <code>ghcr.io/fluxcd/*</code> Docker, GitHub Tags Rook Ceph <code>rook.ceph*</code> Docker, Helm Cilium <code>quay.io/cilium/*</code>, <code>cilium</code> Docker, Helm ARC Actions Runner Controller charts Docker, Helm Talos <code>ghcr.io/siderolabs/installer</code>, <code>talosctl</code> Docker silence-operator <code>silence-operator*</code> Docker, GitHub Releases <p>Separate Minor/Patch</p> <p>Most groups use <code>separateMinorPatch: true</code>, creating separate PRs for minor vs. patch updates to allow different review levels.</p>"},{"location":"ci-cd/renovate/#versioning-renovateversioningjson5","title":"Versioning (<code>.renovate/versioning.json5</code>)","text":"<p>Custom versioning schemes for packages that do not follow standard semver:</p> <pre><code>{\n  packageRules: [\n    {\n      description: \"Loose versioning for non-semver packages\",\n      matchDatasources: [\"docker\"],\n      matchPackageNames: [\n        \"ghcr.io/cross-seed/cross-seed\",\n        \"ghcr.io/home-operations/plex\",\n      ],\n      versioning: \"loose\",\n    },\n    {\n      description: \"Custom versioning for http-https-echo\",\n      matchDatasources: [\"docker\"],\n      matchPackageNames: [\"ghcr.io/mendhak/http-https-echo\"],\n      versioning: \"regex:^(?&lt;major&gt;\\\\d+)$\",\n    },\n  ],\n}\n</code></pre>"},{"location":"ci-cd/renovate/#grafana-dashboards-renovategrafanadashboardsjson5","title":"Grafana Dashboards (<code>.renovate/grafanaDashboards.json5</code>)","text":"<p>Custom manager and datasource for tracking Grafana dashboard revisions:</p> <pre><code>{\n  customDatasources: {\n    \"grafana-dashboards\": {\n      defaultRegistryUrlTemplate:\n        \"https://grafana.com/api/dashboards/{{packageName}}\",\n      format: \"json\",\n      transformTemplates: [\n        '{\"releases\":[{\"version\": $string(revision)}]}',\n      ],\n    },\n  },\n  customManagers: [\n    {\n      customType: \"regex\",\n      description: \"Process Grafana dashboards\",\n      fileMatch: [\"(^|/)kubernetes/.+\\\\.ya?ml(\\\\.j2)?$\"],\n      matchStrings: [\n        '# renovate: dashboardName=\"(?&lt;depName&gt;.*)\"\\\\n(?&lt;indentation&gt;\\\\s+)gnetId: (?&lt;packageName&gt;\\\\d+)\\\\n.+revision: (?&lt;currentValue&gt;\\\\d+)',\n      ],\n      datasourceTemplate: \"custom.grafana-dashboards\",\n      versioningTemplate: \"regex:^(?&lt;major&gt;\\\\d+)$\",\n    },\n  ],\n}\n</code></pre> <p>Grafana dashboard updates are auto-merged and committed directly to the branch (no PR review needed):</p> <pre><code>{\n  packageRules: [\n    {\n      automerge: true,\n      automergeType: \"branch\",\n      matchDatasources: [\"custom.grafana-dashboards\"],\n      matchUpdateTypes: [\"major\"],\n      semanticCommitScope: \"grafana-dashboards\",\n    },\n  ],\n}\n</code></pre>"},{"location":"ci-cd/renovate/#how-it-works","title":"How It Works","text":""},{"location":"ci-cd/renovate/#dependency-detection","title":"Dependency Detection","text":"<p>Renovate detects dependencies in:</p> Source Pattern Example Helm charts <code>version:</code> in <code>kustomization.yaml</code> <code>version: 4.6.2</code> Docker images <code>tag:</code> in <code>values.yaml</code> <code>tag: 39</code> GitHub Actions <code>uses:</code> in workflow files <code>uses: actions/checkout@v6</code> Grafana dashboards <code>gnetId:</code> + <code>revision:</code> <code>gnetId: 1860</code> / <code>revision: 37</code>"},{"location":"ci-cd/renovate/#pr-flow","title":"PR Flow","text":"<ol> <li>Renovate detects an available update</li> <li>Creates a PR with the version bump</li> <li>Based on rules:<ul> <li>Auto-merge eligible: PR is merged automatically after CI passes</li> <li>Manual review required: PR awaits human review and approval</li> </ul> </li> <li>On merge to <code>main</code>, ArgoCD picks up the change and syncs to the cluster</li> </ol>"},{"location":"ci-cd/renovate/#labels-and-commit-messages","title":"Labels and Commit Messages","text":"<ul> <li>Grouped PRs use topic-based commit messages (e.g., <code>chore(deps): update Cilium group</code>)</li> <li>Grafana dashboard PRs are labeled with <code>renovate/grafana-dashboard</code></li> <li>Semantic commit types follow conventional commit format</li> </ul>"},{"location":"development/","title":"Development","text":"<p>Guide for contributing to the home lab repository and developing new applications.</p>"},{"location":"development/#overview","title":"Overview","text":"<p>All cluster state is defined as code in this Git repository. Changes are made via pull requests to the <code>main</code> branch, and ArgoCD automatically syncs the desired state to the cluster.</p> <pre><code>flowchart LR\n    Dev((Developer)) --&gt;|Edit| Local[Local Clone]\n    Local --&gt;|Push| Branch[Feature Branch]\n    Branch --&gt;|PR| Main[main branch]\n    Main --&gt;|Webhook| ArgoCD[ArgoCD]\n    ArgoCD --&gt;|Sync| Cluster[Cluster]</code></pre>"},{"location":"development/#repository-layout","title":"Repository Layout","text":"<pre><code>pitower/kubernetes/apps/\n\u251c\u2500\u2500 ai/                    # AI workloads\n\u251c\u2500\u2500 banking/               # Financial tools\n\u251c\u2500\u2500 cert-manager/          # TLS certificate management\n\u251c\u2500\u2500 cloudnative-pg/        # PostgreSQL operator and clusters\n\u251c\u2500\u2500 home-automation/       # Home Assistant ecosystem\n\u251c\u2500\u2500 kube-system/           # Core Kubernetes components\n\u251c\u2500\u2500 media/                 # Media management stack\n\u251c\u2500\u2500 monitoring/            # Observability stack\n\u251c\u2500\u2500 networking/            # Network infrastructure\n\u251c\u2500\u2500 openebs/               # Local persistent volumes\n\u251c\u2500\u2500 rook-ceph/             # Distributed storage\n\u251c\u2500\u2500 security/              # Auth, secrets, identity\n\u251c\u2500\u2500 selfhosted/            # General self-hosted apps\n\u2514\u2500\u2500 system/                # System utilities\n</code></pre> <p>Each application lives in its own subdirectory and contains:</p> <ul> <li><code>kustomization.yaml</code> -- Kustomize config with Helm chart reference</li> <li><code>values.yaml</code> -- Helm chart values</li> </ul>"},{"location":"development/#local-development","title":"Local Development","text":""},{"location":"development/#prerequisites","title":"Prerequisites","text":"Tool Purpose <code>kubectl</code> Kubernetes CLI <code>kustomize</code> Manifest generation (with Helm support) <code>helm</code> Helm chart operations <code>talosctl</code> Talos node management <code>sops</code> Secret decryption <code>just</code> Task runner for justfile recipes <code>argocd</code> ArgoCD CLI (optional)"},{"location":"development/#validate-changes-locally","title":"Validate Changes Locally","text":"<p>Before pushing, validate your Helm templates render correctly:</p> <pre><code>cd pitower/kubernetes/apps/&lt;category&gt;/&lt;app&gt;\nkustomize build . --enable-helm\n</code></pre> <p>This renders the full Kubernetes manifest without applying it.</p>"},{"location":"development/#test-with-dry-run","title":"Test with Dry Run","text":"<p>Apply with <code>--dry-run=server</code> to validate against the cluster's API:</p> <pre><code>kustomize build . --enable-helm | kubectl apply --dry-run=server -f -\n</code></pre>"},{"location":"development/#development-sections","title":"Development Sections","text":"Page Description App Template bjw-s app-template Helm chart patterns and common values Adding Apps Step-by-step guide to adding a new application to the cluster"},{"location":"development/#conventions","title":"Conventions","text":""},{"location":"development/#naming","title":"Naming","text":"<ul> <li>Namespaces match the category directory name (e.g., <code>selfhosted</code>, <code>media</code>, <code>networking</code>)</li> <li>Release names match the application directory name</li> <li>Hostnames follow the pattern <code>&lt;app&gt;.example.com</code></li> </ul>"},{"location":"development/#values-files","title":"Values Files","text":"<ul> <li>Use the bjw-s app-template chart (v4.6.2) for most applications</li> <li>Always set resource requests and limits</li> <li>Use <code>reloader.stakater.com/auto: \"true\"</code> annotation for automatic reloads on ConfigMap/Secret changes</li> <li>Configure health probes (liveness, readiness, startup)</li> </ul>"},{"location":"development/#gateway-routing","title":"Gateway Routing","text":"<ul> <li>External services (public, Cloudflare-proxied): Use <code>envoy-external</code> gateway</li> <li>Internal services (LAN/VPN only): Use <code>envoy-internal</code> gateway</li> </ul>"},{"location":"development/#secrets","title":"Secrets","text":"<ul> <li>Never commit plaintext secrets</li> <li>Use External Secrets Operator with 1Password Connect for runtime secrets</li> <li>Use SOPS with age encryption for secrets that must live in Git (e.g., Talos configs)</li> </ul>"},{"location":"development/adding-apps/","title":"Adding Applications","text":"<p>Step-by-step guide for adding a new application to the cluster.</p>"},{"location":"development/adding-apps/#overview","title":"Overview","text":"<p>Adding a new application involves creating a directory with two files, pushing to <code>main</code>, and letting ArgoCD auto-discover and deploy it.</p> <pre><code>flowchart LR\n    Create[Create app directory] --&gt; Files[Add kustomization.yaml&lt;br/&gt;+ values.yaml]\n    Files --&gt; Push[Push to main]\n    Push --&gt; ArgoCD[ArgoCD discovers app]\n    ArgoCD --&gt; Sync[App synced to cluster]</code></pre>"},{"location":"development/adding-apps/#step-1-choose-a-category","title":"Step 1: Choose a Category","text":"<p>Applications are organized by category under <code>pitower/kubernetes/apps/</code>. Choose the appropriate category for your application:</p> Category Purpose Examples <code>ai</code> AI and ML workloads kagent <code>banking</code> Financial tools firefly, firefly-importer <code>home-automation</code> Smart home home-assistant, zigbee2mqtt <code>media</code> Media management jellyfin, sonarr, radarr <code>monitoring</code> Observability grafana, loki <code>networking</code> Network infrastructure cloudflared, external-dns <code>security</code> Auth and secrets authelia, lldap <code>selfhosted</code> General self-hosted apps miniflux, n8n, tandoor <code>system</code> System utilities reloader, node-feature-discovery"},{"location":"development/adding-apps/#step-2-create-the-directory","title":"Step 2: Create the Directory","text":"<pre><code>mkdir -p pitower/kubernetes/apps/&lt;category&gt;/&lt;app-name&gt;\n</code></pre> <p>For example:</p> <pre><code>mkdir -p pitower/kubernetes/apps/selfhosted/my-app\n</code></pre>"},{"location":"development/adding-apps/#step-3-create-kustomizationyaml","title":"Step 3: Create kustomization.yaml","text":"<p>Create a <code>kustomization.yaml</code> that references the bjw-s app-template Helm chart:</p> pitower/kubernetes/apps///kustomization.yaml<pre><code>---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: &lt;category&gt;\nhelmCharts:\n  - name: app-template\n    repo: oci://ghcr.io/bjw-s-labs/helm\n    version: 4.6.2\n    releaseName: &lt;app-name&gt;\n    namespace: &lt;category&gt;\n    valuesFile: values.yaml\n</code></pre> <p>Namespace</p> <p>The namespace typically matches the category directory name. Ensure the namespace exists in the cluster or is created by another resource.</p>"},{"location":"development/adding-apps/#step-4-create-valuesyaml","title":"Step 4: Create values.yaml","text":"<p>Create a <code>values.yaml</code> with your application's configuration. At minimum, define a controller, container, and service:</p> pitower/kubernetes/apps///values.yaml<pre><code>controllers:\n  &lt;app-name&gt;:\n    annotations:\n      reloader.stakater.com/auto: \"true\"\n    containers:\n      app:\n        image:\n          repository: &lt;image-repository&gt;\n          tag: &lt;image-tag&gt;\n        env:\n          TZ: Europe/Zurich\n        resources:\n          requests:\n            cpu: 10m\n            memory: 64Mi\n          limits:\n            memory: 256Mi\n        probes:\n          liveness:\n            enabled: true\n          readiness:\n            enabled: true\n          startup:\n            enabled: true\n            spec:\n              failureThreshold: 30\n              periodSeconds: 5\nservice:\n  app:\n    ports:\n      http:\n        port: 8080\n</code></pre> <p>See the App Template page for the full values reference.</p>"},{"location":"development/adding-apps/#step-5-configure-routing-optional","title":"Step 5: Configure Routing (Optional)","text":"<p>If the application needs to be accessible via a URL, add an HTTPRoute configuration to <code>values.yaml</code>:</p> External (Cloudflare-proxied)Internal (LAN/VPN only) <pre><code>route:\n  app:\n    enabled: true\n    hostnames:\n      - &lt;app-name&gt;.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\n</code></pre> <pre><code>route:\n  app:\n    enabled: true\n    hostnames:\n      - &lt;app-name&gt;.internal.example.com\n    parentRefs:\n      - name: envoy-internal\n        namespace: networking\n        sectionName: https\n</code></pre>"},{"location":"development/adding-apps/#step-6-add-persistence-optional","title":"Step 6: Add Persistence (Optional)","text":"<p>If the application needs persistent storage:</p> <pre><code>persistence:\n  config:\n    enabled: true\n    type: persistentVolumeClaim\n    accessMode: ReadWriteOnce\n    size: 1Gi\n    storageClass: ceph-block\n    globalMounts:\n      - path: /config\n</code></pre> <p>Available storage classes:</p> Storage Class Backend Access Modes Use Case <code>ceph-block</code> Rook Ceph RBD ReadWriteOnce General purpose, databases <code>ceph-filesystem</code> Rook CephFS ReadWriteMany Shared storage across pods <code>openebs-hostpath</code> OpenEBS ReadWriteOnce Local node storage"},{"location":"development/adding-apps/#step-7-add-secrets-optional","title":"Step 7: Add Secrets (Optional)","text":"<p>If your application needs secrets, create an ExternalSecret to pull from 1Password:</p> pitower/kubernetes/apps///externalsecret.yaml<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: &lt;app-name&gt;-secrets\n  namespace: &lt;category&gt;\nspec:\n  refreshInterval: 5m\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: onepassword\n  target:\n    name: &lt;app-name&gt;-secrets\n    creationPolicy: Owner\n  data:\n    - secretKey: DB_PASSWORD\n      remoteRef:\n        key: &lt;app-name&gt;\n        property: DB_PASSWORD\n</code></pre> <p>Then reference it in <code>kustomization.yaml</code>:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: &lt;category&gt;\nresources:\n  - externalsecret.yaml\nhelmCharts:\n  - name: app-template\n    repo: oci://ghcr.io/bjw-s-labs/helm\n    version: 4.6.2\n    releaseName: &lt;app-name&gt;\n    namespace: &lt;category&gt;\n    valuesFile: values.yaml\n</code></pre> <p>And reference the secret in <code>values.yaml</code>:</p> <pre><code>envFrom:\n  - secretRef:\n      name: &lt;app-name&gt;-secrets\n</code></pre>"},{"location":"development/adding-apps/#step-8-validate-locally","title":"Step 8: Validate Locally","text":"<p>Test that your manifests render correctly:</p> <pre><code>cd pitower/kubernetes/apps/&lt;category&gt;/&lt;app-name&gt;\nkustomize build . --enable-helm\n</code></pre> <p>This outputs the full rendered Kubernetes manifests. Check for errors and verify the output looks correct.</p> <p>For a server-side dry run:</p> <pre><code>kustomize build . --enable-helm | kubectl apply --dry-run=server -f -\n</code></pre>"},{"location":"development/adding-apps/#step-9-push-to-main","title":"Step 9: Push to Main","text":"<p>Commit and push your changes to the <code>main</code> branch:</p> <pre><code>git add pitower/kubernetes/apps/&lt;category&gt;/&lt;app-name&gt;/\ngit commit -m \"feat(&lt;category&gt;): add &lt;app-name&gt;\"\ngit push origin main\n</code></pre> <p>PR Workflow</p> <p>For non-trivial additions, create a feature branch and open a pull request. This triggers the lint workflow and allows for code review before deployment.</p>"},{"location":"development/adding-apps/#step-10-verify-deployment","title":"Step 10: Verify Deployment","text":"<p>ArgoCD will auto-discover the new application and begin syncing:</p> <pre><code># Check ArgoCD application status\nargocd app list | grep &lt;app-name&gt;\n\n# Check pods\nkubectl get pods -n &lt;category&gt; -l app.kubernetes.io/name=&lt;app-name&gt;\n\n# Check service\nkubectl get svc -n &lt;category&gt; -l app.kubernetes.io/name=&lt;app-name&gt;\n\n# Check HTTPRoute\nkubectl get httproutes -n &lt;category&gt;\n</code></pre>"},{"location":"development/adding-apps/#complete-example","title":"Complete Example","text":"<p>Here is a complete example for adding a new app called \"linkding\" (a bookmarks manager) to the <code>selfhosted</code> category.</p>"},{"location":"development/adding-apps/#directory-structure","title":"Directory structure","text":"<pre><code>pitower/kubernetes/apps/selfhosted/linkding/\n\u251c\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 values.yaml\n</code></pre>"},{"location":"development/adding-apps/#kustomizationyaml","title":"kustomization.yaml","text":"<pre><code>---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: selfhosted\nhelmCharts:\n  - name: app-template\n    repo: oci://ghcr.io/bjw-s-labs/helm\n    version: 4.6.2\n    releaseName: linkding\n    namespace: selfhosted\n    valuesFile: values.yaml\n</code></pre>"},{"location":"development/adding-apps/#valuesyaml","title":"values.yaml","text":"<pre><code>controllers:\n  linkding:\n    annotations:\n      reloader.stakater.com/auto: \"true\"\n    containers:\n      app:\n        image:\n          repository: sissbruecker/linkding\n          tag: 1.25.0\n        env:\n          LD_SUPERUSER_NAME: admin\n          LD_SERVER_PORT: \"9090\"\n        envFrom:\n          - secretRef:\n              name: linkding-secrets\n        resources:\n          requests:\n            cpu: 10m\n            memory: 64Mi\n          limits:\n            memory: 256Mi\n        probes:\n          liveness:\n            enabled: true\n          readiness:\n            enabled: true\n          startup:\n            enabled: true\n            spec:\n              failureThreshold: 30\n              periodSeconds: 5\nservice:\n  app:\n    ports:\n      http:\n        port: 9090\nroute:\n  app:\n    enabled: true\n    hostnames:\n      - linkding.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\npersistence:\n  data:\n    enabled: true\n    type: persistentVolumeClaim\n    accessMode: ReadWriteOnce\n    size: 1Gi\n    storageClass: ceph-block\n    globalMounts:\n      - path: /etc/linkding/data\n</code></pre>"},{"location":"development/adding-apps/#checklist","title":"Checklist","text":"<p>Before pushing your new application:</p> <ul> <li> <code>kustomization.yaml</code> has correct namespace and chart version</li> <li> <code>values.yaml</code> has resource requests and limits set</li> <li> Health probes are configured</li> <li> <code>reloader.stakater.com/auto: \"true\"</code> annotation is set</li> <li> HTTPRoute points to the correct gateway</li> <li> Secrets are managed via ExternalSecret (no plaintext)</li> <li> <code>kustomize build . --enable-helm</code> renders without errors</li> <li> Hostname follows <code>&lt;app&gt;.example.com</code> convention</li> </ul>"},{"location":"development/app-template/","title":"App Template","text":"<p>The bjw-s app-template Helm chart is used for most applications in the cluster. It provides a standardized, opinionated structure for deploying containerized workloads.</p> <p>Current version: 4.6.2</p>"},{"location":"development/app-template/#overview","title":"Overview","text":"<p>The app-template chart abstracts common Kubernetes patterns (Deployments, Services, Ingress/HTTPRoutes, PVCs) into a declarative <code>values.yaml</code> format. Instead of writing raw Kubernetes manifests, you define controllers, containers, services, and routes.</p> <pre><code>flowchart TD\n    Values[values.yaml] --&gt; Chart[app-template chart]\n    Chart --&gt; Deploy[Deployment]\n    Chart --&gt; Svc[Service]\n    Chart --&gt; Route[HTTPRoute]\n    Chart --&gt; PVC[PersistentVolumeClaim]\n    Chart --&gt; CM[ConfigMap]\n    Chart --&gt; SA[ServiceAccount]</code></pre>"},{"location":"development/app-template/#chart-reference","title":"Chart Reference","text":"<p>The chart is referenced in <code>kustomization.yaml</code> using OCI:</p> kustomization.yaml<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: selfhosted\nhelmCharts:\n  - name: app-template\n    repo: oci://ghcr.io/bjw-s-labs/helm\n    version: 4.6.2\n    releaseName: my-app\n    namespace: selfhosted\n    valuesFile: values.yaml\n</code></pre>"},{"location":"development/app-template/#values-structure","title":"Values Structure","text":""},{"location":"development/app-template/#minimal-example","title":"Minimal Example","text":"<p>A minimal application with a single container, service, and HTTP route:</p> values.yaml<pre><code>controllers:\n  my-app:\n    annotations:\n      reloader.stakater.com/auto: \"true\"\n    containers:\n      app:\n        image:\n          repository: ghcr.io/example/my-app\n          tag: 1.0.0\n        env:\n          HTTP_PORT: 8080\n        resources:\n          requests:\n            cpu: 10m\n            memory: 64Mi\n          limits:\n            memory: 128Mi\n        probes:\n          liveness:\n            enabled: true\n          readiness:\n            enabled: true\n          startup:\n            enabled: true\n            spec:\n              failureThreshold: 30\n              periodSeconds: 5\nservice:\n  app:\n    ports:\n      http:\n        port: 8080\nroute:\n  app:\n    enabled: true\n    hostnames:\n      - my-app.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\n</code></pre>"},{"location":"development/app-template/#controllers","title":"Controllers","text":"<p>Controllers define the workload type (Deployment by default) and its containers.</p> <pre><code>controllers:\n  my-app:\n    # Optional: set replicas\n    replicas: 1\n\n    # Optional: set strategy\n    strategy: Recreate\n\n    # Annotations applied to the pod template\n    annotations:\n      reloader.stakater.com/auto: \"true\"\n\n    # Pod-level settings\n    pod:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n\n    containers:\n      app:\n        image:\n          repository: ghcr.io/example/my-app\n          tag: 1.0.0\n        env:\n          TZ: Europe/Zurich\n          PORT: \"8080\"\n        envFrom:\n          - secretRef:\n              name: my-app-secrets\n        resources:\n          requests:\n            cpu: 10m\n            memory: 64Mi\n          limits:\n            memory: 256Mi\n</code></pre>"},{"location":"development/app-template/#multiple-containers-sidecars","title":"Multiple Containers (Sidecars)","text":"<pre><code>controllers:\n  my-app:\n    containers:\n      app:\n        image:\n          repository: ghcr.io/example/my-app\n          tag: 1.0.0\n      sidecar:\n        image:\n          repository: ghcr.io/example/sidecar\n          tag: 2.0.0\n        args:\n          - --config=/etc/sidecar/config.yaml\n</code></pre>"},{"location":"development/app-template/#services","title":"Services","text":"<p>Define Kubernetes Services that expose container ports:</p> <pre><code>service:\n  app:\n    controller: my-app  # Links to the controller (optional if name matches)\n    ports:\n      http:\n        port: 8080\n      metrics:\n        port: 9090\n</code></pre>"},{"location":"development/app-template/#multiple-services","title":"Multiple Services","text":"<pre><code>service:\n  app:\n    ports:\n      http:\n        port: 8080\n  mqtt:\n    type: LoadBalancer\n    ports:\n      mqtt:\n        port: 1883\n</code></pre>"},{"location":"development/app-template/#routes-httproute","title":"Routes (HTTPRoute)","text":"<p>Routes configure Gateway API HTTPRoutes for ingress:</p>"},{"location":"development/app-template/#external-route-cloudflare-proxied","title":"External Route (Cloudflare-proxied)","text":"<pre><code>route:\n  app:\n    enabled: true\n    hostnames:\n      - my-app.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\n</code></pre>"},{"location":"development/app-template/#internal-route-lanvpn-only","title":"Internal Route (LAN/VPN only)","text":"<pre><code>route:\n  app:\n    enabled: true\n    hostnames:\n      - my-app.internal.example.com\n    parentRefs:\n      - name: envoy-internal\n        namespace: networking\n        sectionName: https\n</code></pre> <p>Annotation Placement</p> <p>The <code>cloudflare-proxied</code> annotation is read from the Route resource, not the Gateway. The <code>external-dns.alpha.kubernetes.io/target</code> annotation only works on Gateway resources, not on HTTPRoutes.</p>"},{"location":"development/app-template/#persistence","title":"Persistence","text":"<p>Define persistent storage for your application:</p>"},{"location":"development/app-template/#pvc-rook-ceph","title":"PVC (Rook Ceph)","text":"<pre><code>persistence:\n  data:\n    enabled: true\n    type: persistentVolumeClaim\n    accessMode: ReadWriteOnce\n    size: 5Gi\n    storageClass: ceph-block\n    globalMounts:\n      - path: /data\n</code></pre>"},{"location":"development/app-template/#existing-pvc","title":"Existing PVC","text":"<pre><code>persistence:\n  config:\n    enabled: true\n    type: persistentVolumeClaim\n    existingClaim: my-app-config\n    globalMounts:\n      - path: /config\n</code></pre>"},{"location":"development/app-template/#emptydir","title":"EmptyDir","text":"<pre><code>persistence:\n  tmp:\n    type: emptyDir\n    globalMounts:\n      - path: /tmp\n</code></pre>"},{"location":"development/app-template/#configmap-mount","title":"ConfigMap Mount","text":"<pre><code>persistence:\n  config:\n    type: configMap\n    name: my-app-config\n    globalMounts:\n      - path: /config/app.yaml\n        subPath: app.yaml\n        readOnly: true\n</code></pre>"},{"location":"development/app-template/#secret-mount","title":"Secret Mount","text":"<pre><code>persistence:\n  secrets:\n    type: secret\n    name: my-app-secrets\n    globalMounts:\n      - path: /secrets\n        readOnly: true\n</code></pre>"},{"location":"development/app-template/#health-probes","title":"Health Probes","text":"<p>Always configure health probes for production workloads:</p> <pre><code>probes:\n  liveness:\n    enabled: true\n    custom: true\n    spec:\n      httpGet:\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 10\n      periodSeconds: 30\n  readiness:\n    enabled: true\n  startup:\n    enabled: true\n    spec:\n      failureThreshold: 30\n      periodSeconds: 5\n</code></pre> <p>Startup Probes</p> <p>Use startup probes with a high <code>failureThreshold</code> for applications that take a long time to initialize. This prevents the liveness probe from killing the container during startup.</p>"},{"location":"development/app-template/#environment-variables","title":"Environment Variables","text":""},{"location":"development/app-template/#static-environment-variables","title":"Static Environment Variables","text":"<pre><code>env:\n  TZ: Europe/Zurich\n  LOG_LEVEL: info\n  HTTP_PORT: \"8080\"\n</code></pre>"},{"location":"development/app-template/#from-secrets","title":"From Secrets","text":"<pre><code>envFrom:\n  - secretRef:\n      name: my-app-secrets\n</code></pre>"},{"location":"development/app-template/#from-configmap","title":"From ConfigMap","text":"<pre><code>envFrom:\n  - configMapRef:\n      name: my-app-config\n</code></pre>"},{"location":"development/app-template/#value-from-field-reference","title":"Value From (Field Reference)","text":"<pre><code>env:\n  POD_NAME:\n    valueFrom:\n      fieldRef:\n        fieldPath: metadata.name\n</code></pre>"},{"location":"development/app-template/#full-example","title":"Full Example","text":"<p>A complete example showing all common patterns:</p> values.yaml<pre><code>controllers:\n  tandoor:\n    annotations:\n      reloader.stakater.com/auto: \"true\"\n    containers:\n      app:\n        image:\n          repository: ghcr.io/tandoorrecipes/recipes\n          tag: 1.5.0\n        env:\n          TZ: Europe/Zurich\n          DB_ENGINE: django.db.backends.postgresql\n          POSTGRES_HOST: postgres-rw.cloudnative-pg.svc.cluster.local\n          POSTGRES_PORT: \"5432\"\n        envFrom:\n          - secretRef:\n              name: tandoor-secrets\n        resources:\n          requests:\n            cpu: 50m\n            memory: 256Mi\n          limits:\n            memory: 512Mi\n        probes:\n          liveness:\n            enabled: true\n          readiness:\n            enabled: true\n          startup:\n            enabled: true\n            spec:\n              failureThreshold: 30\n              periodSeconds: 5\nservice:\n  app:\n    ports:\n      http:\n        port: 8080\nroute:\n  app:\n    enabled: true\n    hostnames:\n      - recipes.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\npersistence:\n  data:\n    enabled: true\n    type: persistentVolumeClaim\n    accessMode: ReadWriteOnce\n    size: 2Gi\n    storageClass: ceph-block\n    globalMounts:\n      - path: /opt/recipes/mediafiles\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to the home lab documentation. This section will help you understand what this repository contains, how it is organized, and how to get up and running.</p>"},{"location":"getting-started/#what-this-repository-contains","title":"What This Repository Contains","text":"<p>The home-ops repository is the single source of truth for the Kubernetes cluster. Everything from the operating system configuration to application deployments is declared in code and managed through GitOps.</p> <p>The repository includes:</p> <ul> <li>Talos Linux machine configurations -- patches, secrets, and extension definitions for every node in the cluster</li> <li>Kubernetes manifests -- organized by category (networking, media, security, monitoring, and more), deployed via ArgoCD ApplicationSets</li> <li>ArgoCD definitions -- the ApplicationSet resources that wire each app category to the cluster</li> <li>Bootstrap resources -- initial resources needed to bring the cluster from zero to a running state</li> <li>Documentation -- this MkDocs Material site, covering architecture, operations, and reference material</li> </ul>"},{"location":"getting-started/#how-to-browse-the-documentation","title":"How to Browse the Documentation","text":"<p>The documentation is organized into sections that mirror the layers of the infrastructure:</p> Section What you will find Getting Started Prerequisites, architecture overview, and this orientation page Infrastructure Hardware inventory, Talos Linux configuration, cluster bootstrap, and node management Networking Cilium CNI, Envoy Gateway, DNS management, Cloudflare Tunnel, Tailscale VPN, and load balancers GitOps ArgoCD setup, ApplicationSet patterns, sync policies, and how to add new apps Storage Rook Ceph distributed storage, OpenEBS local volumes, and backup/restore procedures Security Authentication (Authelia, LLDAP), secret management (SOPS, External Secrets, 1Password Connect), and TLS certificates Monitoring Prometheus stack, Grafana dashboards, Loki log aggregation, and Fluent Bit Applications Media stack (Jellyfin, *arr apps), home automation (Home Assistant, Zigbee2MQTT), and self-hosted services Operations Day-to-day tasks: justfile recipes, Talos commands, troubleshooting guides, and upgrade procedures CI/CD GitHub Actions workflows, container image builds, and Renovate dependency management Reference IP allocation table and a full catalog of deployed applications <p>Navigation</p> <p>Use the tabs at the top to jump between major sections, or use the search bar to find specific topics. The sidebar expands to show all pages within the current section.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Prerequisites</p> <p>Tools, access, and network requirements you need before working with this cluster.</p> <p> Prerequisites</p> </li> <li> <p>Architecture Overview</p> <p>Understand the full stack from hardware to applications, including network traffic flows.</p> <p> Architecture Overview</p> </li> </ul>"},{"location":"getting-started/architecture-overview/","title":"Architecture Overview","text":"<p>This page describes the full architecture of the cluster, from physical hardware through to application deployment. Two key diagrams illustrate the infrastructure stack and the network traffic flows.</p>"},{"location":"getting-started/architecture-overview/#infrastructure-stack","title":"Infrastructure Stack","text":"<p>The cluster is built in layers, each managed declaratively through code in this repository.</p> <pre><code>flowchart TB\n    subgraph Hardware[\"Hardware Layer\"]\n        direction LR\n        RPi4[\"Raspberry Pi 4\\nx4\"]\n        Lenovo[\"Lenovo T440p\\nx2\"]\n        Ace[\"Acemagician AM06\\nx3\"]\n        RPi3[\"Raspberry Pi 3B+\\nx1\"]\n    end\n\n    subgraph OS[\"Operating System\"]\n        Talos[\"Talos Linux v1.12.4\\nImmutable, API-driven\"]\n    end\n\n    subgraph K8s[\"Kubernetes Layer\"]\n        direction LR\n        Cilium[\"Cilium CNI\\neBPF / L2 / DSR\"]\n        CoreDNS[\"CoreDNS\"]\n        Metrics[\"Metrics Server\"]\n    end\n\n    subgraph GitOps[\"GitOps Layer\"]\n        direction LR\n        ArgoCD[\"ArgoCD\"]\n        AppSets[\"ApplicationSets\\nper category\"]\n    end\n\n    subgraph Apps[\"Application Categories\"]\n        direction LR\n        Net[\"networking\"]\n        Media[\"media\"]\n        HA[\"home-automation\"]\n        Mon[\"monitoring\"]\n        Sec[\"security\"]\n        Self[\"selfhosted\"]\n        Sys[\"system\"]\n        More[\"ai, banking,\\ncert-manager,\\ncloudnative-pg,\\nkube-system,\\nopenebs, rook-ceph\"]\n    end\n\n    Hardware --&gt; OS\n    OS --&gt; K8s\n    K8s --&gt; GitOps\n    GitOps --&gt; Apps</code></pre>"},{"location":"getting-started/architecture-overview/#layer-descriptions","title":"Layer Descriptions","text":""},{"location":"getting-started/architecture-overview/#hardware-layer","title":"Hardware Layer","text":"<p>The cluster runs on a mix of ARM64 and AMD64 hardware. Raspberry Pi 4 boards serve as control plane nodes and lightweight workers. Lenovo ThinkPad T440p laptops and Acemagician AM06 mini-PCs provide AMD64 compute capacity, with the AM06 units contributing 512 GB NVMe drives for Ceph distributed storage. A TP-Link 24-port PoE switch powers the Pi nodes, and an Eaton 500VA UPS protects core infrastructure from power outages.</p>"},{"location":"getting-started/architecture-overview/#operating-system","title":"Operating System","text":"<p>All nodes run Talos Linux v1.12.4, an immutable, minimal Linux distribution purpose-built for Kubernetes. There is no SSH access, no shell, and no package manager. All configuration is applied through the Talos API using <code>talosctl</code>. Machine configs are generated from patches stored in <code>pitower/talos/patches/</code> and applied per node.</p>"},{"location":"getting-started/architecture-overview/#kubernetes-layer","title":"Kubernetes Layer","text":"<p>The Kubernetes cluster uses Cilium as the CNI, fully replacing kube-proxy with eBPF datapath. Cilium is configured with:</p> <ul> <li>L2 announcements for LoadBalancer IP allocation (<code>192.168.0.220-239</code>)</li> <li>Direct Server Return (DSR) for efficient load balancing</li> <li>Maglev consistent hashing for connection affinity</li> </ul> <p>CoreDNS handles in-cluster DNS, and Metrics Server provides resource utilization data.</p>"},{"location":"getting-started/architecture-overview/#gitops-layer","title":"GitOps Layer","text":"<p>ArgoCD is the sole deployment mechanism. An <code>ApplicationSet</code> resource exists for each app category (e.g., <code>appset-networking.yaml</code>, <code>appset-media.yaml</code>), which automatically discovers and deploys all applications within that category directory. This means adding a new app is as simple as creating a new directory under the appropriate category in <code>pitower/kubernetes/apps/</code>.</p>"},{"location":"getting-started/architecture-overview/#application-layer","title":"Application Layer","text":"<p>Applications are organized into 14 categories:</p> Category Example Applications <code>ai</code> kagent <code>banking</code> -- <code>cert-manager</code> cert-manager <code>cloudnative-pg</code> CloudNativePG operator <code>home-automation</code> Home Assistant, Zigbee2MQTT, Mosquitto, Matter Server, OTBR <code>kube-system</code> Cilium, CoreDNS, metrics-server <code>media</code> Jellyfin, Sonarr, Radarr, Prowlarr, qBittorrent, SABnzbd, Autobrr <code>monitoring</code> kube-prometheus-stack, Grafana, Loki, Fluent Bit <code>networking</code> Envoy Gateway, external-dns, cloudflared, nginx, Tailscale <code>openebs</code> OpenEBS local volumes <code>rook-ceph</code> Rook Ceph distributed storage <code>security</code> Authelia, LLDAP, External Secrets, 1Password Connect <code>selfhosted</code> Miniflux, n8n, Excalidraw, Glance, Homepage, Tandoor, and more <code>system</code> Reloader, VolSync, Node Feature Discovery, snapshot-controller"},{"location":"getting-started/architecture-overview/#network-architecture","title":"Network Architecture","text":"<p>Traffic reaches the cluster through two distinct paths depending on the source and intended audience.</p> <pre><code>flowchart TB\n    subgraph External[\"External Traffic Path\"]\n        direction LR\n        Internet1((Internet))\n        CF[\"Cloudflare\\nDNS + Proxy\"]\n        Tunnel[\"cloudflared\\nTunnel Pod\"]\n        Nginx[\"nginx\\nReverse Proxy\\n192.168.0.231\"]\n        EnvoyExt[\"Envoy External\\nGateway\\n192.168.0.239\"]\n    end\n\n    subgraph Internal[\"Internal / VPN Traffic Path\"]\n        direction LR\n        User((User))\n        TS[\"Tailscale\\nVPN\"]\n        LAN[\"Local\\nNetwork\"]\n        EnvoyInt[\"Envoy Internal\\nGateway\\n192.168.0.238\"]\n    end\n\n    AppPods[\"Application Pods\"]\n\n    Internet1 --&gt;|\"*.example.com\\n(proxied)\"| CF\n    CF --&gt; Tunnel\n    Tunnel --&gt; Nginx\n    Nginx --&gt; EnvoyExt\n    EnvoyExt --&gt; AppPods\n\n    User --&gt;|Remote| TS\n    TS --&gt; EnvoyInt\n    User --&gt;|\"LAN\\n192.168.0.0/24\"| LAN\n    LAN --&gt; EnvoyInt\n    EnvoyInt --&gt; AppPods</code></pre>"},{"location":"getting-started/architecture-overview/#external-traffic-cloudflare-tunnel","title":"External Traffic (Cloudflare Tunnel)","text":"<p>Public-facing services are exposed through Cloudflare's proxy network. DNS records for <code>*.example.com</code> point to Cloudflare, which routes traffic through a <code>cloudflared</code> tunnel pod running in the cluster. The tunnel terminates at an nginx reverse proxy, which forwards to the <code>envoy-external</code> gateway at <code>192.168.0.239</code>. This path provides DDoS protection, caching, and hides the origin IP.</p> <p>No port forwarding required</p> <p>The Cloudflare Tunnel creates an outbound connection from the cluster to Cloudflare's edge, so no inbound firewall rules or port forwarding is needed on the home router.</p>"},{"location":"getting-started/architecture-overview/#internal-vpn-traffic","title":"Internal / VPN Traffic","text":"<p>Internal services are accessed either from the local network (<code>192.168.0.0/24</code>) or remotely through Tailscale VPN. Both paths route through the <code>envoy-internal</code> gateway at <code>192.168.0.238</code>, resolving as <code>internal.example.com</code>. These services are never exposed to the public internet.</p>"},{"location":"getting-started/architecture-overview/#gateway-architecture","title":"Gateway Architecture","text":"<p>The two Envoy Gateway instances serve different audiences and have distinct configurations:</p> Gateway IP Address Domain Target Audience DNS Proxy <code>envoy-external</code> <code>192.168.0.239</code> <code>external.example.com</code> Public (via Cloudflare) Cloudflare proxied <code>envoy-internal</code> <code>192.168.0.238</code> <code>internal.example.com</code> LAN and VPN users Not proxied <p>Cilium L2 announcements advertise the gateway IPs on the local network. The <code>external-dns</code> controller watches for Gateway and HTTPRoute resources with the label <code>external-dns.alpha.kubernetes.io/enabled=true</code> and automatically creates or updates DNS records in Cloudflare.</p> <p>Routing an app to a specific gateway</p> <p>To control which gateway serves an application, set the <code>parentRefs</code> field in the HTTPRoute to reference the desired gateway (<code>envoy-external</code> for public services or <code>envoy-internal</code> for internal-only access).</p>"},{"location":"getting-started/architecture-overview/#storage-architecture","title":"Storage Architecture","text":"<pre><code>flowchart LR\n    subgraph Distributed[\"Distributed Storage\"]\n        NVMe1[\"AM06 #1\\n512GB NVMe\"]\n        NVMe2[\"AM06 #2\\n512GB NVMe\"]\n        NVMe3[\"AM06 #3\\n512GB NVMe\"]\n        Ceph[\"Rook Ceph\\nCluster\"]\n        NVMe1 --&gt; Ceph\n        NVMe2 --&gt; Ceph\n        NVMe3 --&gt; Ceph\n    end\n\n    subgraph Local[\"Local Storage\"]\n        SSD[\"128GB SSD\\nBoot Drives\"]\n        OpenEBS[\"OpenEBS\\nLocal PV\"]\n        SSD --&gt; OpenEBS\n    end\n\n    subgraph External[\"External Storage\"]\n        Synology[\"Synology NAS\\n4-Bay 8TB\"]\n    end\n\n    PVC[\"Persistent\\nVolume Claims\"]\n    Ceph --&gt; PVC\n    OpenEBS --&gt; PVC\n    Synology --&gt;|NFS| PVC</code></pre> <ul> <li>Rook Ceph provides replicated block storage across three Acemagician AM06 nodes, each contributing a 512 GB NVMe drive as a Ceph OSD. This is used for workloads that need high availability and data replication.</li> <li>OpenEBS provides local persistent volumes backed by the 128 GB SSD boot drives. This is used for workloads that benefit from local-disk performance and do not require replication.</li> <li>Synology NAS provides NFS-backed volumes for bulk storage (media files, backups), accessed over the local network.</li> </ul>"},{"location":"getting-started/prerequisites/","title":"Prerequisites","text":"<p>Before working with the cluster, ensure you have the required tools installed, the necessary access credentials, and network connectivity to the cluster.</p>"},{"location":"getting-started/prerequisites/#cli-tools","title":"CLI Tools","text":"<p>The following command-line tools are required to manage and interact with the cluster:</p> Tool Purpose Install <code>talosctl</code> Manage Talos Linux nodes (apply configs, upgrade, reboot) talos.dev/install <code>kubectl</code> Interact with the Kubernetes API kubernetes.io/docs <code>argocd</code> ArgoCD CLI for app management and sync operations argo-cd.readthedocs.io <code>sops</code> Encrypt and decrypt secrets in the repository github.com/getsops/sops <code>age</code> Encryption backend used by SOPS for secret management github.com/FiloSottile/age <code>just</code> Task runner for executing justfile recipes github.com/casey/just <code>kustomize</code> Build and customize Kubernetes manifests kubectl.docs.kubernetes.io <p>Quick install with Homebrew</p> <p>On macOS, all tools can be installed via Homebrew:</p> <pre><code>brew install siderolabs/tap/talosctl \\\n             kubectl \\\n             argocd \\\n             sops \\\n             age \\\n             just \\\n             kustomize\n</code></pre>"},{"location":"getting-started/prerequisites/#optional-but-recommended","title":"Optional but Recommended","text":"Tool Purpose <code>jq</code> JSON processing, used in several justfile recipes <code>yq</code> YAML processing for manifest inspection <code>helm</code> Helm chart management (charts are rendered via kustomize, but useful for debugging) <code>gh</code> GitHub CLI for interacting with the repository <code>tailscale</code> VPN client for remote access to internal services"},{"location":"getting-started/prerequisites/#access-requirements","title":"Access Requirements","text":""},{"location":"getting-started/prerequisites/#github-repository","title":"GitHub Repository","text":"<p>You need read access to the swibrow/home-ops repository. Write access is required if you intend to make changes and trigger GitOps deployments.</p>"},{"location":"getting-started/prerequisites/#cloudflare-api-token","title":"Cloudflare API Token","text":"<p>A Cloudflare API token is required for:</p> <ul> <li>external-dns -- automated DNS record management for <code>example.com</code></li> <li>cert-manager -- DNS-01 challenge solving for TLS certificates</li> <li>cloudflared -- tunnel authentication for external access</li> </ul> <p>Token scope</p> <p>The token must have <code>Zone:DNS:Edit</code> permissions for the <code>example.com</code> zone. Store it as an encrypted secret in the repository using SOPS.</p>"},{"location":"getting-started/prerequisites/#1password-connect","title":"1Password Connect","text":"<p>The cluster uses 1Password Connect as a secrets backend via External Secrets Operator. You need:</p> <ul> <li>Access to the 1Password vault that stores cluster secrets</li> <li>The 1Password Connect credentials (<code>1password-credentials.json</code>) and a connect token</li> </ul>"},{"location":"getting-started/prerequisites/#sops-age-key","title":"SOPS / age Key","text":"<p>Secrets in the repository are encrypted with SOPS using an age key. To decrypt or edit secrets, you need the age private key:</p> <pre><code># Export the key so SOPS can find it\nexport SOPS_AGE_KEY_FILE=~/.config/sops/age/keys.txt\n</code></pre> <p>Keep the age key safe</p> <p>The age private key is the master key for all encrypted secrets in the repository. Never commit it to version control.</p>"},{"location":"getting-started/prerequisites/#network-access","title":"Network Access","text":""},{"location":"getting-started/prerequisites/#local-network","title":"Local Network","text":"<p>The cluster runs on the <code>192.168.0.0/24</code> subnet. You must be on this network (or connected via Tailscale VPN) to reach:</p> Endpoint Address Purpose Talos API <code>192.168.0.201</code> -- <code>192.168.0.204</code> Control plane and worker node management Kubernetes API <code>192.168.0.200:6443</code> kubectl access (VIP) Envoy External <code>192.168.0.239</code> External gateway (Cloudflare tunnel target) Envoy Internal <code>192.168.0.238</code> Internal gateway (LAN / VPN access) ArgoCD Available via internal gateway GitOps dashboard"},{"location":"getting-started/prerequisites/#remote-access","title":"Remote Access","text":"<p>For remote access without being on the local network:</p> <ul> <li>Tailscale VPN -- connects you to the internal network and the <code>envoy-internal</code> gateway</li> <li>Cloudflare Tunnel -- exposes selected services via <code>*.example.com</code> through the external gateway</li> </ul>"},{"location":"getting-started/prerequisites/#verify-your-setup","title":"Verify Your Setup","text":"<p>Once all tools are installed and credentials are in place, verify connectivity:</p> <pre><code># Check Talos API connectivity\ntalosctl version --nodes 192.168.0.201\n\n# Check Kubernetes API connectivity\nkubectl cluster-info\n\n# Verify SOPS can decrypt secrets\nsops -d pitower/talos/secrets.sops.yaml &gt; /dev/null &amp;&amp; echo \"SOPS decryption OK\"\n\n# Check ArgoCD connectivity\nargocd app list\n</code></pre> <p>Ready to go</p> <p>If all four commands succeed, you are ready to work with the cluster. Head to the Architecture Overview to understand how everything fits together.</p>"},{"location":"gitops/","title":"GitOps","text":"<p>The cluster is managed entirely through GitOps using ArgoCD. Every application, infrastructure component, and configuration change flows through a single Git repository. ArgoCD continuously watches the repository and reconciles the cluster state to match what is declared in code.</p>"},{"location":"gitops/#how-it-works","title":"How It Works","text":"<p>The GitOps workflow follows a simple, predictable loop:</p> <ol> <li>A developer pushes a change to the <code>main</code> branch of the home-ops repository.</li> <li>ArgoCD detects the new commit on <code>main</code>.</li> <li>ApplicationSets evaluate the repository directory structure and generate Application resources for each discovered app.</li> <li>Each Application syncs its manifests to the cluster, creating or updating Kubernetes resources as needed.</li> </ol> <pre><code>flowchart LR\n    Dev((Developer)) --&gt;|git push| GH[GitHub\\nmain branch]\n    GH --&gt;|watches| ArgoCD[ArgoCD\\nController]\n    ArgoCD --&gt;|evaluates| AppSets[ApplicationSets\\n15 categories]\n    AppSets --&gt;|generates| Apps[Application\\nResources]\n    Apps --&gt;|syncs| Cluster[Kubernetes\\nCluster]\n\n    style Dev fill:#7c3aed,color:#fff\n    style GH fill:#333,color:#fff\n    style ArgoCD fill:#ef652a,color:#fff\n    style AppSets fill:#18b7be,color:#fff\n    style Apps fill:#18b7be,color:#fff\n    style Cluster fill:#326ce5,color:#fff</code></pre>"},{"location":"gitops/#key-principles","title":"Key Principles","text":"Principle Implementation Single source of truth All cluster state is declared in the <code>home-ops</code> Git repository Declarative configuration Kubernetes manifests and Helm values define desired state, not imperative scripts Automated reconciliation ArgoCD continuously syncs changes from Git to the cluster Pull-based delivery The cluster pulls its own state from Git -- no external CI pushing to the cluster Auditability Every change is a Git commit with full history and attribution"},{"location":"gitops/#repository-layout","title":"Repository Layout","text":"<p>ArgoCD manages the cluster through three key directories:</p> <pre><code>pitower/kubernetes/\n\u251c\u2500\u2500 argocd/              # ArgoCD Application + ApplicationSet definitions\n\u2502   \u251c\u2500\u2500 app-argocd.yaml  # Bootstrap Application (self-managing)\n\u2502   \u251c\u2500\u2500 appset-networking.yaml\n\u2502   \u251c\u2500\u2500 appset-media.yaml\n\u2502   \u251c\u2500\u2500 appset-security.yaml\n\u2502   \u2514\u2500\u2500 ...              # 15 ApplicationSets total\n\u251c\u2500\u2500 bootstrap/           # Initial cluster resources (ArgoCD Helm chart, project, namespace)\n\u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u251c\u2500\u2500 appproject.yaml\n\u2502   \u251c\u2500\u2500 namespace.yaml\n\u2502   \u2514\u2500\u2500 argocd-values.yaml\n\u2514\u2500\u2500 apps/                # Application manifests organized by category\n    \u251c\u2500\u2500 ai/\n    \u251c\u2500\u2500 banking/\n    \u251c\u2500\u2500 cert-manager/\n    \u251c\u2500\u2500 cloudnative-pg/\n    \u251c\u2500\u2500 home-automation/\n    \u251c\u2500\u2500 kube-system/\n    \u251c\u2500\u2500 media/\n    \u251c\u2500\u2500 monitoring/\n    \u251c\u2500\u2500 networking/\n    \u251c\u2500\u2500 openebs/\n    \u251c\u2500\u2500 rook-ceph/\n    \u251c\u2500\u2500 security/\n    \u251c\u2500\u2500 selfhosted/\n    \u2514\u2500\u2500 system/\n</code></pre> <p>15 Application Categories</p> <p>Each category directory under <code>apps/</code> has a corresponding ApplicationSet in <code>argocd/</code>. When you add a new subdirectory to any category, ArgoCD automatically creates and syncs a new Application for it.</p>"},{"location":"gitops/#sections","title":"Sections","text":"Page Description ArgoCD Setup Bootstrap process, self-managing Application, project configuration ApplicationSets Git directory generator pattern, Go templates, naming conventions Sync Policies Automated sync, prune, selfHeal, retry strategy, and syncOptions Adding Apps Step-by-step guide to deploying a new application through GitOps"},{"location":"gitops/#design-decisions","title":"Design Decisions","text":"<ul> <li>ArgoCD over Flux -- ArgoCD was chosen for its mature UI, ApplicationSet pattern, and straightforward Helm/Kustomize integration.</li> <li>ApplicationSets over individual Applications -- A single ApplicationSet per category eliminates boilerplate. Adding a new app is as simple as creating a directory.</li> <li>Git directory generator -- Directory structure drives application discovery. No manual Application YAML is needed for each app.</li> <li>selfHeal disabled -- Manual intervention is preferred over automatic drift correction, giving operators time to investigate before changes are reverted. See Sync Policies for the rationale.</li> <li>Kustomize with HelmChartInflationGenerator -- Most apps use <code>kustomization.yaml</code> to inflate Helm charts with local <code>values.yaml</code> files, combining the flexibility of Helm with the composability of Kustomize.</li> </ul>"},{"location":"gitops/adding-apps/","title":"Adding a New Application","text":"<p>Adding a new application to the cluster requires no ArgoCD configuration changes. Thanks to the ApplicationSet pattern, creating a directory in the right place is all it takes.</p>"},{"location":"gitops/adding-apps/#quick-start","title":"Quick Start","text":"<p>To deploy a new application, you need to:</p> <ol> <li>Choose the appropriate category for your app.</li> <li>Create a directory under <code>pitower/kubernetes/apps/&lt;category&gt;/&lt;app-name&gt;/</code>.</li> <li>Add a <code>kustomization.yaml</code> and (optionally) a <code>values.yaml</code>.</li> <li>Push to <code>main</code>.</li> <li>ArgoCD auto-discovers and syncs the new app.</li> </ol> <pre><code>flowchart LR\n    A[Create directory\\nand manifests] --&gt; B[Push to main]\n    B --&gt; C[ApplicationSet\\ndetects new dir]\n    C --&gt; D[Application\\ncreated]\n    D --&gt; E[Resources synced\\nto cluster]\n\n    style A fill:#7c3aed,color:#fff\n    style C fill:#18b7be,color:#fff\n    style D fill:#18b7be,color:#fff\n    style E fill:#326ce5,color:#fff</code></pre>"},{"location":"gitops/adding-apps/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"gitops/adding-apps/#1-choose-a-category","title":"1. Choose a Category","text":"<p>Pick the category that best fits your application:</p> Category Use For <code>ai</code> AI and machine learning workloads <code>banking</code> Financial tools and services <code>cert-manager</code> TLS certificate resources <code>cloudnative-pg</code> PostgreSQL clusters and backups <code>home-automation</code> Home Assistant, Zigbee2MQTT, MQTT brokers <code>kube-system</code> Core cluster components <code>media</code> Jellyfin, *arr apps, downloaders <code>monitoring</code> Prometheus, Grafana, Loki, alerting <code>networking</code> Envoy Gateway, Cilium, DNS, tunnels <code>openebs</code> Local PV storage <code>rook-ceph</code> Distributed storage <code>security</code> Authentication, secrets, certificates <code>selfhosted</code> General self-hosted applications <code>system</code> System-level utilities <p>When in Doubt</p> <p>If your app does not clearly fit a category, <code>selfhosted</code> is a good default for general-purpose applications.</p>"},{"location":"gitops/adding-apps/#2-create-the-directory","title":"2. Create the Directory","text":"<pre><code>mkdir -p pitower/kubernetes/apps/&lt;category&gt;/&lt;app-name&gt;\n</code></pre> <p>The directory name becomes the app name in ArgoCD. Choose a name that is lowercase, uses hyphens for separation, and is descriptive:</p> <ul> <li><code>echo-server</code> (good)</li> <li><code>home-assistant</code> (good)</li> <li><code>myApp</code> (bad -- use lowercase with hyphens)</li> </ul>"},{"location":"gitops/adding-apps/#3-create-the-manifests","title":"3. Create the Manifests","text":"<p>Most applications in the cluster use the bjw-s app-template Helm chart (v4.6.2) inflated through Kustomize. This pattern requires two files:</p> kustomization.yamlvalues.yaml <pre><code>---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: &lt;category&gt;\nhelmCharts:\n  - name: app-template\n    repo: oci://ghcr.io/bjw-s-labs/helm\n    version: 4.6.2\n    releaseName: &lt;app-name&gt;\n    namespace: &lt;category&gt;\n    valuesFile: values.yaml\n</code></pre> <pre><code>controllers:\n  &lt;app-name&gt;:\n    containers:\n      app:\n        image:\n          repository: &lt;image-repo&gt;\n          tag: &lt;image-tag&gt;\n        env:\n          # Environment variables\n          TZ: Europe/Zurich\n        resources:\n          requests:\n            cpu: 10m\n            memory: 64Mi\n          limits:\n            memory: 256Mi\n        probes:\n          liveness:\n            enabled: true\n          readiness:\n            enabled: true\nservice:\n  app:\n    ports:\n      http:\n        port: 8080\nroute:\n  app:\n    enabled: true\n    hostnames:\n      - &lt;app-name&gt;.example.com\n    parentRefs:\n      - name: envoy-internal\n        namespace: networking\n        sectionName: https\n</code></pre> <p>Gateway Selection</p> <p>Choose the appropriate gateway in <code>parentRefs</code> based on how the app should be accessed:</p> Gateway Use Case <code>envoy-external</code> Accessible via Cloudflare tunnel (<code>*.example.com</code> proxied) <code>envoy-internal</code> Internal-only access (VPN/LAN via <code>internal.example.com</code>)"},{"location":"gitops/adding-apps/#4-complete-example-echo-server","title":"4. Complete Example: echo-server","text":"<p>Here is a real example from the cluster -- the <code>echo-server</code> app in the <code>selfhosted</code> category.</p> <p>Directory structure:</p> <pre><code>pitower/kubernetes/apps/selfhosted/echo-server/\n\u251c\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 values.yaml\n</code></pre> <p>kustomization.yaml:</p> <pre><code>---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: selfhosted\nhelmCharts:\n  - name: app-template\n    repo: oci://ghcr.io/bjw-s-labs/helm\n    version: 4.6.2\n    releaseName: echo-server\n    namespace: selfhosted\n    valuesFile: values.yaml\n</code></pre> <p>values.yaml:</p> <pre><code>controllers:\n  echo-server:\n    annotations:\n      reloader.stakater.com/auto: \"true\"\n    containers:\n      app:\n        image:\n          repository: ghcr.io/mendhak/http-https-echo\n          tag: 39\n        env:\n          HTTP_PORT: 8080\n          LOG_WITHOUT_NEWLINE: \"true\"\n          LOG_IGNORE_PATH: /healthz\n        resources:\n          requests:\n            cpu: 5m\n            memory: 32Mi\n          limits:\n            memory: 64Mi\n        probes:\n          liveness:\n            enabled: true\n          readiness:\n            enabled: true\n          startup:\n            enabled: true\n            spec:\n              failureThreshold: 30\n              periodSeconds: 5\nservice:\n  app:\n    ports:\n      http:\n        port: 8080\nroute:\n  app:\n    enabled: true\n    hostnames:\n      - echo.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\n</code></pre> <p>This produces an ArgoCD Application named <code>selfhosted-echo-server</code> that deploys into the <code>selfhosted</code> namespace.</p>"},{"location":"gitops/adding-apps/#5-push-and-verify","title":"5. Push and Verify","text":"<pre><code>git add pitower/kubernetes/apps/&lt;category&gt;/&lt;app-name&gt;/\ngit commit -m \"feat(&lt;category&gt;): add &lt;app-name&gt;\"\ngit push origin main\n</code></pre> <p>After pushing, verify in ArgoCD:</p> <pre><code># Check that the Application was created\nargocd app list -l app.kubernetes.io/name=&lt;app-name&gt;\n\n# Watch the sync status\nargocd app get &lt;category&gt;-&lt;app-name&gt;\n\n# Or use kubectl\nkubectl get applications -n argocd &lt;category&gt;-&lt;app-name&gt;\n</code></pre>"},{"location":"gitops/adding-apps/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"gitops/adding-apps/#apps-with-extra-resources","title":"Apps with Extra Resources","text":"<p>Some apps need additional Kubernetes resources beyond what the Helm chart provides (certificates, gateway routes, etc.). Add them as extra files and reference them in <code>kustomization.yaml</code>:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: networking\nresources:\n  - certificate.yaml\n  - external.yaml\n  - internal.yaml\nhelmCharts:\n  - name: app-template\n    repo: oci://ghcr.io/bjw-s-labs/helm\n    version: 4.6.2\n    releaseName: envoy-gateway\n    namespace: networking\n    valuesFile: values.yaml\n</code></pre>"},{"location":"gitops/adding-apps/#apps-without-helm","title":"Apps Without Helm","text":"<p>If your app does not use a Helm chart, you can use plain manifests with Kustomize:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: &lt;category&gt;\nresources:\n  - deployment.yaml\n  - service.yaml\n  - httproute.yaml\n</code></pre>"},{"location":"gitops/adding-apps/#apps-with-sops-secrets","title":"Apps with SOPS Secrets","text":"<p>For apps that need encrypted secrets, add the SOPS-encrypted file and reference it in <code>kustomization.yaml</code>:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: &lt;category&gt;\nresources:\n  - secret.sops.yaml\nhelmCharts:\n  - name: app-template\n    # ...\n</code></pre> <p>Never Commit Plaintext Secrets</p> <p>Always encrypt secrets with SOPS before committing. See the SOPS documentation for details.</p>"},{"location":"gitops/adding-apps/#removing-an-application","title":"Removing an Application","text":"<p>To remove an application from the cluster:</p> <ol> <li>Delete the app directory:     <pre><code>rm -rf pitower/kubernetes/apps/&lt;category&gt;/&lt;app-name&gt;\n</code></pre></li> <li>Push to <code>main</code>:     <pre><code>git add -A &amp;&amp; git commit -m \"feat(&lt;category&gt;): remove &lt;app-name&gt;\"\ngit push origin main\n</code></pre></li> <li>The ApplicationSet detects the missing directory and deletes the Application.</li> <li>The <code>resources-finalizer.argocd.argoproj.io</code> finalizer ensures all managed resources are cleaned up from the cluster.</li> </ol>"},{"location":"gitops/adding-apps/#checklist","title":"Checklist","text":"<p>Use this checklist when adding a new app:</p> <ul> <li> Directory created at <code>pitower/kubernetes/apps/&lt;category&gt;/&lt;app-name&gt;/</code></li> <li> <code>kustomization.yaml</code> with correct <code>namespace</code> and chart configuration</li> <li> <code>values.yaml</code> with image, resources, probes, and service defined</li> <li> HTTPRoute configured with the correct gateway (<code>envoy-external</code> or <code>envoy-internal</code>)</li> <li> Resource requests and limits set appropriately</li> <li> Secrets encrypted with SOPS (if applicable)</li> <li> Pushed to <code>main</code> and verified in ArgoCD UI</li> </ul>"},{"location":"gitops/application-sets/","title":"ApplicationSets","text":"<p>ApplicationSets are the core mechanism that makes the GitOps workflow scalable. Instead of writing individual Application resources for each app, a single ApplicationSet per category automatically generates Applications based on the repository directory structure.</p>"},{"location":"gitops/application-sets/#overview","title":"Overview","text":"<p>The cluster uses 15 ApplicationSets, one per application category:</p> ApplicationSet Directory Scanned Example Apps <code>ai</code> <code>pitower/kubernetes/apps/ai/*</code> AI/ML workloads <code>banking</code> <code>pitower/kubernetes/apps/banking/*</code> Financial tools <code>cert-manager</code> <code>pitower/kubernetes/apps/cert-manager/*</code> TLS certificate automation <code>cloudnative-pg</code> <code>pitower/kubernetes/apps/cloudnative-pg/*</code> PostgreSQL operator and clusters <code>home-automation</code> <code>pitower/kubernetes/apps/home-automation/*</code> Home Assistant, Zigbee2MQTT, Mosquitto <code>kube-system</code> <code>pitower/kubernetes/apps/kube-system/*</code> Core cluster components <code>media</code> <code>pitower/kubernetes/apps/media/*</code> Jellyfin, *arr stack, downloaders <code>monitoring</code> <code>pitower/kubernetes/apps/monitoring/*</code> Prometheus, Grafana, Loki <code>networking</code> <code>pitower/kubernetes/apps/networking/*</code> Envoy Gateway, Cilium, external-dns <code>openebs</code> <code>pitower/kubernetes/apps/openebs/*</code> Local PV storage <code>rook-ceph</code> <code>pitower/kubernetes/apps/rook-ceph/*</code> Distributed block storage <code>security</code> <code>pitower/kubernetes/apps/security/*</code> Authelia, LLDAP, External Secrets <code>selfhosted</code> <code>pitower/kubernetes/apps/selfhosted/*</code> Miniflux, Tandoor, Glance, and more <code>system</code> <code>pitower/kubernetes/apps/system/*</code> System-level utilities"},{"location":"gitops/application-sets/#git-directory-generator","title":"Git Directory Generator","text":"<p>Each ApplicationSet uses the Git directory generator, which scans a specific path in the repository for subdirectories. Every subdirectory it finds becomes an ArgoCD Application.</p> <pre><code>flowchart LR\n    subgraph \"Repository Structure\"\n        dir1[\"apps/networking/cloudflared/\"]\n        dir2[\"apps/networking/envoy-gateway/\"]\n        dir3[\"apps/networking/external-dns/\"]\n        dir4[\"apps/networking/nginx/\"]\n        dir5[\"apps/networking/tailscale/\"]\n    end\n\n    AS[ApplicationSet\\nnetworking]\n\n    subgraph \"Generated Applications\"\n        app1[\"networking-cloudflared\"]\n        app2[\"networking-envoy-gateway\"]\n        app3[\"networking-external-dns\"]\n        app4[\"networking-nginx\"]\n        app5[\"networking-tailscale\"]\n    end\n\n    dir1 &amp; dir2 &amp; dir3 &amp; dir4 &amp; dir5 --&gt; AS\n    AS --&gt; app1 &amp; app2 &amp; app3 &amp; app4 &amp; app5\n\n    style AS fill:#18b7be,color:#fff\n    style app1 fill:#326ce5,color:#fff\n    style app2 fill:#326ce5,color:#fff\n    style app3 fill:#326ce5,color:#fff\n    style app4 fill:#326ce5,color:#fff\n    style app5 fill:#326ce5,color:#fff</code></pre> <p>The generator configuration:</p> <pre><code>generators:\n  - git:\n      repoURL: https://github.com/swibrow/home-ops\n      revision: main\n      directories:\n        - path: pitower/kubernetes/apps/networking/*\n</code></pre> <p>This produces the following template variables for each matched directory:</p> Variable Example Value Description <code>{{.path.path}}</code> <code>pitower/kubernetes/apps/networking/envoy-gateway</code> Full path to the directory <code>{{.path.basename}}</code> <code>envoy-gateway</code> Directory name (last segment) <code>{{index .path.segments 0}}</code> <code>pitower</code> First path segment <code>{{index .path.segments 3}}</code> <code>networking</code> Fourth segment (the category)"},{"location":"gitops/application-sets/#go-template-usage","title":"Go Template Usage","text":"<p>ApplicationSets use Go templates with <code>missingkey=error</code> to ensure all template variables are resolved. If a variable is missing, the ApplicationSet controller raises an error instead of silently producing empty strings.</p> <pre><code>spec:\n  goTemplate: true\n  goTemplateOptions: [\"missingkey=error\"]\n</code></pre>"},{"location":"gitops/application-sets/#naming-convention","title":"Naming Convention","text":"<p>Application names follow the pattern <code>&lt;category&gt;-&lt;app-name&gt;</code>:</p> <pre><code>name: \"{{index .path.segments 3}}-{{.path.basename}}\"\n</code></pre> Directory Path Generated Application Name <code>apps/networking/envoy-gateway</code> <code>networking-envoy-gateway</code> <code>apps/media/jellyfin</code> <code>media-jellyfin</code> <code>apps/security/authelia</code> <code>security-authelia</code> <code>apps/selfhosted/miniflux</code> <code>selfhosted-miniflux</code>"},{"location":"gitops/application-sets/#namespace-derivation","title":"Namespace Derivation","text":"<p>The target namespace is derived from the category segment of the path:</p> <pre><code>destination:\n  namespace: \"{{index .path.segments 3}}\"\n</code></pre> <p>This means all apps in <code>apps/networking/*</code> deploy to the <code>networking</code> namespace, all apps in <code>apps/media/*</code> deploy to the <code>media</code> namespace, and so on.</p> <p>Namespace = Category</p> <p>The namespace matches the category directory name. This convention keeps things predictable -- if an app is in the <code>selfhosted</code> category, its resources land in the <code>selfhosted</code> namespace.</p>"},{"location":"gitops/application-sets/#labels","title":"Labels","text":"<p>Each generated Application receives three standard labels:</p> <pre><code>labels:\n  app.kubernetes.io/category: \"{{index .path.segments 3}}\"\n  app.kubernetes.io/name: \"{{.path.basename}}\"\n  app.kubernetes.io/instance: \"{{.path.basename}}\"\n</code></pre> <p>These labels enable filtering in the ArgoCD UI and CLI. For example, to list all networking applications:</p> <pre><code>argocd app list -l app.kubernetes.io/category=networking\n</code></pre>"},{"location":"gitops/application-sets/#full-example-appset-networkingyaml","title":"Full Example: appset-networking.yaml","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: networking\n  namespace: argocd\nspec:\n  goTemplate: true\n  goTemplateOptions: [\"missingkey=error\"]\n  generators:\n    - git:\n        repoURL: https://github.com/swibrow/home-ops\n        revision: main\n        directories:\n          - path: pitower/kubernetes/apps/networking/*\n  template:\n    metadata:\n      name: \"{{index .path.segments 3}}-{{.path.basename}}\"\n      namespace: argocd\n      labels:\n        app.kubernetes.io/category: \"{{index .path.segments 3}}\"\n        app.kubernetes.io/name: \"{{.path.basename}}\"\n        app.kubernetes.io/instance: \"{{.path.basename}}\"\n      finalizers:\n        - resources-finalizer.argocd.argoproj.io\n    spec:\n      project: apps\n      source:\n        repoURL: https://github.com/swibrow/home-ops\n        targetRevision: main\n        path: \"{{.path.path}}\"\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: \"{{index .path.segments 3}}\"\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: false\n          allowEmpty: true\n        syncOptions:\n          - CreateNamespace=true\n          - ServerSideApply=true\n          - SkipDryRunOnMissingResource=true\n          - ApplyOutOfSyncOnly=true\n        retry:\n          limit: 5\n          backoff:\n            duration: 5s\n            factor: 2\n            maxDuration: 3m\n      revisionHistoryLimit: 3\n</code></pre>"},{"location":"gitops/application-sets/#finalizers","title":"Finalizers","text":"<p>Every generated Application includes the <code>resources-finalizer.argocd.argoproj.io</code> finalizer:</p> <pre><code>finalizers:\n  - resources-finalizer.argocd.argoproj.io\n</code></pre> <p>This ensures that when an Application is deleted (for example, by removing its directory from the repository), ArgoCD cleans up all Kubernetes resources that the Application managed. Without the finalizer, deleting the Application would leave orphaned resources in the cluster.</p>"},{"location":"gitops/application-sets/#auto-discovery-in-action","title":"Auto-Discovery in Action","text":"<p>The power of this pattern is that adding a new app requires zero ArgoCD configuration. The ApplicationSet does all the work:</p> <ol> <li>Create a new directory: <code>pitower/kubernetes/apps/networking/my-new-app/</code></li> <li>Add a <code>kustomization.yaml</code> and <code>values.yaml</code> inside it.</li> <li>Push to <code>main</code>.</li> <li>The Git directory generator detects the new directory.</li> <li>A new Application named <code>networking-my-new-app</code> is created automatically.</li> <li>ArgoCD syncs the new Application to the cluster.</li> </ol> <p>Removing an app is equally simple -- delete the directory and push. The finalizer ensures all resources are cleaned up.</p> <p>No ApplicationSet Changes Needed</p> <p>You never need to modify an ApplicationSet to add or remove an app. The directory structure is the only thing that matters.</p>"},{"location":"gitops/application-sets/#creating-a-new-applicationset","title":"Creating a New ApplicationSet","text":"<p>If you need a new category (rare), create a new ApplicationSet in <code>pitower/kubernetes/argocd/</code>:</p> <pre><code># pitower/kubernetes/argocd/appset-&lt;category&gt;.yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: &lt;category&gt;\n  namespace: argocd\nspec:\n  goTemplate: true\n  goTemplateOptions: [\"missingkey=error\"]\n  generators:\n    - git:\n        repoURL: https://github.com/swibrow/home-ops\n        revision: main\n        directories:\n          - path: pitower/kubernetes/apps/&lt;category&gt;/*\n  template:\n    metadata:\n      name: \"{{index .path.segments 3}}-{{.path.basename}}\"\n      # ... (same template as above)\n</code></pre> <p>Then create the corresponding directory under <code>pitower/kubernetes/apps/&lt;category&gt;/</code> and add your first app inside it.</p>"},{"location":"gitops/argocd-setup/","title":"ArgoCD Setup","text":"<p>ArgoCD is the GitOps engine that drives the cluster. It is installed via Helm, bootstrapped with a single <code>kubectl apply</code>, and then manages itself going forward.</p>"},{"location":"gitops/argocd-setup/#bootstrap-process","title":"Bootstrap Process","text":"<p>The bootstrap follows a three-step process that takes the cluster from a bare Kubernetes installation to a fully self-managing GitOps platform.</p> <pre><code>flowchart TD\n    A[1. Apply app-argocd.yaml] --&gt;|creates| B[ArgoCD Bootstrap Application]\n    B --&gt;|points to| C[pitower/kubernetes/bootstrap/]\n    C --&gt;|installs| D[ArgoCD Helm Chart\\n+ Namespace\\n+ AppProject]\n    D --&gt;|ArgoCD starts| E[ArgoCD watches repository]\n    E --&gt;|discovers| F[ApplicationSets in\\npitower/kubernetes/argocd/]\n    F --&gt;|generates| G[Application per app directory]\n    G --&gt;|syncs| H[All cluster workloads running]\n\n    style A fill:#7c3aed,color:#fff\n    style B fill:#ef652a,color:#fff\n    style D fill:#ef652a,color:#fff\n    style E fill:#ef652a,color:#fff\n    style F fill:#18b7be,color:#fff\n    style H fill:#326ce5,color:#fff</code></pre>"},{"location":"gitops/argocd-setup/#step-1-apply-the-bootstrap-application","title":"Step 1: Apply the Bootstrap Application","text":"<p>The only manual <code>kubectl</code> command needed to bring up the entire cluster:</p> <pre><code>kubectl apply -f pitower/kubernetes/argocd/app-argocd.yaml\n</code></pre> <p>This creates the <code>argocd-bootstrap</code> Application resource, which tells ArgoCD to look at the <code>pitower/kubernetes/bootstrap/</code> directory for its own installation manifests.</p>"},{"location":"gitops/argocd-setup/#step-2-argocd-installs-itself","title":"Step 2: ArgoCD Installs Itself","text":"<p>The bootstrap Application points to a Kustomization that includes:</p> Resource Purpose <code>namespace.yaml</code> Creates the <code>argocd</code> namespace <code>appproject.yaml</code> Creates the <code>apps</code> AppProject with admin RBAC <code>argocd-values.yaml</code> Helm values for the ArgoCD chart <code>kustomization.yaml</code> Ties everything together with HelmChartInflationGenerator <p>The <code>kustomization.yaml</code> uses the <code>helmCharts</code> field to install ArgoCD from the official Helm chart:</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - namespace.yaml\n  - appproject.yaml\n\nhelmCharts:\n  - name: argo-cd\n    version: 9.4.2\n    repo: https://argoproj.github.io/argo-helm\n    releaseName: argocd\n    namespace: argocd\n    valuesFile: argocd-values.yaml\n</code></pre>"},{"location":"gitops/argocd-setup/#step-3-argocd-discovers-everything-else","title":"Step 3: ArgoCD Discovers Everything Else","text":"<p>Once ArgoCD is running, it picks up the ApplicationSets defined in <code>pitower/kubernetes/argocd/</code>. Each ApplicationSet scans a category directory and generates Applications for every subdirectory it finds.</p> <p>Self-Managing</p> <p>After the initial <code>kubectl apply</code>, ArgoCD manages its own upgrades. Changing the Helm chart version in <code>kustomization.yaml</code> and pushing to <code>main</code> triggers ArgoCD to upgrade itself.</p>"},{"location":"gitops/argocd-setup/#bootstrap-application","title":"Bootstrap Application","text":"<p>The <code>app-argocd.yaml</code> is the single entry point for the entire cluster:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: argocd-bootstrap\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: 'https://github.com/swibrow/home-ops.git'\n    targetRevision: main\n    path: pitower/kubernetes/bootstrap\n  destination:\n    server: 'https://kubernetes.default.svc'\n    namespace: argocd\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n      allowEmpty: true\n    syncOptions:\n      - CreateNamespace=true\n      - ServerSideApply=true\n</code></pre> <p>selfHeal is enabled here</p> <p>Unlike the ApplicationSets for workloads (which disable selfHeal), the bootstrap Application does enable selfHeal. ArgoCD must always match the desired state to keep the cluster operational. If someone accidentally modifies ArgoCD's own resources, it will self-correct.</p>"},{"location":"gitops/argocd-setup/#appproject-apps","title":"AppProject: <code>apps</code>","text":"<p>The bootstrap process creates an AppProject named <code>apps</code> that all ApplicationSet-generated Applications belong to. This project defines the security boundary for workload applications.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: apps\n  namespace: argocd\nspec:\n  description: Apps\n  sourceRepos:\n    - https://github.com/swibrow/home-ops\n  destinations:\n    - namespace: \"*\"\n      name: \"*\"\n  clusterResourceWhitelist:\n    - group: \"*\"\n      kind: \"*\"\n</code></pre> <p>Key properties of the <code>apps</code> project:</p> Property Value Reason <code>sourceRepos</code> <code>home-ops</code> only Applications can only pull from the home-ops repository <code>destinations</code> All namespaces, all clusters Apps can deploy to any namespace on the local cluster <code>clusterResourceWhitelist</code> All groups, all kinds Apps can create cluster-scoped resources (CRDs, ClusterRoles, etc.) <p>Broad Permissions</p> <p>The <code>apps</code> project grants wide permissions because this is a single-tenant home lab. In a multi-team environment, you would scope <code>destinations</code> and <code>clusterResourceWhitelist</code> per project.</p>"},{"location":"gitops/argocd-setup/#project-structure-summary","title":"Project Structure Summary","text":"<pre><code>flowchart TD\n    subgraph \"ArgoCD Projects\"\n        Default[\"default project\\n(bootstrap only)\"]\n        AppsProj[\"apps project\\n(all workloads)\"]\n    end\n\n    subgraph \"Applications\"\n        Bootstrap[\"argocd-bootstrap\\n(self-managing)\"]\n        AppSets[\"15 ApplicationSets\"]\n        GenApps[\"Generated Applications\\n(one per app directory)\"]\n    end\n\n    Default --&gt; Bootstrap\n    Bootstrap --&gt;|installs ArgoCD +\\ncreates apps project| AppsProj\n    AppsProj --&gt; AppSets\n    AppSets --&gt;|Git directory generator| GenApps\n\n    style Default fill:#333,color:#fff\n    style AppsProj fill:#7c3aed,color:#fff\n    style Bootstrap fill:#ef652a,color:#fff\n    style AppSets fill:#18b7be,color:#fff\n    style GenApps fill:#326ce5,color:#fff</code></pre> <ul> <li>The <code>argocd-bootstrap</code> Application lives in the default project (ArgoCD's built-in project).</li> <li>All workload Applications generated by ApplicationSets live in the apps project.</li> <li>The bootstrap Application is the only resource that needs to be applied manually. Everything else is discovered and managed automatically.</li> </ul>"},{"location":"gitops/sync-policies/","title":"Sync Policies","text":"<p>Every Application generated by the ApplicationSets shares a common sync policy. This page explains each setting, why it was chosen, and how it affects day-to-day operations.</p>"},{"location":"gitops/sync-policies/#policy-overview","title":"Policy Overview","text":"<pre><code>syncPolicy:\n  automated:\n    prune: true\n    selfHeal: false\n    allowEmpty: true\n  syncOptions:\n    - CreateNamespace=true\n    - ServerSideApply=true\n    - SkipDryRunOnMissingResource=true\n    - ApplyOutOfSyncOnly=true\n  retry:\n    limit: 5\n    backoff:\n      duration: 5s\n      factor: 2\n      maxDuration: 3m\nrevisionHistoryLimit: 3\n</code></pre>"},{"location":"gitops/sync-policies/#automated-sync","title":"Automated Sync","text":"Setting Value Description <code>prune</code> <code>true</code> Resources removed from Git are deleted from the cluster <code>selfHeal</code> <code>false</code> Manual changes in the cluster are not automatically reverted <code>allowEmpty</code> <code>true</code> Applications with no manifests are still considered healthy"},{"location":"gitops/sync-policies/#why-prune-true","title":"Why <code>prune: true</code>","text":"<p>When a resource is removed from the Git repository, ArgoCD should remove it from the cluster. This keeps the cluster in sync with Git and prevents orphaned resources from accumulating over time.</p> <p>Prune in Practice</p> <p>If you delete a ConfigMap from <code>values.yaml</code> and push to <code>main</code>, ArgoCD will delete that ConfigMap from the cluster on the next sync cycle. Without prune, the ConfigMap would remain in the cluster indefinitely.</p>"},{"location":"gitops/sync-policies/#why-selfheal-false","title":"Why <code>selfHeal: false</code>","text":"<p>This is a deliberate choice. In many production environments, <code>selfHeal: true</code> is the default -- ArgoCD detects drift from the desired state and automatically reverts the cluster to match Git. In the cluster, this is disabled for workload applications.</p> <p>The reasoning:</p> <ul> <li>Debugging -- When troubleshooting a live issue, you may need to temporarily patch a Deployment (e.g., increase replicas, add debug flags). With selfHeal enabled, ArgoCD would immediately revert those changes.</li> <li>Intentional overrides -- Occasionally, a quick manual fix is needed before a proper Git commit can be made. Disabling selfHeal gives the operator breathing room.</li> <li>Visibility -- When drift occurs, ArgoCD marks the Application as \"OutOfSync\" in the UI. This makes drift visible without automatically acting on it, giving the operator a chance to investigate.</li> </ul> <p>Drift Will Persist</p> <p>Because selfHeal is disabled, any manual changes to cluster resources will persist until the next Git push triggers a sync. Always commit your changes to Git when you are done troubleshooting.</p> <p>Bootstrap Exception</p> <p>The <code>argocd-bootstrap</code> Application does enable <code>selfHeal: true</code>. ArgoCD's own resources must always match the desired state to keep the GitOps platform operational.</p>"},{"location":"gitops/sync-policies/#why-allowempty-true","title":"Why <code>allowEmpty: true</code>","text":"<p>Some applications may temporarily have no manifests (e.g., during migration or when a Kustomization is being restructured). Without <code>allowEmpty</code>, ArgoCD would mark these Applications as errored. Enabling it ensures they stay healthy even when empty.</p>"},{"location":"gitops/sync-policies/#sync-options","title":"Sync Options","text":""},{"location":"gitops/sync-policies/#createnamespacetrue","title":"<code>CreateNamespace=true</code>","text":"<p>ArgoCD will create the target namespace if it does not already exist. Since namespace names are derived from the category directory (<code>networking</code>, <code>media</code>, <code>security</code>, etc.), this ensures namespaces are created on first deployment without requiring a separate resource.</p>"},{"location":"gitops/sync-policies/#serversideapplytrue","title":"<code>ServerSideApply=true</code>","text":"<p>Server-Side Apply (SSA) is used instead of the default client-side <code>kubectl apply</code>. SSA offers several advantages:</p> Benefit Explanation Field ownership tracking Kubernetes tracks which controller owns each field, reducing conflicts Larger resource support SSA avoids the <code>last-applied-configuration</code> annotation, which can exceed the 262 KB annotation limit on large resources like CRDs Better conflict detection Conflicts between different managers are surfaced explicitly CRD compatibility Many modern CRDs (e.g., Cilium, Envoy Gateway) are large and work best with SSA <p>SSA is Recommended</p> <p>Server-Side Apply is the recommended approach for ArgoCD in clusters that use CRDs heavily. The cluster uses Cilium, Envoy Gateway, Rook Ceph, and other operators that ship large CRDs -- SSA prevents annotation overflow issues.</p>"},{"location":"gitops/sync-policies/#skipdryrunonmissingresourcetrue","title":"<code>SkipDryRunOnMissingResource=true</code>","text":"<p>During initial deployment, some resources depend on CRDs that have not yet been installed. For example, an <code>HTTPRoute</code> resource depends on the Gateway API CRDs, which may not exist until Envoy Gateway is installed.</p> <p>Without this option, ArgoCD's dry-run phase would fail because it cannot validate resources against missing CRDs. Skipping the dry run for missing resource types allows the sync to proceed -- the resources will be applied once their CRDs are available.</p> <p>Common Scenario</p> <p>When bootstrapping the cluster from scratch, CRDs for Prometheus (<code>ServiceMonitor</code>), Envoy Gateway (<code>HTTPRoute</code>), and Ceph (<code>CephCluster</code>) may not exist yet. This option prevents sync failures during the initial rollout.</p>"},{"location":"gitops/sync-policies/#applyoutofsynconlytrue","title":"<code>ApplyOutOfSyncOnly=true</code>","text":"<p>ArgoCD only applies resources that are detected as out of sync, rather than re-applying all resources on every sync cycle. This reduces API server load and speeds up sync operations, especially for large Applications with many resources.</p>"},{"location":"gitops/sync-policies/#retry-strategy","title":"Retry Strategy","text":"<pre><code>retry:\n  limit: 5\n  backoff:\n    duration: 5s\n    factor: 2\n    maxDuration: 3m\n</code></pre> <p>When a sync fails (due to transient errors, resource conflicts, or dependency ordering), ArgoCD retries with exponential backoff:</p> Attempt Wait Time Cumulative 1st retry 5s 5s 2nd retry 10s 15s 3rd retry 20s 35s 4th retry 40s 1m 15s 5th retry 80s (capped at 3m) 2m 35s <p>After 5 failed attempts, ArgoCD stops retrying and marks the Application as failed. This is usually enough to handle:</p> <ul> <li>CRD ordering -- A resource fails because its CRD is not installed yet, but another Application installs the CRD within the retry window.</li> <li>Dependency chains -- A Secret or ConfigMap referenced by a Deployment is not yet created.</li> <li>Transient API errors -- Brief API server unavailability during cluster operations.</li> </ul>"},{"location":"gitops/sync-policies/#revision-history-limit","title":"Revision History Limit","text":"<pre><code>revisionHistoryLimit: 3\n</code></pre> <p>ArgoCD keeps the last 3 sync revisions per Application. Older history is pruned to reduce etcd storage consumption. In a home lab with frequent pushes, keeping unlimited history would bloat the ArgoCD Application resources over time.</p>"},{"location":"gitops/sync-policies/#summary","title":"Summary","text":"<pre><code>flowchart TD\n    Push[Git Push to main] --&gt; Detect[ArgoCD detects change]\n    Detect --&gt; Diff{Resources\\nout of sync?}\n    Diff --&gt;|Yes| Apply[Apply out-of-sync\\nresources only]\n    Diff --&gt;|No| Done[No action]\n    Apply --&gt; SSA[Server-Side Apply]\n    SSA --&gt; Success{Sync\\nsucceeded?}\n    Success --&gt;|Yes| Prune{Removed\\nfrom Git?}\n    Success --&gt;|No| Retry{Retries\\nremaining?}\n    Retry --&gt;|Yes| Backoff[Wait with\\nexponential backoff]\n    Backoff --&gt; Apply\n    Retry --&gt;|No| Failed[Mark as Failed]\n    Prune --&gt;|Yes| Delete[Delete from cluster]\n    Prune --&gt;|No| Healthy[Mark as Healthy]\n    Delete --&gt; Healthy\n\n    style Push fill:#7c3aed,color:#fff\n    style SSA fill:#18b7be,color:#fff\n    style Healthy fill:#22c55e,color:#fff\n    style Failed fill:#ef4444,color:#fff</code></pre>"},{"location":"infrastructure/","title":"Infrastructure","text":"<p>The cluster runs on a mix of ARM and x86 hardware, managed entirely through Talos Linux -- an immutable, API-driven Kubernetes OS. This section covers the physical and logical layers that make up the cluster.</p>"},{"location":"infrastructure/#overview","title":"Overview","text":"<pre><code>graph TD\n    subgraph Network\n        SW[TP-Link 24-Port PoE Switch]\n        RT[NanoPi R5C Router]\n        AP1[Ubiquiti U7-Pro AP]\n        AP2[Ubiquiti U6-Lite AP]\n    end\n\n    subgraph Control Plane\n        CP1[worker-01&lt;br/&gt;Lenovo 440p]\n        CP2[worker-02&lt;br/&gt;Lenovo 440p]\n        CP3[worker-03&lt;br/&gt;Acemagician AM06]\n    end\n\n    subgraph Workers\n        W4[worker-04&lt;br/&gt;Acemagician AM06]\n        W5[worker-05&lt;br/&gt;Acemagician AM06]\n        W6[worker-06&lt;br/&gt;Acemagician AM06]\n        WP1[worker-pi-01&lt;br/&gt;Raspberry Pi 4]\n        WP2[worker-pi-02&lt;br/&gt;Raspberry Pi 4]\n        WP3[worker-pi-03&lt;br/&gt;Raspberry Pi 4]\n    end\n\n    subgraph Storage\n        SYN[Synology 4-Bay NAS&lt;br/&gt;8TB]\n        CEPH[(Ceph on NVMe)]\n    end\n\n    SW --&gt; CP1 &amp; CP2 &amp; CP3\n    SW --&gt; W4 &amp; W5 &amp; W6\n    SW --&gt; WP1 &amp; WP2 &amp; WP3\n    SW --&gt; SYN\n    RT --&gt; SW\n    UPS[Eaton 500VA UPS] -.-&gt; SW &amp; RT</code></pre>"},{"location":"infrastructure/#sections","title":"Sections","text":"Page Description Hardware Full hardware inventory -- compute nodes, network gear, and storage Talos Linux Talos OS configuration, factory images, extensions, and patches Cluster Bootstrap Step-by-step guide to bootstrapping the cluster from scratch Node Management Day-2 operations: applying configs, rebooting, resetting, and upgrading nodes"},{"location":"infrastructure/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Talos Linux was chosen over traditional distributions for its immutability, minimal attack surface, and fully API-driven management (no SSH, no shell).</li> <li>Mixed architecture (ARM + x86) is supported through Talos factory images with per-node schematics and extensions.</li> <li>Control plane nodes also run workloads (<code>allowSchedulingOnControlPlanes: true</code>) to maximize resource utilization.</li> <li>Cilium replaces kube-proxy entirely and serves as the CNI, configured at bootstrap time via kustomize addons.</li> <li>PoE powers the Raspberry Pi nodes directly from the switch, reducing cable clutter.</li> </ul>"},{"location":"infrastructure/cluster-bootstrap/","title":"Cluster Bootstrap","text":"<p>This page documents the full process of bootstrapping the cluster from scratch. All steps use <code>just</code> recipes defined in <code>pitower/talos/justfile</code>.</p>"},{"location":"infrastructure/cluster-bootstrap/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure the following are available:</p> <ul> <li> talosctl installed (matching Talos v1.12.4)</li> <li> kubectl installed</li> <li> just command runner installed</li> <li> sops installed and configured with the correct age key or GPG key</li> <li> kustomize installed (for building CNI addons)</li> <li> All nodes are powered on and booted into Talos maintenance mode</li> <li> Network connectivity between your workstation and all node IPs</li> <li> <code>secrets.sops.yaml</code> present in <code>pitower/talos/</code></li> </ul> <p>SOPS Key Required</p> <p>The <code>secrets.sops.yaml</code> file contains the cluster secrets (CA certs, tokens, encryption keys). You must have the corresponding SOPS decryption key (age or GPG) available in your environment.</p>"},{"location":"infrastructure/cluster-bootstrap/#bootstrap-steps","title":"Bootstrap Steps","text":""},{"location":"infrastructure/cluster-bootstrap/#overview","title":"Overview","text":"<pre><code>flowchart TD\n    A[1. Decrypt Secrets] --&gt; B[2. Generate Configs]\n    B --&gt; C[3. Patch Per-Node Configs]\n    C --&gt; D[4. Apply Control Plane Configs]\n    D --&gt; E[5. Apply Worker Configs]\n    E --&gt; F[6. Bootstrap Cluster]\n    F --&gt; G[7. Get Kubeconfig]\n    G --&gt; H[8. Apply CNI Addons]\n    H --&gt; I[9. Verify Cluster Health]</code></pre>"},{"location":"infrastructure/cluster-bootstrap/#step-1-generate-base-machine-configs","title":"Step 1: Generate Base Machine Configs","text":"<pre><code>cd pitower/talos\njust config\n</code></pre> <p>This recipe:</p> <ol> <li>Decrypts <code>secrets.sops.yaml</code> to <code>secrets.yaml</code> using SOPS</li> <li>Runs <code>talosctl gen config</code> with:<ul> <li>Cluster name: <code>home-ops</code></li> <li>Endpoint: <code>https://192.168.0.200:6443</code></li> <li>Global patch: <code>patches/general.patch</code></li> <li>Control plane patch: <code>patches/controlplane.patch</code></li> </ul> </li> <li>Outputs <code>controlplane.yaml</code> and <code>worker.yaml</code> into <code>clusterconfig/</code></li> </ol> <p>The decrypted <code>secrets.yaml</code> is written to disk temporarily. It is not committed to Git.</p>"},{"location":"infrastructure/cluster-bootstrap/#step-2-apply-per-node-patches","title":"Step 2: Apply Per-Node Patches","text":"<pre><code>just patch\n</code></pre> <p>This applies node-specific patches from <code>patches/nodes/</code> to create individual config files:</p> Base Config Node Patches Applied <code>controlplane.yaml</code> <code>worker-01.patch</code>, <code>worker-02.patch</code>, <code>worker-03.patch</code> <code>worker.yaml</code> <code>worker-04.patch</code>, <code>worker-05.patch</code>, <code>worker-06.patch</code> <code>worker.yaml</code> <code>worker-pi-01.patch</code>, <code>worker-pi-02.patch</code>, <code>worker-pi-03.patch</code> <p>Output files are written to <code>clusterconfig/</code> (e.g., <code>worker-01.yaml</code>, <code>worker-pi-02.yaml</code>).</p>"},{"location":"infrastructure/cluster-bootstrap/#step-3-apply-configs-to-control-plane-nodes","title":"Step 3: Apply Configs to Control Plane Nodes","text":"<pre><code>just apply-controlplanes\n</code></pre> <p>Applies the generated configs to the three control plane nodes:</p> Node IP Config File worker-01 192.168.0.201 <code>worker-01.yaml</code> worker-02 192.168.0.202 <code>worker-02.yaml</code> worker-03 192.168.0.203 <code>worker-03.yaml</code> <p>Apply Order</p> <p>Control plane configs must be applied before workers. The control plane nodes form the etcd cluster that workers will join.</p>"},{"location":"infrastructure/cluster-bootstrap/#step-4-apply-configs-to-worker-nodes","title":"Step 4: Apply Configs to Worker Nodes","text":"<pre><code>just apply-workers\n</code></pre> <p>Applies configs to the worker nodes:</p> Node IP Config File worker-04 192.168.0.204 <code>worker-04.yaml</code> worker-pi-01 192.168.0.211 <code>worker-pi-01.yaml</code> worker-pi-02 192.168.0.212 <code>worker-pi-02.yaml</code> worker-pi-03 192.168.0.213 <code>worker-pi-03.yaml</code>"},{"location":"infrastructure/cluster-bootstrap/#step-5-bootstrap-the-cluster","title":"Step 5: Bootstrap the Cluster","text":"<pre><code>just bootstrap\n</code></pre> <p>This recipe:</p> <ol> <li>Bootstraps etcd on the first control plane node (<code>192.168.0.201</code>)</li> <li>Retrieves kubeconfig from the VIP (<code>192.168.0.200</code>)</li> </ol> <pre><code>talosctl bootstrap --nodes 192.168.0.201\ntalosctl kubeconfig --nodes 192.168.0.200\n</code></pre> <p>Bootstrap is a one-time operation</p> <p>The <code>bootstrap</code> command should only be run once during initial cluster creation. Running it again on an existing cluster can corrupt etcd.</p>"},{"location":"infrastructure/cluster-bootstrap/#step-6-verify-talos-node-health","title":"Step 6: Verify Talos Node Health","text":"<p>Before applying addons, confirm all nodes are up:</p> <pre><code>talosctl get members\n</code></pre> <p>Expected output should show all nodes with their IPs and roles.</p>"},{"location":"infrastructure/cluster-bootstrap/#post-bootstrap","title":"Post-Bootstrap","text":""},{"location":"infrastructure/cluster-bootstrap/#apply-cni-and-addons","title":"Apply CNI and Addons","text":"<p>The cluster starts with no CNI (<code>cni.name: none</code>) and no kube-proxy (<code>proxy.disabled: true</code>). Cilium and the kubelet CSR approver must be installed as the first addons:</p> <pre><code>just addons\n</code></pre> <p>This runs:</p> <pre><code>kustomize build ./addons --enable-helm | kubectl apply -f -\n</code></pre> <p>The addons kustomization includes:</p> Addon Namespace Purpose Cilium (v1.18.7) <code>kube-system</code> CNI, kube-proxy replacement, network policies kubelet-csr-approver (v1.2.13) <code>system-controllers</code> Auto-approves kubelet certificate signing requests <p>Cilium Must Be Applied First</p> <p>Pods will remain in <code>Pending</code> state until Cilium is installed because there is no CNI to assign pod IPs. The <code>just addons</code> step is critical and must be run immediately after bootstrap.</p>"},{"location":"infrastructure/cluster-bootstrap/#verify-cluster-health","title":"Verify Cluster Health","text":"<p>After addons are applied, verify the cluster is healthy:</p> Check Node StatusCheck Cilium StatusCheck System PodsTalos Health <pre><code>kubectl get nodes -o wide\n</code></pre> <p>All nodes should show <code>Ready</code> status.</p> <pre><code>kubectl -n kube-system get pods -l app.kubernetes.io/name=cilium\n</code></pre> <p>All Cilium agent pods should be <code>Running</code>.</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Core system pods (kube-apiserver, kube-controller-manager, kube-scheduler, etcd) should be <code>Running</code> on control plane nodes.</p> <pre><code>talosctl health --nodes 192.168.0.201\n</code></pre> <p>Should report all checks passing.</p>"},{"location":"infrastructure/cluster-bootstrap/#troubleshooting","title":"Troubleshooting","text":"Nodes not appearing after bootstrap <ul> <li>Verify network connectivity: <code>talosctl get addresses -n &lt;node-ip&gt;</code></li> <li>Check if the node received its config: <code>talosctl get machineconfig -n &lt;node-ip&gt;</code></li> <li>Ensure the bootstrap node (192.168.0.201) completed etcd initialization</li> </ul> Pods stuck in Pending after bootstrap <p>This is expected until Cilium is installed. Run <code>just addons</code> to install the CNI.</p> Certificate errors after bootstrap <p>The kubelet CSR approver handles automatic certificate approval. If nodes show certificate errors:</p> <pre><code>kubectl get csr\nkubectl certificate approve &lt;csr-name&gt;\n</code></pre> <p>Once the kubelet-csr-approver is running, future CSRs are approved automatically.</p> SOPS decryption fails <p>Ensure your SOPS key is available:</p> <ul> <li>age: Set <code>SOPS_AGE_KEY_FILE</code> environment variable</li> <li>GPG: Ensure the GPG key is in your keyring</li> </ul>"},{"location":"infrastructure/hardware/","title":"Hardware Inventory","text":"<p>The cluster is built from a mix of ARM and x86 hardware. Raspberry Pi 4 nodes handle lightweight workloads and control plane duties, while Acemagician AM06 mini PCs provide Intel GPUs and NVMe storage for Ceph.</p>"},{"location":"infrastructure/hardware/#compute-nodes","title":"Compute Nodes","text":""},{"location":"infrastructure/hardware/#control-plane-workers","title":"Control Plane + Workers","text":"<p>These nodes run control plane components and schedule workloads (<code>allowSchedulingOnControlPlanes: true</code>). The VIP <code>192.168.0.200</code> floats across all three for API server high availability.</p> Hostname Hardware IP Role CPU RAM Storage Image Schematic worker-01 Lenovo 440p 192.168.0.201 Control Plane + Worker AMD -- -- <code>amd</code> worker-02 Lenovo 440p 192.168.0.202 Control Plane + Worker AMD -- -- <code>amd</code> worker-03 Acemagician AM06 192.168.0.203 Control Plane + Worker AMD -- NVMe (Ceph) <code>amd</code> <p>VIP for API Server</p> <p>All control plane nodes share the virtual IP <code>192.168.0.200</code> for the Kubernetes API endpoint (<code>https://192.168.0.200:6443</code>).</p>"},{"location":"infrastructure/hardware/#worker-nodes-intel","title":"Worker Nodes (Intel)","text":"Hostname Hardware IP Role CPU RAM Storage Image Schematic worker-04 Acemagician AM06 192.168.0.204 Worker Intel -- eMMC + NVMe (Ceph) <code>intel</code> worker-05 Acemagician AM06 -- Worker Intel -- NVMe (Ceph) <code>intel</code> worker-06 Acemagician AM06 -- Worker Intel -- NVMe (Ceph) <code>intel</code> <p>Intel GPU Workloads</p> <p>The Acemagician AM06 nodes include Intel integrated GPUs (i915) with firmware loaded via Talos extensions. These are used for hardware transcoding in applications like Jellyfin.</p>"},{"location":"infrastructure/hardware/#worker-nodes-raspberry-pi","title":"Worker Nodes (Raspberry Pi)","text":"Hostname Hardware IP Role Image Schematic worker-pi-01 Raspberry Pi 4 192.168.0.211 Worker <code>rpi-poe</code> worker-pi-02 Raspberry Pi 4 192.168.0.212 Worker <code>rpi-poe</code> worker-pi-03 Raspberry Pi 4 192.168.0.213 Worker <code>rpi-poe</code> <p>PoE Hat Fan Control</p> <p>The Raspberry Pi nodes use PoE hats for power delivery. Fan speed thresholds are configured in the Talos overlay:</p> <pre><code>dtparam=poe_fan_temp0=50000   # 50C\ndtparam=poe_fan_temp1=60000   # 60C\ndtparam=poe_fan_temp2=70000   # 70C\ndtparam=poe_fan_temp3=80000   # 80C\n</code></pre>"},{"location":"infrastructure/hardware/#other-raspberry-pis-not-in-cluster","title":"Other Raspberry Pis (Not in cluster)","text":"Quantity Model Use 4 Raspberry Pi 2B+ Spare / other projects 1 Raspberry Pi 3B+ Spare / other projects"},{"location":"infrastructure/hardware/#network-equipment","title":"Network Equipment","text":"Device Model Purpose Switch TP-Link 24-Port PoE Core network switch, powers Pi nodes via PoE Router NanoPi R5C Primary router Access Point Ubiquiti U7-Pro Wi-Fi 7 AP Access Point Ubiquiti U6-Lite Wi-Fi 6 AP"},{"location":"infrastructure/hardware/#storage","title":"Storage","text":"Device Capacity Purpose Synology 4-Bay NAS 8 TB Bulk storage (NFS) 128 GB SSD x3 Boot drives 512 GB NVMe x3 Ceph OSD storage (on Acemagician AM06 nodes) <p>Ceph Storage</p> <p>The three 512 GB NVMe drives in the Acemagician AM06 nodes form a Rook-Ceph cluster, providing replicated block storage for persistent volumes.</p>"},{"location":"infrastructure/hardware/#power","title":"Power","text":"Device Model Capacity UPS Eaton 500VA Protects switch, router, and critical nodes"},{"location":"infrastructure/hardware/#network-topology","title":"Network Topology","text":"<pre><code>graph LR\n    Internet --&gt;|WAN| RT[NanoPi R5C&lt;br/&gt;Router]\n    RT --&gt;|LAN| SW[TP-Link 24-Port&lt;br/&gt;PoE Switch]\n\n    SW --&gt;|PoE| WP1[Pi worker-pi-01]\n    SW --&gt;|PoE| WP2[Pi worker-pi-02]\n    SW --&gt;|PoE| WP3[Pi worker-pi-03]\n\n    SW --&gt; CP1[worker-01&lt;br/&gt;Lenovo 440p]\n    SW --&gt; CP2[worker-02&lt;br/&gt;Lenovo 440p]\n    SW --&gt; CP3[worker-03&lt;br/&gt;AM06]\n\n    SW --&gt; W4[worker-04&lt;br/&gt;AM06]\n    SW --&gt; W5[worker-05&lt;br/&gt;AM06]\n    SW --&gt; W6[worker-06&lt;br/&gt;AM06]\n\n    SW --&gt; NAS[Synology NAS]\n\n    SW -.-&gt; AP1[U7-Pro AP]\n    SW -.-&gt; AP2[U6-Lite AP]\n\n    UPS[Eaton 500VA] -.-&gt;|Power| SW\n    UPS -.-&gt;|Power| RT</code></pre>"},{"location":"infrastructure/node-management/","title":"Node Management","text":"<p>Day-to-day operations for managing Talos nodes in the cluster. All commands use <code>just</code> recipes defined in <code>pitower/talos/justfile</code> or direct <code>talosctl</code> commands.</p>"},{"location":"infrastructure/node-management/#applying-configuration-changes","title":"Applying Configuration Changes","text":""},{"location":"infrastructure/node-management/#full-regeneration","title":"Full Regeneration","text":"<p>When patches or secrets change, regenerate and reapply all configs:</p> <pre><code>cd pitower/talos\njust config       # Regenerate base configs from secrets + patches\njust patch        # Apply per-node patches\njust apply-controlplanes   # Push configs to control plane nodes\njust apply-workers         # Push configs to worker nodes\n</code></pre> <p>Config Apply is Non-Disruptive (Usually)</p> <p>Most config changes are applied live without a reboot. Talos will indicate if a reboot is required after applying.</p>"},{"location":"infrastructure/node-management/#single-node-apply","title":"Single Node Apply","text":"<p>To apply configuration to a single worker by name:</p> <pre><code>just apply-worker 04        # Applies to worker-04\njust apply-worker pi-01     # Applies to worker-pi-01\n</code></pre> <p>This recipe dynamically resolves the node's IP from Talos member information:</p> <pre><code>talosctl get members -o json | jq -rs \\\n  --arg h \"worker-&lt;name&gt;\" \\\n  '[.[] | select(.spec.hostname == $h) | .spec.addresses[0]][0]'\n</code></pre>"},{"location":"infrastructure/node-management/#control-plane-nodes-individually","title":"Control Plane Nodes Individually","text":"<p>For individual control plane nodes, use <code>talosctl</code> directly:</p> <pre><code>talosctl apply-config \\\n    --endpoints 192.168.0.201 \\\n    --nodes 192.168.0.201 \\\n    --file clusterconfig/worker-01.yaml\n</code></pre>"},{"location":"infrastructure/node-management/#rebooting-nodes","title":"Rebooting Nodes","text":"<p>Reboot Order Matters</p> <p>Always reboot nodes sequentially with <code>--wait</code> to maintain cluster availability. Never reboot all control plane nodes simultaneously.</p>"},{"location":"infrastructure/node-management/#control-plane-nodes","title":"Control Plane Nodes","text":"<pre><code>just reboot-controlplanes\n</code></pre> <p>Reboots each control plane node one at a time, waiting for each to come back before proceeding:</p> <pre><code>for i in 1 2 3; do\n    talosctl reboot \\\n        --endpoints \"192.168.0.20${i}\" \\\n        --nodes \"192.168.0.20${i}\" \\\n        --wait\ndone\n</code></pre>"},{"location":"infrastructure/node-management/#worker-nodes","title":"Worker Nodes","text":"<pre><code>just reboot-workers\n</code></pre> <p>Reboots worker nodes sequentially:</p> <pre><code>for i in 1 2 3 4; do\n    talosctl reboot --nodes \"192.168.0.21${i}\" --wait\ndone\n</code></pre> <p>Note</p> <p>The worker reboot recipe currently covers <code>192.168.0.211-214</code>. Adjust the loop if the worker node set changes.</p>"},{"location":"infrastructure/node-management/#resetting-nodes","title":"Resetting Nodes","text":"<p>To wipe and reset a node (for reprovisioning or troubleshooting):</p> <pre><code>just reset 04       # Resets 192.168.0.204\njust reset 11       # Resets 192.168.0.211\n</code></pre> <p>This runs:</p> <pre><code>talosctl reset \\\n    --system-labels-to-wipe=EPHEMERAL \\\n    --system-labels-to-wipe=META \\\n    --reboot \\\n    --graceful=false \\\n    -n 192.168.0.2&lt;suffix&gt;\n</code></pre> Flag Purpose <code>--system-labels-to-wipe=EPHEMERAL</code> Wipes the ephemeral partition (kubelet data, pod storage) <code>--system-labels-to-wipe=META</code> Wipes the META partition (machine config, state) <code>--reboot</code> Reboots the node after reset <code>--graceful=false</code> Skips graceful shutdown (forces immediate reset) <p>Destructive Operation</p> <p>Reset wipes node state completely. The node will return to maintenance mode and need its config reapplied. For control plane nodes, ensure the remaining control plane has quorum before resetting.</p>"},{"location":"infrastructure/node-management/#upgrading-nodes","title":"Upgrading Nodes","text":"<p>Upgrades swap the Talos OS image atomically. Each node type uses a different factory image with the appropriate extensions baked in.</p>"},{"location":"infrastructure/node-management/#upgrade-flow","title":"Upgrade Flow","text":"<pre><code>flowchart LR\n    A[New Talos Version&lt;br/&gt;Released] --&gt; B[Update Image&lt;br/&gt;Variables in justfile]\n    B --&gt; C[Upgrade Control Plane&lt;br/&gt;Nodes Sequentially]\n    C --&gt; D[Upgrade Intel Workers&lt;br/&gt;Sequentially]\n    D --&gt; E[Upgrade RPi Workers&lt;br/&gt;Sequentially]\n    E --&gt; F[Verify Cluster&lt;br/&gt;Health]</code></pre>"},{"location":"infrastructure/node-management/#control-plane-nodes-armrpi","title":"Control Plane Nodes (ARM/RPi)","text":"<pre><code>just upgrade-controlplanes\n</code></pre> <pre><code>for i in 1 2 3; do\n    talosctl upgrade \\\n        --image factory.talos.dev/installer/de94b242...:v1.12.4 \\\n        --nodes \"192.168.0.20${i}\" \\\n        --preserve \\\n        --wait\ndone\n</code></pre>"},{"location":"infrastructure/node-management/#control-plane-nodes-amd","title":"Control Plane Nodes (AMD)","text":"<pre><code>just upgrade-controlplanes-amd\n</code></pre> <p>Uses the AMD-specific factory image for AMD-based control plane nodes.</p>"},{"location":"infrastructure/node-management/#intel-workers","title":"Intel Workers","text":"<pre><code>just upgrade-workers-intel\n</code></pre> <pre><code>for i in 4; do\n    talosctl upgrade \\\n        --image factory.talos.dev/installer/97bf8e92...:v1.12.4 \\\n        --nodes \"192.168.0.20${i}\" \\\n        --preserve \\\n        --wait\ndone\n</code></pre>"},{"location":"infrastructure/node-management/#raspberry-pi-workers","title":"Raspberry Pi Workers","text":"<pre><code>just upgrade-workers-rpi\n</code></pre> <pre><code>for i in 1 2 3; do\n    talosctl upgrade \\\n        --image factory.talos.dev/installer/a862538d...:v1.12.4 \\\n        --nodes \"192.168.0.21${i}\" \\\n        --preserve \\\n        --wait\ndone\n</code></pre> <p>The <code>--preserve</code> Flag</p> <p>All upgrade recipes use <code>--preserve</code> to retain the ephemeral partition across upgrades. This avoids re-downloading container images and preserves local state.</p>"},{"location":"infrastructure/node-management/#updating-image-variables","title":"Updating Image Variables","text":"<p>When upgrading to a new Talos version, update the image variables at the top of <code>pitower/talos/justfile</code>:</p> <pre><code>cp_image := \"factory.talos.dev/installer/&lt;SCHEMATIC_ID&gt;:&lt;NEW_VERSION&gt;\"\ncp_amd_image := \"factory.talos.dev/installer/&lt;SCHEMATIC_ID&gt;:&lt;NEW_VERSION&gt;\"\nworker_intel_image := \"factory.talos.dev/installer/&lt;SCHEMATIC_ID&gt;:&lt;NEW_VERSION&gt;\"\nworker_rpi_image := \"factory.talos.dev/installer/&lt;SCHEMATIC_ID&gt;:&lt;NEW_VERSION&gt;\"\n</code></pre> <p>If extensions have changed, regenerate schematic IDs first:</p> <pre><code>just image-id\n</code></pre>"},{"location":"infrastructure/node-management/#image-management","title":"Image Management","text":""},{"location":"infrastructure/node-management/#list-images-on-a-node","title":"List Images on a Node","text":"<p>View all container images on a specific node, sorted by size:</p> <pre><code>just image-list 192.168.0.201\n</code></pre> <p>This runs:</p> <pre><code>talosctl -n &lt;node&gt; image list | sort -k4 -h\n</code></pre>"},{"location":"infrastructure/node-management/#image-usage-across-nodes","title":"Image Usage Across Nodes","text":"<p>Get a summary of image count and disk usage for all control plane and worker nodes:</p> <pre><code>just image-usage\n</code></pre> <p>Example output:</p> <pre><code>192.168.0.201       142 images  3.2 GB\n192.168.0.202       138 images  3.1 GB\n192.168.0.203       145 images  3.4 GB\n192.168.0.204       112 images  2.8 GB\n</code></pre> <p>This iterates over all nodes defined in the <code>nodes</code> variable and reports containerd storage usage.</p>"},{"location":"infrastructure/node-management/#node-specific-patches","title":"Node-Specific Patches","text":"<p>Each node has a dedicated patch file under <code>pitower/talos/patches/nodes/</code>:</p> <pre><code>patches/nodes/\n  worker-01.patch      # CP node 1 (Lenovo 440p, AMD)\n  worker-02.patch      # CP node 2 (Lenovo 440p, AMD)\n  worker-03.patch      # CP node 3 (Acemagician AM06, AMD)\n  worker-04.patch      # Worker (Acemagician AM06, Intel)\n  worker-05.patch      # Worker (Acemagician AM06, Intel)\n  worker-06.patch      # Worker (Acemagician AM06, Intel)\n  worker-pi-01.patch   # Worker (Raspberry Pi 4)\n  worker-pi-02.patch   # Worker (Raspberry Pi 4)\n  worker-pi-03.patch   # Worker (Raspberry Pi 4)\n</code></pre>"},{"location":"infrastructure/node-management/#what-node-patches-configure","title":"What Node Patches Configure","text":"Field Control Plane Nodes Intel Workers RPi Workers <code>hostname</code> Yes Yes Yes <code>install.image</code> Factory image (AMD) Factory image (Intel) Factory image (RPi PoE) <code>install.disk</code> -- <code>/dev/mmcblk0</code> (where applicable) -- <code>network.interfaces</code> DHCP + VIP (<code>192.168.0.200</code>) Default DHCP <code>install.extraKernelArgs</code> -- -- <code>initcall_blacklist=sensors_nct6683_init</code>"},{"location":"infrastructure/node-management/#editing-a-node-patch","title":"Editing a Node Patch","text":"<ol> <li>Edit the patch file (e.g., <code>patches/nodes/worker-04.patch</code>)</li> <li>Regenerate configs: <code>just config &amp;&amp; just patch</code></li> <li>Apply to the specific node: <code>just apply-worker 04</code></li> </ol> <p>VIP Assignment</p> <p>The Kubernetes API VIP (<code>192.168.0.200</code>) is configured on all three control plane nodes via their per-node patches. Talos manages VIP failover automatically.</p>"},{"location":"infrastructure/node-management/#quick-reference","title":"Quick Reference","text":"Operation Command Generate configs <code>just config</code> Patch per-node <code>just patch</code> Apply to all control planes <code>just apply-controlplanes</code> Apply to all workers <code>just apply-workers</code> Apply to single worker <code>just apply-worker &lt;name&gt;</code> Reboot control planes <code>just reboot-controlplanes</code> Reboot workers <code>just reboot-workers</code> Reset a node <code>just reset &lt;suffix&gt;</code> Upgrade control planes (RPi) <code>just upgrade-controlplanes</code> Upgrade control planes (AMD) <code>just upgrade-controlplanes-amd</code> Upgrade Intel workers <code>just upgrade-workers-intel</code> Upgrade RPi workers <code>just upgrade-workers-rpi</code> Build/apply addons <code>just addons</code> List images on node <code>just image-list &lt;node-ip&gt;</code> Image usage summary <code>just image-usage</code> Regenerate schematic IDs <code>just image-id</code>"},{"location":"infrastructure/talos-linux/","title":"Talos Linux","text":"<p>The cluster runs Talos Linux v1.12.4 -- a purpose-built, immutable operating system for Kubernetes. Talos has no shell, no SSH, and no package manager. All management is done through a gRPC API via <code>talosctl</code>.</p>"},{"location":"infrastructure/talos-linux/#why-talos","title":"Why Talos","text":"Property Benefit Immutable The OS is read-only. No drift, no manual changes, no configuration surprises. API-driven All operations go through <code>talosctl</code>. Infrastructure is code, not a series of SSH commands. Minimal attack surface No shell, no SSH, no unnecessary services. The only way in is the API. Declarative Machine configs are YAML documents that describe the desired state of each node. Atomic upgrades Upgrades swap the entire OS image atomically. Rollback is automatic on failure."},{"location":"infrastructure/talos-linux/#talos-version-and-factory-images","title":"Talos Version and Factory Images","text":"<p>The cluster uses Talos v1.12.4 with custom factory images from <code>factory.talos.dev</code>. Each node type has a different image built from a schematic -- a YAML file that declares which system extensions and overlays to include.</p>"},{"location":"infrastructure/talos-linux/#how-factory-images-work","title":"How Factory Images Work","text":"<pre><code>flowchart LR\n    A[Extension YAML&lt;br/&gt;e.g. intel.yaml] --&gt;|POST| B[factory.talos.dev/schematics]\n    B --&gt;|Returns| C[Schematic ID&lt;br/&gt;97bf8e92...]\n    C --&gt; D[factory.talos.dev/installer/&lt;br/&gt;SCHEMATIC_ID:VERSION]\n    D --&gt; E[Custom Talos Image&lt;br/&gt;with Extensions]</code></pre> <p>The schematic ID is embedded in the installer image URL used by each node. For example:</p> <pre><code>factory.talos.dev/installer/97bf8e92fc6bba0f03928b859c08295d7615737b29db06a97be51dc63004e403:v1.12.4\n</code></pre>"},{"location":"infrastructure/talos-linux/#image-definitions","title":"Image Definitions","text":"<p>The justfile defines four image variables, one per node type:</p> <pre><code>cp_image     := \"factory.talos.dev/installer/de94b242...:v1.12.4\"  # RPi (control plane)\ncp_amd_image := \"factory.talos.dev/installer/f19ad7b4...:v1.12.4\"  # AMD (control plane)\nworker_intel_image := \"factory.talos.dev/installer/97bf8e92...:v1.12.4\"  # Intel workers\nworker_rpi_image   := \"factory.talos.dev/installer/a862538d...:v1.12.4\"  # RPi workers\n</code></pre> <p>To regenerate schematic IDs after changing extensions:</p> <pre><code>just image-id\n</code></pre> <p>This POSTs each extension YAML to <code>factory.talos.dev/schematics</code> and prints the resulting IDs.</p>"},{"location":"infrastructure/talos-linux/#extensions-per-node-type","title":"Extensions per Node Type","text":"Intel WorkersAMD NodesRaspberry Pi (PoE) extensions/intel.yaml<pre><code>customization:\n  systemExtensions:\n    officialExtensions:\n      - siderolabs/util-linux-tools\n      - siderolabs/i915-ucode\n      - siderolabs/intel-ucode\n</code></pre> <p>Extensions provide Intel GPU firmware (<code>i915-ucode</code>), CPU microcode updates (<code>intel-ucode</code>), and additional userspace utilities (<code>util-linux-tools</code>).</p> extensions/amd.yaml<pre><code>customization:\n  systemExtensions:\n    officialExtensions:\n      - siderolabs/util-linux-tools\n      - siderolabs/amd-ucode\n      - siderolabs/amdgpu-firmware\n</code></pre> <p>Extensions provide AMD CPU microcode and GPU firmware for the Lenovo 440p and Acemagician AM06 nodes running AMD processors.</p> extensions/rpi-poe.yaml<pre><code>overlay:\n  image: siderolabs/sbc-raspberrypi\n  name: rpi_generic\n  options:\n    configTxtAppend: |-\n      # PoE Hat Fan Speeds\n      dtoverlay=rpi-poe\n      dtparam=poe_fan_temp0=50000\n      dtparam=poe_fan_temp1=60000\n      dtparam=poe_fan_temp2=70000\n      dtparam=poe_fan_temp3=80000\ncustomization:\n  systemExtensions:\n    officialExtensions:\n      - siderolabs/util-linux-tools\n</code></pre> <p>Uses the <code>sbc-raspberrypi</code> overlay with PoE hat fan speed thresholds configured via device tree parameters.</p>"},{"location":"infrastructure/talos-linux/#configuration-generation-flow","title":"Configuration Generation Flow","text":"<p>All Talos configuration is generated and applied through <code>just</code> recipes defined in <code>pitower/talos/justfile</code>.</p> <pre><code>flowchart TD\n    A[secrets.sops.yaml&lt;br/&gt;Encrypted] --&gt;|sops -d| B[secrets.yaml&lt;br/&gt;Decrypted]\n    B --&gt; C[talosctl gen config]\n    D[general.patch] --&gt; C\n    E[controlplane.patch] --&gt; C\n    C --&gt; F[clusterconfig/controlplane.yaml]\n    C --&gt; G[clusterconfig/worker.yaml]\n\n    F --&gt; H{Per-node patches}\n    G --&gt; H\n\n    H --&gt;|worker-01.patch| I[worker-01.yaml]\n    H --&gt;|worker-02.patch| J[worker-02.yaml]\n    H --&gt;|worker-03.patch| K[worker-03.yaml]\n    H --&gt;|worker-04.patch| L[worker-04.yaml]\n    H --&gt;|worker-05.patch| M[worker-05.yaml]\n    H --&gt;|worker-06.patch| N[worker-06.yaml]\n    H --&gt;|worker-pi-01.patch| O[worker-pi-01.yaml]\n    H --&gt;|worker-pi-02.patch| P[worker-pi-02.yaml]\n    H --&gt;|worker-pi-03.patch| Q[worker-pi-03.yaml]</code></pre>"},{"location":"infrastructure/talos-linux/#step-1-generate-base-configs","title":"Step 1: Generate Base Configs","text":"<pre><code>just config\n</code></pre> <p>This decrypts <code>secrets.sops.yaml</code> and runs <code>talosctl gen config</code> with two global patches:</p> <ul> <li><code>patches/general.patch</code> -- applied to all nodes</li> <li><code>patches/controlplane.patch</code> -- applied to control plane nodes only</li> </ul> <p>Output goes to <code>clusterconfig/controlplane.yaml</code> and <code>clusterconfig/worker.yaml</code>.</p>"},{"location":"infrastructure/talos-linux/#step-2-apply-per-node-patches","title":"Step 2: Apply Per-Node Patches","text":"<pre><code>just patch\n</code></pre> <p>Each node gets its own patch applied on top of the base config, setting hostname, install image, network interfaces, and VIP assignments. The patched configs are written to <code>clusterconfig/worker-XX.yaml</code>.</p> <p>Control Plane Nodes Use controlplane.yaml as Base</p> <p>worker-01, worker-02, and worker-03 are control plane nodes despite their naming. Their per-node patches are applied on top of <code>controlplane.yaml</code>, not <code>worker.yaml</code>.</p>"},{"location":"infrastructure/talos-linux/#key-patches","title":"Key Patches","text":""},{"location":"infrastructure/talos-linux/#general-patch-all-nodes","title":"General Patch (all nodes)","text":"patches/general.patch<pre><code>machine:\n  kubelet:\n    extraArgs:\n      rotate-server-certificates: true\n    extraConfig:\n      imageGCHighThresholdPercent: 60\n      imageGCLowThresholdPercent: 50\n    extraMounts:\n      - destination: /var/mnt/extra\n        type: bind\n        source: /var/mnt/extra\n        options:\n          - rbind\n          - rshared\n          - rw\n  features:\n    hostDNS:\n      enabled: true\n      forwardKubeDNSToHost: false\n      resolveMemberNames: true\ncluster:\n  network:\n    cni:\n      name: none\n  proxy:\n    disabled: true\n</code></pre> Setting Purpose <code>rotate-server-certificates</code> Enables automatic kubelet server certificate rotation <code>imageGCHighThresholdPercent: 60</code> Triggers image garbage collection when disk usage exceeds 60% <code>imageGCLowThresholdPercent: 50</code> Stops GC when disk usage drops below 50% Extra mount <code>/var/mnt/extra</code> Provides a writable bind mount for workloads that need host-level storage <code>hostDNS.enabled</code> Enables Talos host-level DNS resolution <code>resolveMemberNames</code> Allows resolving cluster member names via host DNS <code>cni.name: none</code> Disables default CNI -- Cilium is installed as a post-bootstrap addon <code>proxy.disabled: true</code> Disables kube-proxy -- Cilium operates in kube-proxy replacement mode"},{"location":"infrastructure/talos-linux/#control-plane-patch","title":"Control Plane Patch","text":"patches/controlplane.patch<pre><code>cluster:\n  allowSchedulingOnControlPlanes: true\n  coreDNS:\n    disabled: true\n  apiServer:\n    certSANs:\n      - 127.0.0.1\n    extraArgs:\n      service-account-issuer: https://raw.githubusercontent.com/swibrow/home-ops/main/pitower/kubernetes\n      service-account-jwks-uri: https://k8s.cluster.internal:6443/openid/v1/jwks\n</code></pre> Setting Purpose <code>allowSchedulingOnControlPlanes</code> Permits workloads on control plane nodes to maximize resource use <code>coreDNS.disabled</code> Disables built-in CoreDNS -- DNS is handled by Cilium or an alternative <code>certSANs: [127.0.0.1]</code> Adds localhost to the API server certificate SANs <code>service-account-issuer</code> Sets the OIDC issuer URL for service account tokens to a GitHub-hosted endpoint <code>service-account-jwks-uri</code> JWKS endpoint for verifying service account tokens"},{"location":"infrastructure/talos-linux/#per-node-patches","title":"Per-Node Patches","text":"<p>Each node has a patch under <code>patches/nodes/</code> that sets:</p> <ul> <li>Hostname (e.g., <code>worker-01</code>)</li> <li>Install image (factory image URL with schematic ID and Talos version)</li> <li>Network interfaces (DHCP, VIP assignment for control plane nodes)</li> <li>Install disk (where applicable, e.g., <code>/dev/mmcblk0</code> for eMMC)</li> </ul> <p>Example control plane node patch:</p> patches/nodes/worker-01.patch<pre><code>machine:\n  install:\n    image: factory.talos.dev/installer/f19ad7b4...:v1.12.4\n  network:\n    hostname: worker-01\n    interfaces:\n      - deviceSelector:\n          physical: true\n        dhcp: true\n        vip:\n          ip: 192.168.0.200\n</code></pre> <p>Example Raspberry Pi worker patch:</p> patches/nodes/worker-pi-01.patch<pre><code>machine:\n  network:\n    hostname: worker-pi-01\n    interfaces:\n      - deviceSelector:\n          physical: true\n        dhcp: true\n  install:\n    image: factory.talos.dev/installer/de94b242...:v1.12.4\n    extraKernelArgs:\n      - initcall_blacklist=sensors_nct6683_init\n</code></pre> <p>Extra Kernel Args</p> <p>The Raspberry Pi workers include <code>initcall_blacklist=sensors_nct6683_init</code> to prevent a kernel module from causing issues on ARM hardware.</p>"},{"location":"monitoring/","title":"Monitoring","text":"<p>The cluster uses a comprehensive observability stack built on Prometheus for metrics, Loki for logs, and Grafana for visualization. All components are deployed in the <code>monitoring</code> namespace and managed via Helm charts through ArgoCD.</p>"},{"location":"monitoring/#overview","title":"Overview","text":"<pre><code>flowchart LR\n    subgraph Applications\n        App1[Cilium / Hubble]\n        App2[external-dns]\n        App3[cloudflared]\n        App4[Authelia]\n        App5[Other Apps]\n    end\n\n    subgraph Metrics Pipeline\n        SM[ServiceMonitors]\n        Prom[Prometheus]\n    end\n\n    subgraph Logs Pipeline\n        FB[Fluent Bit]\n        Loki[Loki]\n    end\n\n    Grafana[Grafana]\n\n    App1 &amp; App2 &amp; App3 &amp; App4 &amp; App5 --&gt;|expose /metrics| SM\n    SM --&gt;|scrape targets| Prom\n    Prom --&gt;|query metrics| Grafana\n\n    App1 &amp; App2 &amp; App3 &amp; App4 &amp; App5 --&gt;|stdout/stderr| FB\n    FB --&gt;|forward logs| Loki\n    Loki --&gt;|query logs| Grafana</code></pre>"},{"location":"monitoring/#observability-strategy","title":"Observability Strategy","text":"<p>The monitoring stack follows a pull-based model for metrics and a push-based model for logs:</p> <ul> <li>Metrics -- Applications expose Prometheus-compatible <code>/metrics</code> endpoints. <code>ServiceMonitor</code> resources tell Prometheus where to scrape. The kube-prometheus-stack provides built-in monitoring for Kubernetes internals (kubelet, API server, etcd, controller manager, scheduler).</li> <li>Logs -- Fluent Bit runs as a DaemonSet on every node, tailing container log files from <code>/var/log/containers/</code> and forwarding them to Loki. Logs are queryable via LogQL in Grafana.</li> <li>Dashboards -- Grafana auto-provisions dashboards from ConfigMaps labeled <code>grafana_dashboard: \"true\"</code> and organizes them into folders using the <code>grafana_folder</code> annotation. Additional dashboards are loaded from Grafana.com and upstream project repositories.</li> </ul> <p>No Alertmanager</p> <p>Alertmanager is currently disabled in this cluster. Alerting can be enabled in the kube-prometheus-stack values when needed.</p>"},{"location":"monitoring/#components","title":"Components","text":"Component Helm Chart Version Purpose kube-prometheus-stack <code>prometheus-community/kube-prometheus-stack</code> 81.6.9 Prometheus, node-exporter, kube-state-metrics, recording rules Grafana <code>grafana/grafana</code> 10.5.15 Dashboard visualization, SSO via Authelia Loki <code>grafana/loki</code> 6.51.0 Log aggregation and storage Fluent Bit <code>fluent/fluent-bit</code> 0.55.0 Log collection from all nodes"},{"location":"monitoring/#namespace-configuration","title":"Namespace Configuration","text":"<p>The <code>monitoring</code> namespace runs with privileged pod security standards to accommodate node-exporter and Fluent Bit, which require host-level access:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring\n  labels:\n    pod-security.kubernetes.io/audit: privileged\n    pod-security.kubernetes.io/enforce: privileged\n    pod-security.kubernetes.io/warn: privileged\n</code></pre>"},{"location":"monitoring/#key-endpoints","title":"Key Endpoints","text":"Service URL Gateway Prometheus <code>https://prometheus.example.com</code> envoy-internal Grafana <code>https://grafana.example.com</code> envoy-external"},{"location":"monitoring/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Separate Grafana deployment -- Grafana is deployed as its own Helm release rather than the one bundled in kube-prometheus-stack, allowing independent upgrades and more flexible configuration.</li> <li>Sidecar-based dashboard discovery -- The Grafana sidecar watches all namespaces for ConfigMaps with the <code>grafana_dashboard</code> label, so any application can ship its own dashboards.</li> <li>SingleBinary Loki -- Loki runs in single-binary mode with filesystem storage on OpenEBS, keeping the deployment simple for a single-cluster setup.</li> <li>Fluent Bit over Promtail -- Fluent Bit was chosen for log collection due to its low resource footprint and flexible filtering pipeline.</li> <li>WAL compression -- Prometheus uses WAL compression to reduce disk usage on the 20Gi Ceph-backed PVC.</li> </ul>"},{"location":"monitoring/fluent-bit/","title":"Fluent Bit","text":"<p>Fluent Bit is a lightweight log processor and forwarder that runs as a DaemonSet on every node in the cluster. It tails container log files, enriches them with Kubernetes metadata, and forwards them to Loki for storage and querying.</p>"},{"location":"monitoring/fluent-bit/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Node\n        Containers[Container\\nLog Files]\n        FB[Fluent Bit]\n    end\n\n    Loki[Loki]\n    Grafana[Grafana]\n\n    Containers --&gt;|\"/var/log/containers/*.log\"| FB\n    FB --&gt;|push via Loki output plugin| Loki\n    Grafana --&gt;|LogQL| Loki</code></pre> <p>Fluent Bit runs as a DaemonSet, ensuring one instance per node. Each instance reads container logs from the node's filesystem and processes them through a pipeline of inputs, filters, and outputs.</p>"},{"location":"monitoring/fluent-bit/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>The Fluent Bit pipeline has four stages: service configuration, input, filters, and output.</p>"},{"location":"monitoring/fluent-bit/#service","title":"Service","text":"<pre><code>[SERVICE]\n    Daemon Off\n    Flush 1\n    Log_Level info\n    Parsers_File /fluent-bit/etc/parsers.conf\n    Parsers_File /fluent-bit/etc/conf/custom_parsers.conf\n    HTTP_Server On\n    HTTP_Listen 0.0.0.0\n    HTTP_Port 2020\n    Health_Check On\n</code></pre> <p>The HTTP server exposes metrics and a health check endpoint for Kubernetes liveness/readiness probes.</p>"},{"location":"monitoring/fluent-bit/#input","title":"Input","text":"<pre><code>[INPUT]\n    name tail\n    alias kubernetes\n    path /var/log/containers/*.log\n    parser containerd\n    tag kubernetes.*\n</code></pre> Setting Value Purpose Plugin <code>tail</code> Follows log files like <code>tail -f</code> Path <code>/var/log/containers/*.log</code> All container log files on the node Parser <code>containerd</code> Parses the containerd log format Tag <code>kubernetes.*</code> Tags all records for downstream matching"},{"location":"monitoring/fluent-bit/#filters","title":"Filters","text":"<p>Fluent Bit applies three filters to enrich and reshape log records:</p>"},{"location":"monitoring/fluent-bit/#1-kubernetes-metadata-enrichment","title":"1. Kubernetes Metadata Enrichment","text":"<pre><code>[FILTER]\n    name kubernetes\n    alias kubernetes\n    match kubernetes.*\n    buffer_size 0\n    merge_log on\n    kube_tag_prefix kubernetes.var.log.containers.\n    k8s-logging.parser on\n    k8s-logging.exclude on\n    namespace_labels off\n    annotations off\n</code></pre> <p>This filter queries the Kubernetes API to enrich each log record with metadata: pod name, namespace, container name, and labels. The <code>merge_log on</code> setting parses JSON-formatted log messages and merges their fields into the top-level record.</p> <p>k8s-logging annotations</p> <p>When <code>k8s-logging.exclude</code> is enabled, pods with the annotation <code>fluentbit.io/exclude: \"true\"</code> will have their logs excluded from collection. Similarly, <code>k8s-logging.parser</code> allows pods to specify a custom parser via annotations.</p>"},{"location":"monitoring/fluent-bit/#2-source-label","title":"2. Source Label","text":"<pre><code>[FILTER]\n    name modify\n    match kubernetes.*\n    add source kubernetes\n    remove logtag\n</code></pre> <p>Adds a <code>source=kubernetes</code> field to every record and removes the unnecessary <code>logtag</code> field.</p>"},{"location":"monitoring/fluent-bit/#3-metadata-flattening","title":"3. Metadata Flattening","text":"<pre><code>[FILTER]\n    name nest\n    match *\n    wildcard pod_name\n    operation lift\n    nested_under kubernetes\n    add_prefix kubernetes_\n</code></pre> <p>Lifts Kubernetes metadata fields out of the nested <code>kubernetes</code> object and flattens them with a <code>kubernetes_</code> prefix. This makes fields like <code>kubernetes_namespace_name</code> and <code>kubernetes_pod_name</code> available as top-level labels in Loki.</p>"},{"location":"monitoring/fluent-bit/#output","title":"Output","text":"<pre><code>[OUTPUT]\n    Name loki\n    Match kubernetes.*\n    host loki-headless.monitoring.svc.cluster.local\n    port 3100\n    line_format json\n    labels job=fluent-bit, cluster=home-ops, source=$source, namespace=$kubernetes_namespace_name\n</code></pre> Setting Value Purpose Plugin <code>loki</code> Native Loki output plugin Host <code>loki-headless.monitoring.svc.cluster.local</code> Loki's headless service in the monitoring namespace Port <code>3100</code> Loki's HTTP API port Line format <code>json</code> Log lines are stored as JSON Labels <code>job</code>, <code>cluster</code>, <code>source</code>, <code>namespace</code> Loki index labels for efficient querying <p>Label Cardinality</p> <p>The labels are kept intentionally minimal (<code>job</code>, <code>cluster</code>, <code>source</code>, <code>namespace</code>) to avoid high cardinality in Loki's index. Pod-level details are available in the log line itself and can be queried with LogQL's parser expressions rather than being stored as index labels.</p>"},{"location":"monitoring/fluent-bit/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>flowchart TD\n    Input[\"INPUT: tail\\n/var/log/containers/*.log\"]\n    F1[\"FILTER: kubernetes\\nEnrich with K8s metadata\"]\n    F2[\"FILTER: modify\\nAdd source=kubernetes\"]\n    F3[\"FILTER: nest\\nFlatten kubernetes_ fields\"]\n    Output[\"OUTPUT: loki\\nloki-headless:3100\"]\n\n    Input --&gt; F1 --&gt; F2 --&gt; F3 --&gt; Output</code></pre>"},{"location":"monitoring/fluent-bit/#querying-fluent-bit-logs-in-grafana","title":"Querying Fluent Bit Logs in Grafana","text":"<p>Once logs reach Loki, they can be queried in Grafana using LogQL. The labels set by Fluent Bit's output plugin are available as selectors:</p> <pre><code># All logs from the networking namespace\n{job=\"fluent-bit\", namespace=\"networking\"}\n\n# Logs from the cluster (useful in multi-cluster setups)\n{cluster=\"home-ops\"}\n\n# Filter by source\n{source=\"kubernetes\"}\n</code></pre> <p>See the Loki documentation for more LogQL query examples.</p>"},{"location":"monitoring/fluent-bit/#health-and-metrics","title":"Health and Metrics","text":"<p>Fluent Bit exposes an HTTP server on port 2020 that provides:</p> <ul> <li><code>/api/v1/health</code> -- health check endpoint used by Kubernetes probes</li> <li><code>/api/v1/metrics/prometheus</code> -- Prometheus-format metrics for monitoring Fluent Bit itself</li> </ul>"},{"location":"monitoring/fluent-bit/#helm-chart-reference","title":"Helm Chart Reference","text":"Property Value Chart <code>fluent/fluent-bit</code> Version <code>0.55.0</code> Namespace <code>monitoring</code> Deployment type DaemonSet (one per node) Manifest path <code>pitower/kubernetes/apps/monitoring/fluent-bit/</code>"},{"location":"monitoring/grafana/","title":"Grafana","text":"<p>Grafana serves as the central visualization layer for the cluster. It connects to Prometheus for metrics and Loki for logs, and auto-provisions dashboards from ConfigMaps across all namespaces. Authentication is handled via Authelia OIDC.</p>"},{"location":"monitoring/grafana/#data-sources","title":"Data Sources","text":"<p>Grafana is configured with four data sources, all provisioned declaratively through the Helm values:</p> Data Source Type URL Default Prometheus <code>prometheus</code> <code>http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090</code> Yes Loki <code>loki</code> <code>http://loki-headless.monitoring.svc.cluster.local:3100</code> No Alertmanager <code>alertmanager</code> <code>http://alertmanager.monitoring.svc.cluster.local:9093</code> No GitHub <code>grafana-github-datasource</code> N/A (API-based) No <pre><code>datasources:\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n      - name: Prometheus\n        type: prometheus\n        uid: prometheus\n        access: proxy\n        url: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090\n        isDefault: true\n      - name: Loki\n        type: loki\n        access: proxy\n        url: http://loki-headless.monitoring.svc.cluster.local:3100\n</code></pre>"},{"location":"monitoring/grafana/#dashboard-auto-provisioning","title":"Dashboard Auto-Provisioning","text":"<p>Grafana uses two complementary mechanisms for dashboard management:</p>"},{"location":"monitoring/grafana/#1-sidecar-discovery-cross-namespace","title":"1. Sidecar Discovery (Cross-Namespace)","text":"<p>The Grafana sidecar watches all namespaces for ConfigMaps with the label <code>grafana_dashboard: \"true\"</code>. Dashboards are organized into folders using the <code>grafana_folder</code> annotation on the ConfigMap.</p> <pre><code>sidecar:\n  dashboards:\n    enabled: true\n    searchNamespace: ALL\n    label: grafana_dashboard\n    folderAnnotation: grafana_folder\n    provider:\n      disableDelete: true\n      foldersFromFilesStructure: true\n</code></pre> <p>This allows any application to ship its own dashboard by creating a ConfigMap in its own namespace. For example, Cilium's Helm chart creates dashboard ConfigMaps with:</p> <pre><code>dashboards:\n  enabled: true\n  annotations:\n    grafana_folder: Cilium\n</code></pre> <p>Similarly, Rook Ceph dashboards use a Kustomize <code>configMapGenerator</code> with the appropriate labels and annotations:</p> <pre><code>generatorOptions:\n  annotations:\n    grafana_folder: Rook CEPH\n  labels:\n    grafana_dashboard: \"true\"\n</code></pre> <pre><code>flowchart LR\n    subgraph kube-system\n        CM1[Cilium Dashboard\\nConfigMap]\n    end\n    subgraph rook-ceph\n        CM2[Ceph Dashboard\\nConfigMap]\n    end\n    subgraph monitoring\n        Sidecar[Grafana Sidecar]\n        Grafana[Grafana]\n    end\n\n    CM1 --&gt;|\"label: grafana_dashboard\\nannotation: grafana_folder=Cilium\"| Sidecar\n    CM2 --&gt;|\"label: grafana_dashboard\\nannotation: grafana_folder=Rook CEPH\"| Sidecar\n    Sidecar --&gt;|provisions| Grafana</code></pre>"},{"location":"monitoring/grafana/#2-dashboard-providers-grafana-values","title":"2. Dashboard Providers (Grafana Values)","text":"<p>Dashboards can also be defined directly in the Grafana Helm values. These are organized into named providers, each mapping to a folder in Grafana:</p> Provider Folder Dashboards <code>default</code> (root) Authelia, cloudflared, external-dns, external-secrets, cert-manager, node-exporter, CloudNativePG <code>flux</code> Flux Flux cluster overview, control plane <code>kubernetes</code> Kubernetes API server, CoreDNS, global views, namespaces, nodes, pods <code>nginx</code> Nginx Request metrics, handling performance <code>ceph</code> Ceph Cluster overview, OSD, pools <code>github</code> GitHub Repository insights <p>Dashboard Sources</p> <p>Dashboards are loaded from three types of sources:</p> <ul> <li>Grafana.com via <code>gnetId</code> -- e.g., Node Exporter Full (ID: 1860)</li> <li>Raw URLs -- JSON files from upstream project repositories (e.g., Flux, Kubernetes, cert-manager)</li> <li>ConfigMaps -- shipped by Helm charts (e.g., Cilium, Rook Ceph) via the sidecar</li> </ul>"},{"location":"monitoring/grafana/#key-dashboard-categories","title":"Key Dashboard Categories","text":""},{"location":"monitoring/grafana/#cilium-and-hubble","title":"Cilium and Hubble","text":"<p>Cilium and Hubble dashboards are provisioned via the sidecar into the Cilium folder. They cover:</p> <ul> <li>Cilium agent health and eBPF datapath metrics</li> <li>Cilium operator status and IPAM allocation</li> <li>Hubble network observability: DNS queries, TCP connections, HTTP requests, drops, ICMP, flow counts, and port distribution</li> </ul>"},{"location":"monitoring/grafana/#kubernetes","title":"Kubernetes","text":"<p>The Kubernetes folder includes dashboards from the dotdc/grafana-dashboards-kubernetes project:</p> <ul> <li>API Server performance and request rates</li> <li>CoreDNS query metrics</li> <li>Global cluster resource utilization</li> <li>Per-namespace resource breakdown</li> <li>Per-node CPU, memory, and disk usage</li> <li>Per-pod resource consumption</li> </ul>"},{"location":"monitoring/grafana/#node-health","title":"Node Health","text":"<p>The Node Exporter Full dashboard (Grafana.com ID: 1860) provides deep hardware-level visibility into each node: CPU, memory, disk I/O, network, filesystem usage, and system load.</p>"},{"location":"monitoring/grafana/#storage","title":"Storage","text":"<p>Ceph dashboards in the Ceph folder show cluster health, OSD performance, and pool utilization -- critical for monitoring the distributed block storage backing most PVCs.</p>"},{"location":"monitoring/grafana/#networking","title":"Networking","text":"<p>Nginx dashboards track request rates, latency distributions, and error rates for both the external and internal ingress controllers.</p>"},{"location":"monitoring/grafana/#authentication","title":"Authentication","text":"<p>Grafana authenticates users via Authelia using OpenID Connect (OIDC). The configuration maps Authelia groups to Grafana roles:</p> <pre><code>grafana.ini:\n  auth.generic_oauth:\n    enabled: true\n    name: Authelia\n    client_id: grafana\n    scopes: openid profile email groups\n    auth_url: https://auth.example.com/api/oidc/authorization\n    token_url: https://auth.example.com/api/oidc/token\n    api_url: https://auth.example.com/api/oidc/userinfo\n    use_pkce: true\n  auth.generic_oauth.group_mapping:\n    role_attribute_path: |\n      contains(groups[*], 'admins') &amp;&amp; 'Admin' || contains(groups[*], 'people') &amp;&amp; 'Viewer'\n</code></pre> Authelia Group Grafana Role <code>admins</code> Admin <code>people</code> Viewer"},{"location":"monitoring/grafana/#secrets-management","title":"Secrets Management","text":"<p>Grafana credentials are managed through two ExternalSecrets:</p> Secret Source Contents <code>grafana-admin-secret</code> Infisical (<code>/monitoring/grafana/</code>) <code>admin-user</code>, <code>admin-password</code> <code>grafana-secrets</code> 1Password Connect (<code>grafana</code>) <code>GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET</code> <p>The deployment uses <code>reloader.stakater.com/auto: \"true\"</code> to automatically restart Grafana when secrets are updated.</p>"},{"location":"monitoring/grafana/#plugins","title":"Plugins","text":"<p>The following Grafana plugins are installed:</p> Plugin Purpose <code>grafana-worldmap-panel</code> Geographic visualization of data <code>grafana-clock-panel</code> Clock display for dashboards <code>grafana-github-datasource</code> GitHub repository metrics and insights"},{"location":"monitoring/grafana/#access","title":"Access","text":"<p>Grafana is exposed externally via the Cloudflare tunnel through Envoy Gateway:</p> <pre><code>route:\n  main:\n    enabled: true\n    hostnames:\n      - grafana.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\n</code></pre> <p>This makes Grafana accessible at <code>https://grafana.example.com</code> from anywhere via Cloudflare.</p>"},{"location":"monitoring/grafana/#configuration-reference","title":"Configuration Reference","text":"Property Value Chart <code>grafana/grafana</code> Version <code>10.5.15</code> Namespace <code>monitoring</code> Persistence Disabled (dashboards are provisioned, no state to persist) Image Renderer Enabled (for PNG rendering in alerts and sharing) Manifest path <code>pitower/kubernetes/apps/monitoring/grafana/</code>"},{"location":"monitoring/loki/","title":"Loki","text":"<p>Loki is a horizontally scalable log aggregation system inspired by Prometheus. In the cluster, Loki runs in SingleBinary mode and receives logs from Fluent Bit, storing them on local disk via OpenEBS.</p>"},{"location":"monitoring/loki/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Nodes\n        FB1[Fluent Bit\\nNode 1]\n        FB2[Fluent Bit\\nNode 2]\n        FB3[Fluent Bit\\nNode N]\n    end\n\n    subgraph monitoring namespace\n        Loki[Loki\\nSingleBinary]\n        PV[(OpenEBS\\n20Gi PVC)]\n    end\n\n    Grafana[Grafana]\n\n    FB1 &amp; FB2 &amp; FB3 --&gt;|push logs| Loki\n    Loki --&gt;|persist| PV\n    Grafana --&gt;|LogQL queries| Loki</code></pre>"},{"location":"monitoring/loki/#deployment-mode","title":"Deployment Mode","text":"<p>Loki is deployed in SingleBinary mode with a single replica. This keeps the deployment simple and resource-efficient for a home lab environment. All Loki components (ingester, querier, compactor, ruler) run in a single process.</p> <pre><code>deploymentMode: SingleBinary\n\nsingleBinary:\n  replicas: 1\n  persistence:\n    enabled: true\n    storageClass: openebs-hostpath\n    size: 20Gi\n</code></pre> <p>Why SingleBinary?</p> <p>For a single-cluster home lab, the SingleBinary deployment mode avoids the operational complexity of running separate read, write, and backend components. It trades horizontal scalability for simplicity -- a good fit when log volume is moderate.</p> <p>All distributed mode components are explicitly disabled:</p> <pre><code>backend:\n  replicas: 0\nread:\n  replicas: 0\nwrite:\n  replicas: 0\ngateway:\n  replicas: 0\nchunksCache:\n  enabled: false\nresultsCache:\n  enabled: false\nlokiCanary:\n  enabled: false\n</code></pre>"},{"location":"monitoring/loki/#storage","title":"Storage","text":"<p>Loki uses filesystem storage backed by an OpenEBS hostpath PVC. Logs are stored locally on the node where Loki is scheduled.</p> Setting Value Storage type <code>filesystem</code> StorageClass <code>openebs-hostpath</code> PVC size <code>20Gi</code> Schema <code>v13</code> (TSDB index) Index prefix <code>loki_index_</code> Index period <code>24h</code> <pre><code>loki:\n  schemaConfig:\n    configs:\n      - from: \"2024-04-01\"\n        store: tsdb\n        index:\n          prefix: loki_index_\n          period: 24h\n        object_store: filesystem\n        schema: v13\n  storage:\n    type: filesystem\n</code></pre> <p>Single Point of Failure</p> <p>With <code>openebs-hostpath</code> and a single replica, Loki's storage is tied to one node. If that node fails, log data is unavailable until it recovers. This is an acceptable trade-off for a home lab where logs are primarily used for debugging, not compliance.</p>"},{"location":"monitoring/loki/#retention","title":"Retention","text":"<p>Loki is configured with a 14-day retention period. The compactor runs in the background to delete expired log chunks:</p> <pre><code>loki:\n  compactor:\n    working_directory: /var/loki/compactor/retention\n    delete_request_store: filesystem\n    retention_enabled: true\n  limits_config:\n    retention_period: 14d\n</code></pre>"},{"location":"monitoring/loki/#configuration","title":"Configuration","text":"<p>Key Loki settings:</p> Setting Value Purpose <code>auth_enabled</code> <code>false</code> No multi-tenancy -- single tenant mode <code>replication_factor</code> <code>1</code> Single replica, no replication <code>chunk_encoding</code> <code>snappy</code> Fast compression for log chunks <code>retention_period</code> <code>14d</code> Logs older than 14 days are deleted"},{"location":"monitoring/loki/#querying-with-logql","title":"Querying with LogQL","text":"<p>Loki uses LogQL, a log query language inspired by PromQL. Queries can be run in the Grafana Explore view with the Loki data source selected.</p>"},{"location":"monitoring/loki/#common-queries","title":"Common Queries","text":"<p>All logs from a specific namespace:</p> <pre><code>{namespace=\"networking\"}\n</code></pre> <p>Logs from a specific pod:</p> <pre><code>{namespace=\"monitoring\", pod=~\"loki.*\"}\n</code></pre> <p>Filter logs containing an error:</p> <pre><code>{namespace=\"networking\"} |= \"error\"\n</code></pre> <p>Parse JSON logs and filter by field:</p> <pre><code>{namespace=\"networking\", job=\"fluent-bit\"} | json | level=\"error\"\n</code></pre> <p>Count log lines per namespace over time:</p> <pre><code>sum by (namespace) (rate({job=\"fluent-bit\"}[5m]))\n</code></pre> <p>Top 10 pods by log volume:</p> <pre><code>topk(10, sum by (pod) (rate({job=\"fluent-bit\"}[1h])))\n</code></pre> <p>Explore in Grafana</p> <p>Open Grafana at <code>https://grafana.example.com</code>, navigate to Explore, select the Loki data source, and start querying. Use the label browser to discover available labels like <code>namespace</code>, <code>pod</code>, <code>container</code>, and <code>source</code>.</p>"},{"location":"monitoring/loki/#integration-with-grafana","title":"Integration with Grafana","text":"<p>Loki is configured as a data source in Grafana at the cluster-internal URL:</p> <pre><code>- name: Loki\n  type: loki\n  access: proxy\n  url: http://loki-headless.monitoring.svc.cluster.local:3100\n</code></pre> <p>This enables:</p> <ul> <li>Explore -- ad-hoc log querying with LogQL</li> <li>Dashboard panels -- embedding log panels alongside metric panels</li> <li>Annotations -- overlaying log events on time-series graphs</li> <li>Alerting -- creating alert rules based on log patterns (when Alertmanager is enabled)</li> </ul>"},{"location":"monitoring/loki/#sidecar-rules","title":"Sidecar Rules","text":"<p>The Loki Helm chart includes a sidecar that watches all namespaces for recording and alerting rules:</p> <pre><code>sidecar:\n  rules:\n    searchNamespace: ALL\n</code></pre> <p>This allows applications in any namespace to define Loki recording rules via ConfigMaps.</p>"},{"location":"monitoring/loki/#helm-chart-reference","title":"Helm Chart Reference","text":"Property Value Chart <code>grafana/loki</code> Version <code>6.51.0</code> Namespace <code>monitoring</code> Manifest path <code>pitower/kubernetes/apps/monitoring/loki/</code>"},{"location":"monitoring/prometheus-stack/","title":"kube-prometheus-stack","text":"<p>The kube-prometheus-stack Helm chart deploys a complete Prometheus monitoring pipeline. In the cluster, it provides Prometheus server, node-exporter, kube-state-metrics, and a curated set of recording and alerting rules for Kubernetes internals.</p>"},{"location":"monitoring/prometheus-stack/#whats-included","title":"What's Included","text":"Component Purpose Prometheus Time-series database that scrapes and stores metrics node-exporter Exposes hardware and OS-level metrics from each node kube-state-metrics Generates metrics about the state of Kubernetes objects (pods, deployments, PVCs) PrometheusOperator Manages Prometheus instances and watches for ServiceMonitor/PodMonitor CRDs Recording Rules Pre-computed queries for common Kubernetes metrics <p>Grafana and Alertmanager</p> <p>Grafana is deployed as a separate Helm release for independent lifecycle management. Alertmanager is currently disabled (<code>alertmanager.enabled: false</code>).</p>"},{"location":"monitoring/prometheus-stack/#prometheus-configuration","title":"Prometheus Configuration","text":"<p>Prometheus is configured with the following key settings:</p> <pre><code>prometheusSpec:\n  externalLabels:\n    cluster: home-ops\n  ruleSelectorNilUsesHelmValues: false\n  serviceMonitorSelectorNilUsesHelmValues: false\n  podMonitorSelectorNilUsesHelmValues: false\n  probeSelectorNilUsesHelmValues: false\n  scrapeConfigSelectorNilUsesHelmValues: false\n  enableAdminAPI: true\n  walCompression: true\n  retentionSize: 15GB\n  storageSpec:\n    volumeClaimTemplate:\n      spec:\n        storageClassName: ceph-block\n        resources:\n          requests:\n            storage: 20Gi\n</code></pre> <p>Selector Configuration</p> <p>All <code>*SelectorNilUsesHelmValues: false</code> settings ensure Prometheus discovers ServiceMonitors, PodMonitors, ProbeMonitors, and recording rules from all namespaces -- not just those created by the Helm chart. This is essential for applications in other namespaces to be scraped.</p>"},{"location":"monitoring/prometheus-stack/#storage","title":"Storage","text":"<p>Prometheus stores its TSDB on a 20Gi Ceph block volume (<code>ceph-block</code> StorageClass). WAL compression is enabled to reduce write amplification and disk usage. The retention policy is size-based at 15GB, meaning Prometheus will automatically prune old data when the TSDB approaches this limit.</p>"},{"location":"monitoring/prometheus-stack/#access","title":"Access","text":"<p>Prometheus is exposed internally via Envoy Gateway:</p> <pre><code>route:\n  main:\n    enabled: true\n    hostnames:\n      - prometheus.example.com\n    parentRefs:\n      - name: envoy-internal\n        namespace: networking\n        sectionName: https\n</code></pre> <p>This makes Prometheus available at <code>https://prometheus.example.com</code> for internal/VPN users only.</p>"},{"location":"monitoring/prometheus-stack/#servicemonitor-pattern","title":"ServiceMonitor Pattern","text":"<p>The cluster uses <code>ServiceMonitor</code> resources extensively to define scrape targets. A ServiceMonitor tells Prometheus which services to scrape, on which port, and at which path. The Prometheus Operator watches for these CRDs and automatically configures Prometheus scrape jobs.</p> <pre><code>flowchart LR\n    App[Application Pod] --&gt;|exposes /metrics| Svc[Kubernetes Service]\n    SM[ServiceMonitor] --&gt;|selects| Svc\n    PO[Prometheus Operator] --&gt;|watches| SM\n    PO --&gt;|configures| Prom[Prometheus]\n    Prom --&gt;|scrapes| Svc</code></pre>"},{"location":"monitoring/prometheus-stack/#applications-with-servicemonitors","title":"Applications with ServiceMonitors","text":"<p>The following applications across the cluster expose ServiceMonitors:</p> Application Namespace Metrics Cilium Agent <code>kube-system</code> eBPF datapath, policy, endpoint metrics Cilium Operator <code>kube-system</code> Operator health, IPAM allocation Hubble <code>kube-system</code> DNS, TCP, HTTP, ICMP, flow, drop, port-distribution Hubble Relay <code>kube-system</code> Relay connection and forwarding metrics external-dns <code>networking</code> DNS record sync metrics cloudflared <code>networking</code> Tunnel connection metrics nginx (external) <code>networking</code> HTTP request metrics nginx (internal) <code>networking</code> HTTP request metrics Authelia <code>security</code> Authentication and authorization metrics External Secrets Operator <code>security</code> Secret sync metrics Grafana <code>monitoring</code> Dashboard rendering, data source query metrics snapshot-controller <code>system</code> Volume snapshot metrics metrics-server <code>kube-system</code> API metrics"},{"location":"monitoring/prometheus-stack/#creating-a-servicemonitor","title":"Creating a ServiceMonitor","text":"<p>To add monitoring for a new application, create a ServiceMonitor in the application's namespace. Example for an app using the bjw-s app-template:</p> <pre><code># In your app's values.yaml\nservice:\n  main:\n    ports:\n      http:\n        port: 8080\n      metrics:\n        port: 9090\n\nserviceMonitor:\n  main:\n    enabled: true\n    endpoints:\n      - port: metrics\n        interval: 1m\n</code></pre> <p>For Helm charts that don't have built-in ServiceMonitor support, create one manually:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: my-app\n  namespace: my-namespace\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: my-app\n  endpoints:\n    - port: metrics\n      interval: 1m\n      path: /metrics\n</code></pre>"},{"location":"monitoring/prometheus-stack/#kubernetes-component-monitoring","title":"Kubernetes Component Monitoring","text":"<p>The kube-prometheus-stack scrapes all major Kubernetes control plane components:</p> Component Enabled Endpoints kubelet Yes Auto-discovered kube-apiserver Yes Auto-discovered kube-controller-manager Yes <code>192.168.0.201</code>, <code>192.168.0.202</code>, <code>192.168.0.203</code> kube-scheduler Yes <code>192.168.0.201</code>, <code>192.168.0.202</code>, <code>192.168.0.203</code> etcd Yes <code>192.168.0.201</code>, <code>192.168.0.202</code>, <code>192.168.0.203</code> kube-proxy No Disabled (Cilium replaces kube-proxy via eBPF) <p>Static Endpoints</p> <p>The controller-manager, scheduler, and etcd endpoints are statically configured to the three control plane node IPs because Talos Linux does not expose these components as Kubernetes services.</p>"},{"location":"monitoring/prometheus-stack/#metric-relabeling","title":"Metric Relabeling","text":"<p>The stack applies metric relabeling rules to reduce cardinality and filter out unnecessary metrics. Each component has a keep-list regex that retains only the metrics that are actually used in dashboards and alerts.</p> <p>For example, the kubelet ServiceMonitor keeps only metrics matching prefixes like <code>container_cpu</code>, <code>container_memory</code>, <code>kubelet_*</code>, and drops high-cardinality labels like <code>uid</code>, <code>id</code>, and <code>name</code>:</p> <pre><code>metricRelabelings:\n  - action: keep\n    sourceLabels: [\"__name__\"]\n    regex: (container_cpu|container_memory|kubelet_*|...)_(.+)\n  - action: labeldrop\n    regex: (uid)\n  - action: labeldrop\n    regex: (id|name)\n</code></pre> <p>This keeps storage costs down and query performance high on the 20Gi PVC.</p>"},{"location":"monitoring/prometheus-stack/#kube-state-metrics","title":"kube-state-metrics","text":"<p>kube-state-metrics is configured to expose all labels on key resource types, which enables label-based filtering in Grafana dashboards:</p> <pre><code>kube-state-metrics:\n  metricLabelsAllowlist:\n    - \"deployments=[*]\"\n    - \"persistentvolumeclaims=[*]\"\n    - \"pods=[*]\"\n</code></pre> <p>A relabeling rule also adds the <code>kubernetes_node</code> label to every metric, derived from the pod's node name, enabling per-node breakdowns in dashboards.</p>"},{"location":"monitoring/prometheus-stack/#helm-chart-reference","title":"Helm Chart Reference","text":"Property Value Chart <code>prometheus-community/kube-prometheus-stack</code> Version <code>81.6.9</code> Namespace <code>monitoring</code> Manifest path <code>pitower/kubernetes/apps/monitoring/kube-prometheus-stack/</code>"},{"location":"networking/","title":"Networking","text":"<p>The cluster implements a multi-gateway networking architecture built on Cilium as the CNI, Envoy Gateway for ingress, Cloudflare for DNS and tunnel-based external access, and Tailscale for VPN connectivity. This design separates traffic into two distinct paths -- external (proxied through Cloudflare) and internal (LAN/VPN only).</p>"},{"location":"networking/#network-topology","title":"Network Topology","text":"<pre><code>flowchart TB\n    Internet((Internet))\n    CF[Cloudflare Edge&lt;br/&gt;*.example.com]\n    TSCloud[Tailscale&lt;br/&gt;Coordination Server]\n\n    subgraph Cluster[\"Cluster\"]\n        direction TB\n\n        subgraph Tunnel[\"Cloudflare Tunnel\"]\n            CFD[cloudflared&lt;br/&gt;QUIC + Post-Quantum&lt;br/&gt;2 replicas]\n        end\n\n        subgraph Nginx[\"Reverse Proxy\"]\n            NE[nginx-external&lt;br/&gt;192.168.0.231]\n        end\n\n        subgraph Gateways[\"Envoy Gateways\"]\n            EE[envoy-external&lt;br/&gt;192.168.0.239]\n            EI[envoy-internal&lt;br/&gt;192.168.0.238]\n        end\n\n        subgraph Network[\"Cilium CNI\"]\n            L2[L2 Announcements]\n            LBIPAM[LBIPAM Pool&lt;br/&gt;192.168.0.220-239]\n        end\n\n        TS[Tailscale Operator&lt;br/&gt;Subnet Router + Exit Node]\n\n        Apps[Applications]\n    end\n\n    User((User))\n\n    %% External path\n    Internet --&gt;|\"HTTPS proxied\"| CF\n    CF --&gt;|\"Tunnel (QUIC)\"| CFD\n    CFD --&gt;|\"HTTPS\"| NE\n    NE --&gt; EE\n    EE --&gt; Apps\n\n    %% VPN path\n    User --&gt;|\"WireGuard\"| TSCloud\n    TSCloud --&gt; TS\n    TS --&gt; EI\n    EI --&gt; Apps\n\n    %% Internal LAN\n    User --&gt;|\"LAN\"| EI\n\n    %% Cilium provides IPs\n    L2 -.-&gt;|\"ARP\"| EE &amp; EI\n    LBIPAM -.-&gt;|\"Assigns IPs\"| L2\n\n    classDef gateway fill:#7c3aed,stroke:#5b21b6,color:#fff\n    classDef tunnel fill:#f59e0b,stroke:#d97706,color:#000\n    classDef cilium fill:#00b894,stroke:#00a381,color:#fff\n    class EE,EI gateway\n    class CFD,NE tunnel\n    class L2,LBIPAM cilium</code></pre>"},{"location":"networking/#traffic-paths","title":"Traffic Paths","text":""},{"location":"networking/#external-cloudflare-proxied","title":"External (Cloudflare-Proxied)","text":"<p>Most public-facing services use this path. Traffic flows through Cloudflare's CDN and WAF, through a QUIC tunnel to the cluster, then via nginx to the <code>envoy-external</code> gateway.</p> <pre><code>User --&gt; Cloudflare CDN --&gt; cloudflared tunnel --&gt; nginx-external --&gt; envoy-external --&gt; App\n</code></pre>"},{"location":"networking/#internal-lan-vpn","title":"Internal (LAN / VPN)","text":"<p>Internal-only services are accessible via the LAN or through Tailscale VPN. The <code>envoy-internal</code> gateway has no external-dns label, so no public DNS records are created.</p> <pre><code>User --&gt; LAN/Tailscale --&gt; envoy-internal --&gt; App\n</code></pre>"},{"location":"networking/#component-overview","title":"Component Overview","text":"Component Purpose Key Detail Cilium CNI Container networking, L2 announcements, load balancing DSR mode, Maglev, kube-proxy replacement Envoy Gateway Two-gateway ingress architecture HTTP/3, TLS 1.2+, Brotli + Gzip compression DNS Management DNS resolution and the port 53 interception problem Talos hostDNS, DoH sidecar workaround External DNS Automated Cloudflare DNS record management Gateway label filter pattern Cloudflare Tunnel Secure external access without port forwarding QUIC transport, post-quantum encryption Tailscale VPN access to internal services Subnet router, exit node, API server proxy Load Balancers IP allocation and Cilium LBIPAM 20-IP pool: 192.168.0.220-239"},{"location":"networking/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Cilium over MetalLB: Cilium's L2 announcement mode replaces MetalLB entirely, reducing the number of components while gaining eBPF-powered networking, Hubble observability, and native kube-proxy replacement.</li> <li>Two gateways over one: Separating external and internal traffic allows different security policies, DNS behavior, and protocol support per path.</li> <li>Cloudflare tunnel over port forwarding: No inbound firewall rules needed. Cloudflare handles DDoS protection, CDN caching, and WAF.</li> <li>Envoy Gateway over nginx-only: Envoy Gateway provides native Gateway API support with HTTP/3, automatic compression, and fine-grained traffic policies. Nginx is retained only as the Cloudflare tunnel termination point.</li> <li>Tailscale over traditional VPN: Zero-config mesh networking with SSO integration, no VPN server to maintain, and built-in NAT traversal.</li> </ul>"},{"location":"networking/cilium-cni/","title":"Cilium CNI","text":"<p>Cilium is the Container Network Interface (CNI) for the cluster, replacing both kube-proxy and MetalLB. It uses eBPF for high-performance packet processing and provides L2 announcements for LoadBalancer IP allocation, Direct Server Return (DSR) for optimized traffic flow, and Maglev consistent hashing for load balancing.</p>"},{"location":"networking/cilium-cni/#why-cilium","title":"Why Cilium","text":"<p>Traditional Kubernetes networking stacks involve multiple components: a CNI plugin (Flannel, Calico), kube-proxy for service routing, and MetalLB for LoadBalancer IPs. Cilium consolidates all of these into a single eBPF-powered component:</p> Concern Traditional Stack Cilium CNI Flannel / Calico Cilium (eBPF) Service proxy kube-proxy (iptables) Cilium (eBPF, DSR) LoadBalancer IPs MetalLB (L2/BGP) Cilium L2 Announcements + LBIPAM Network observability Third-party tools Hubble (built-in) Network policy CNI-specific Cilium NetworkPolicy + CiliumNetworkPolicy"},{"location":"networking/cilium-cni/#helm-values","title":"Helm Values","text":"<p>The full Cilium Helm values used in this cluster:</p> pitower/kubernetes/apps/kube-system/cilium/operator/values.yaml<pre><code>hubble:\n  enabled: true\n  metrics:\n    enabled:\n      - dns:query;ignoreAAAA\n      - drop\n      - tcp\n      - flow\n      - port-distribution\n      - icmp\n      - http\n  relay:\n    enabled: true\n    rollOutPods: true\n    prometheus:\n      serviceMonitor:\n        enabled: true\n  ui:\n    enabled: true\n    rollOutPods: true\n    ingress:\n      enabled: false\n  serviceMonitor:\n    enabled: true\n  dashboards:\n    enabled: true\n    annotations:\n      grafana_folder: Cilium\n\noperator:\n  prometheus:\n    enabled: true\n    serviceMonitor:\n      enabled: true\n  dashboards:\n    enabled: true\n    annotations:\n      grafana_folder: Cilium\n  replicas: 2\n  rollOutPods: true\n\ncluster:\n  name: home-ops\n  id: 1\n\ncgroup:\n  autoMount:\n    enabled: false\n  hostRoot: /sys/fs/cgroup\n\nautoDirectNodeRoutes: true\n\nbpf:\n  masquerade: true\n\nendpointRoutes:\n  enabled: true\n\nipam:\n  mode: kubernetes\nipv4NativeRoutingCIDR: 10.244.0.0/16\nk8sServiceHost: 127.0.0.1\nk8sServicePort: 7445\nkubeProxyReplacement: true\nkubeProxyReplacementHealthzBindAddr: 0.0.0.0:10256\n\nl2announcements:\n  enabled: true\n\nloadBalancer:\n  algorithm: maglev\n  mode: dsr\n\nlocalRedirectPolicy: true\n\nroutingMode: native\n\nrollOutCiliumPods: true\n</code></pre>"},{"location":"networking/cilium-cni/#kube-proxy-replacement","title":"Kube-Proxy Replacement","text":"<p>Cilium fully replaces kube-proxy in this cluster. The key settings:</p> <pre><code>kubeProxyReplacement: true\nkubeProxyReplacementHealthzBindAddr: 0.0.0.0:10256\nk8sServiceHost: 127.0.0.1\nk8sServicePort: 7445\n</code></pre> <p>Talos Linux Integration</p> <p>On Talos Linux, kube-proxy is disabled at bootstrap time. The <code>k8sServiceHost: 127.0.0.1</code> and <code>k8sServicePort: 7445</code> point to Talos's built-in API server proxy, allowing Cilium to operate without kube-proxy from the very first boot.</p> <p>With kube-proxy replacement, all ClusterIP, NodePort, and LoadBalancer service handling is done in eBPF -- no iptables rules are created for service routing.</p>"},{"location":"networking/cilium-cni/#l2-announcements","title":"L2 Announcements","text":"<p>Instead of MetalLB, Cilium's L2 announcement mode responds to ARP requests for LoadBalancer service IPs. This is configured with two resources:</p>"},{"location":"networking/cilium-cni/#ciliuml2announcementpolicy","title":"CiliumL2AnnouncementPolicy","text":"<p>Tells Cilium to respond to ARP requests for LoadBalancer IPs on all Linux nodes:</p> pitower/kubernetes/apps/kube-system/cilium/config/cilium-l2.yaml<pre><code>apiVersion: cilium.io/v2alpha1\nkind: CiliumL2AnnouncementPolicy\nmetadata:\n  name: policy\nspec:\n  loadBalancerIPs: true\n  nodeSelector:\n    matchLabels:\n      kubernetes.io/os: linux\n</code></pre>"},{"location":"networking/cilium-cni/#ciliumloadbalancerippool-lbipam","title":"CiliumLoadBalancerIPPool (LBIPAM)","text":"<p>Defines the pool of IPs that Cilium can assign to LoadBalancer services:</p> pitower/kubernetes/apps/kube-system/cilium/config/cilium-l2.yaml<pre><code>apiVersion: cilium.io/v2alpha1\nkind: CiliumLoadBalancerIPPool\nmetadata:\n  name: pool\nspec:\n  allowFirstLastIPs: \"Yes\"\n  blocks:\n    - start: 192.168.0.220\n      stop: 192.168.0.239\n</code></pre> <p>IP Allocation</p> <p>The pool covers <code>192.168.0.220</code> through <code>192.168.0.239</code> -- a 20-IP range. Services can request a specific IP from this pool using the <code>lbipam.cilium.io/ips</code> annotation on their Service resource. See the Load Balancers page for the full allocation table.</p>"},{"location":"networking/cilium-cni/#dsr-mode-direct-server-return","title":"DSR Mode (Direct Server Return)","text":"<p>DSR mode allows response traffic to bypass the load balancer node and go directly from the backend pod to the client. This reduces latency and cuts the load balancer's bandwidth usage in half.</p> <pre><code>loadBalancer:\n  mode: dsr\n</code></pre> <pre><code>sequenceDiagram\n    participant Client\n    participant LB as Load Balancer Node\n    participant Pod as Backend Pod\n\n    Note over Client,Pod: Standard Mode (SNAT)\n    Client-&gt;&gt;LB: Request\n    LB-&gt;&gt;Pod: Forward (SNAT)\n    Pod-&gt;&gt;LB: Response\n    LB-&gt;&gt;Client: Forward response\n\n    Note over Client,Pod: DSR Mode\n    Client-&gt;&gt;LB: Request\n    LB-&gt;&gt;Pod: Forward\n    Pod-&gt;&gt;Client: Response (direct)</code></pre> <p>DSR and Source IP</p> <p>DSR mode preserves the client's source IP address at the backend pod. However, the <code>externalTrafficPolicy</code> on the gateway services is set to <code>Cluster</code> (not <code>Local</code>), which means any node can handle the traffic -- the eBPF program handles the DSR encapsulation.</p>"},{"location":"networking/cilium-cni/#maglev-load-balancing","title":"Maglev Load Balancing","text":"<p>Maglev is Google's consistent hashing algorithm for load balancing. It provides better distribution than standard hashing and maintains connection affinity even when backends change.</p> <pre><code>loadBalancer:\n  algorithm: maglev\n</code></pre> <p>Benefits over the default random algorithm:</p> <ul> <li>Consistent hashing: The same client IP maps to the same backend, providing session affinity without cookies</li> <li>Minimal disruption: Adding or removing backends only remaps a small fraction of connections</li> <li>Even distribution: Maglev's permutation-based hashing produces very uniform load distribution</li> </ul>"},{"location":"networking/cilium-cni/#native-routing","title":"Native Routing","text":"<p>The cluster uses native routing mode (instead of encapsulation/overlay):</p> <pre><code>routingMode: native\nipv4NativeRoutingCIDR: 10.244.0.0/16\nautoDirectNodeRoutes: true\nendpointRoutes: true\n</code></pre> <p>This means pod-to-pod traffic is routed directly at the kernel level without VXLAN or Geneve encapsulation, reducing overhead and improving performance. The <code>autoDirectNodeRoutes</code> setting automatically inserts routes to other nodes' pod CIDRs, and <code>endpointRoutes</code> creates per-endpoint routes for more precise routing.</p>"},{"location":"networking/cilium-cni/#hubble-observability","title":"Hubble Observability","text":"<p>Hubble is Cilium's built-in observability platform, providing deep visibility into network flows, DNS queries, and HTTP requests.</p>"},{"location":"networking/cilium-cni/#components","title":"Components","text":"Component Purpose Hubble Agent Runs on every node, captures eBPF events Hubble Relay Aggregates flows from all agents Hubble UI Web dashboard for flow visualization"},{"location":"networking/cilium-cni/#enabled-metrics","title":"Enabled Metrics","text":"<pre><code>hubble:\n  metrics:\n    enabled:\n      - dns:query;ignoreAAAA\n      - drop\n      - tcp\n      - flow\n      - port-distribution\n      - icmp\n      - http\n</code></pre> <p>The <code>dns:query;ignoreAAAA</code> filter captures DNS query metrics while ignoring AAAA (IPv6) lookups, which reduces noise in an IPv4-only cluster.</p>"},{"location":"networking/cilium-cni/#accessing-hubble-ui","title":"Accessing Hubble UI","text":"<p>Hubble UI is exposed on the <code>envoy-internal</code> gateway:</p> pitower/kubernetes/apps/kube-system/cilium/operator/httproute-hubble.yaml<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: hubble-ui\n  namespace: kube-system\nspec:\n  hostnames:\n    - hubble.example.com\n  parentRefs:\n    - name: envoy-internal\n      namespace: networking\n      sectionName: https\n  rules:\n    - backendRefs:\n        - name: hubble-ui\n          port: 80\n</code></pre> <p>Access it at <code>https://hubble.example.com</code> from the LAN or via Tailscale.</p>"},{"location":"networking/cilium-cni/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Hubble, Cilium agent, and Cilium operator all export Prometheus metrics with Grafana dashboards auto-provisioned into the <code>Cilium</code> folder:</p> <pre><code>dashboards:\n  enabled: true\n  annotations:\n    grafana_folder: Cilium\n</code></pre>"},{"location":"networking/cilium-cni/#bpf-masquerade","title":"BPF Masquerade","text":"<pre><code>bpf:\n  masquerade: true\n</code></pre> <p>BPF masquerade performs SNAT in eBPF instead of iptables. This is required for native routing mode to work correctly -- outbound traffic from pods to external destinations is masqueraded to the node's IP address using eBPF programs rather than iptables MASQUERADE rules.</p>"},{"location":"networking/cilium-cni/#troubleshooting","title":"Troubleshooting","text":""},{"location":"networking/cilium-cni/#check-cilium-status","title":"Check Cilium Status","text":"<pre><code>kubectl -n kube-system exec ds/cilium -- cilium status --brief\n</code></pre>"},{"location":"networking/cilium-cni/#verify-l2-announcements","title":"Verify L2 Announcements","text":"<pre><code># Check if the L2 announcement policy is active\nkubectl get ciliuml2announcementpolicies\n\n# Check IP pool allocation\nkubectl get ciliumloadbalancerippools\nkubectl get services -A -o wide | grep LoadBalancer\n</code></pre>"},{"location":"networking/cilium-cni/#verify-kube-proxy-replacement","title":"Verify kube-proxy Replacement","text":"<pre><code># Should show kube-proxy replacement is active\nkubectl -n kube-system exec ds/cilium -- cilium status | grep KubeProxyReplacement\n\n# Verify no iptables rules for services\nkubectl -n kube-system exec ds/cilium -- cilium service list\n</code></pre>"},{"location":"networking/cilium-cni/#hubble-cli","title":"Hubble CLI","text":"<pre><code># Observe live flows\nkubectl -n kube-system exec ds/cilium -- hubble observe --follow\n\n# Filter by namespace\nkubectl -n kube-system exec ds/cilium -- hubble observe --namespace networking\n\n# DNS queries\nkubectl -n kube-system exec ds/cilium -- hubble observe --protocol dns\n</code></pre>"},{"location":"networking/cloudflare-tunnel/","title":"Cloudflare Tunnel","text":"<p>The cluster uses a Cloudflare Tunnel (via <code>cloudflared</code>) to securely expose services to the internet without opening any inbound ports on the router. Traffic flows from Cloudflare's edge network through an encrypted QUIC tunnel to the cluster, then through nginx to the <code>envoy-external</code> gateway.</p>"},{"location":"networking/cloudflare-tunnel/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Internet\n        User((User))\n        CFEdge[Cloudflare Edge&lt;br/&gt;CDN + WAF + DDoS]\n    end\n\n    subgraph Cluster[\"Cluster\"]\n        subgraph CFD[\"cloudflared (2 replicas)\"]\n            CFD1[cloudflared-1]\n            CFD2[cloudflared-2]\n        end\n\n        NX[nginx-external&lt;br/&gt;192.168.0.231]\n        EE[envoy-external&lt;br/&gt;192.168.0.239]\n        Apps[Applications]\n    end\n\n    User --&gt;|\"HTTPS&lt;br/&gt;*.example.com\"| CFEdge\n    CFEdge --&gt;|\"QUIC Tunnel&lt;br/&gt;(post-quantum)\"| CFD1\n    CFEdge --&gt;|\"QUIC Tunnel&lt;br/&gt;(post-quantum)\"| CFD2\n    CFD1 --&gt; NX\n    CFD2 --&gt; NX\n    NX --&gt;|\"HTTPS&lt;br/&gt;SNI: external.example.com\"| EE\n    EE --&gt; Apps\n\n    classDef cf fill:#f59e0b,stroke:#d97706,color:#000\n    classDef gw fill:#7c3aed,stroke:#5b21b6,color:#fff\n    class CFEdge,CFD1,CFD2 cf\n    class EE gw</code></pre>"},{"location":"networking/cloudflare-tunnel/#traffic-flow","title":"Traffic Flow","text":"<ol> <li>User requests <code>https://myapp.example.com</code></li> <li>Cloudflare DNS resolves <code>myapp.example.com</code> to a Cloudflare edge IP (proxied/orange cloud)</li> <li>Cloudflare Edge terminates TLS, applies WAF rules, caches if applicable</li> <li>Cloudflare Tunnel sends the request over QUIC to <code>cloudflared</code> in the cluster</li> <li>cloudflared forwards to <code>nginx-external</code> (the tunnel ingress config target)</li> <li>nginx-external terminates TLS (using <code>originServerName: external.example.com</code>) and forwards to <code>envoy-external</code></li> <li>envoy-external routes to the application based on the HTTPRoute hostname match</li> </ol> <p>Why nginx Between Tunnel and Envoy?</p> <p>The Cloudflare tunnel needs a single origin server to connect to. nginx-external serves as this single endpoint, handling TLS termination and routing all tunnel traffic to the envoy-external gateway. This avoids having to configure individual tunnel ingress rules per application.</p>"},{"location":"networking/cloudflare-tunnel/#deployment-configuration","title":"Deployment Configuration","text":"<p>cloudflared runs as 2 replicas with topology spread constraints for high availability:</p> pitower/kubernetes/apps/networking/cloudflared/values.yaml<pre><code>controllers:\n  cloudflared:\n    replicas: 2\n    strategy: RollingUpdate\n\n    annotations:\n      reloader.stakater.com/auto: \"true\"\n\n    pod:\n      topologySpreadConstraints:\n        - maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: DoNotSchedule\n          labelSelector:\n            matchLabels:\n              app.kubernetes.io/name: cloudflared\n\n    containers:\n      app:\n        image:\n          repository: docker.io/cloudflare/cloudflared\n          tag: 2026.1.2\n        args:\n          - tunnel\n          - --config\n          - /etc/cloudflared/config/config.yaml\n          - run\n          - \"$(TUNNEL_ID)\"\n        env:\n          NO_AUTOUPDATE: \"true\"\n          TUNNEL_CRED_FILE: /etc/cloudflared/creds/credentials.json\n          TUNNEL_METRICS: 0.0.0.0:8080\n          TUNNEL_ORIGIN_ENABLE_HTTP2: true\n          TUNNEL_POST_QUANTUM: true\n          TUNNEL_TRANSPORT_PROTOCOL: quic\n          TUNNEL_ID:\n            valueFrom:\n              secretKeyRef:\n                name: cloudflared-secret\n                key: TUNNEL_ID\n        probes:\n          liveness:\n            enabled: true\n            custom: true\n            spec:\n              httpGet:\n                path: /ready\n                port: 8080\n          readiness:\n            enabled: true\n            custom: true\n            spec:\n              httpGet:\n                path: /ready\n                port: 8080\n          startup:\n            enabled: true\n            custom: true\n            spec:\n              httpGet:\n                path: /ready\n                port: 8080\n              failureThreshold: 30\n              periodSeconds: 10\n        resources:\n          requests:\n            cpu: 6m\n            memory: 105Mi\n          limits:\n            memory: 105Mi\n</code></pre>"},{"location":"networking/cloudflare-tunnel/#key-environment-variables","title":"Key Environment Variables","text":"Variable Value Purpose <code>TUNNEL_TRANSPORT_PROTOCOL</code> <code>quic</code> Use QUIC instead of HTTP/2 for the tunnel connection <code>TUNNEL_POST_QUANTUM</code> <code>true</code> Enable post-quantum cryptography for tunnel encryption <code>TUNNEL_ORIGIN_ENABLE_HTTP2</code> <code>true</code> Use HTTP/2 when connecting to the origin (nginx) <code>TUNNEL_METRICS</code> <code>0.0.0.0:8080</code> Expose Prometheus metrics on port 8080 <code>NO_AUTOUPDATE</code> <code>true</code> Disable auto-update (managed by Renovate instead) <p>Post-Quantum Encryption</p> <p>The <code>TUNNEL_POST_QUANTUM: true</code> setting enables post-quantum key exchange (using ML-KEM/Kyber) for the tunnel connection. This protects against \"harvest now, decrypt later\" attacks by quantum computers. The QUIC transport protocol is required for post-quantum support.</p>"},{"location":"networking/cloudflare-tunnel/#topology-spread","title":"Topology Spread","text":"<p>The topology spread constraint ensures the two cloudflared replicas run on different physical nodes:</p> <pre><code>topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: DoNotSchedule\n</code></pre> <p>This provides resilience against single-node failures -- if one node goes down, the other replica continues serving tunnel traffic.</p>"},{"location":"networking/cloudflare-tunnel/#tunnel-configuration","title":"Tunnel Configuration","text":"<p>The tunnel ingress rules define how cloudflared routes incoming requests:</p> pitower/kubernetes/apps/networking/cloudflared/configs/config.yaml<pre><code>originRequest:\n  http2Origin: true\n\ningress:\n  - hostname: \"example.com\"\n    service: https://nginx-external-controller.networking.svc.cluster.local:443\n    originRequest:\n      originServerName: \"external.example.com\"\n  - hostname: \"*.example.com\"\n    service: https://nginx-external-controller.networking.svc.cluster.local:443\n    originRequest:\n      originServerName: \"external.example.com\"\n  - service: http_status:404\n</code></pre>"},{"location":"networking/cloudflare-tunnel/#ingress-rules-explained","title":"Ingress Rules Explained","text":"Rule Hostname Target Purpose 1 <code>example.com</code> nginx-external:443 Bare domain traffic 2 <code>*.example.com</code> nginx-external:443 All subdomain traffic 3 (catch-all) <code>http_status:404</code> Return 404 for unmatched requests <p>The <code>originServerName: \"external.example.com\"</code> setting tells cloudflared to set the TLS SNI (Server Name Indication) to <code>external.example.com</code> when connecting to nginx. This allows nginx to match the request to the correct server block.</p> <p>All Traffic Through One nginx</p> <p>Rather than configuring individual tunnel ingress rules per application, all traffic goes to a single nginx-external instance. This means adding a new application only requires creating an HTTPRoute on the <code>envoy-external</code> gateway -- no tunnel config changes needed.</p>"},{"location":"networking/cloudflare-tunnel/#dnsendpoint-for-tunnel-cname","title":"DNSEndpoint for Tunnel CNAME","text":"<p>For the tunnel to work, Cloudflare DNS must have a CNAME record pointing <code>external.example.com</code> to the tunnel's hostname:</p> pitower/kubernetes/apps/networking/cloudflared/dnsendpoint.yaml<pre><code>apiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  name: cloudflared\n  namespace: networking\nspec:\n  endpoints:\n    - dnsName: \"external.example.com\"\n      recordType: CNAME\n      targets: [\"7ee9277a-e2f3-45ae-a0ac-4e85d39fc334.cfargotunnel.com\"]\n</code></pre> <p>This creates: <code>external.example.com</code> -&gt; <code>7ee9277a-e2f3-45ae-a0ac-4e85d39fc334.cfargotunnel.com</code></p> <p>When Cloudflare receives a request for <code>myapp.example.com</code>, it resolves the CNAME chain:</p> <ol> <li><code>myapp.example.com</code> -&gt; <code>external.example.com</code> (CNAME from external-dns)</li> <li><code>external.example.com</code> -&gt; <code>&lt;tunnel-id&gt;.cfargotunnel.com</code> (CNAME from DNSEndpoint)</li> <li>Cloudflare recognizes <code>.cfargotunnel.com</code> and routes through the tunnel</li> </ol>"},{"location":"networking/cloudflare-tunnel/#secrets","title":"Secrets","text":"<p>The tunnel requires two secrets stored in <code>cloudflared-secret</code> (managed via External Secrets from 1Password):</p> Key Purpose <code>TUNNEL_ID</code> The Cloudflare tunnel UUID <code>credentials.json</code> Tunnel credentials file (contains the tunnel secret) <p>These are mounted into the cloudflared pods:</p> <pre><code>persistence:\n  config:\n    type: configMap\n    name: cloudflared-configmap\n    globalMounts:\n      - path: /etc/cloudflared/config/config.yaml\n        subPath: config.yaml\n        readOnly: true\n  creds:\n    type: secret\n    name: cloudflared-secret\n    globalMounts:\n      - path: /etc/cloudflared/creds/credentials.json\n        subPath: credentials.json\n        readOnly: true\n</code></pre>"},{"location":"networking/cloudflare-tunnel/#monitoring","title":"Monitoring","text":"<p>cloudflared exposes Prometheus metrics on port 8080, scraped by a ServiceMonitor:</p> <pre><code>service:\n  app:\n    controller: cloudflared\n    ports:\n      http:\n        port: 8080\n\nserviceMonitor:\n  app:\n    serviceName: cloudflared\n    endpoints:\n      - port: http\n        scheme: http\n        path: /metrics\n        interval: 1m\n        scrapeTimeout: 30s\n</code></pre> <p>Key metrics to monitor:</p> Metric Description <code>cloudflared_tunnel_total_requests</code> Total requests through the tunnel <code>cloudflared_tunnel_request_errors</code> Request errors <code>cloudflared_tunnel_response_by_code</code> Response status code distribution <code>cloudflared_tunnel_concurrent_requests_per_tunnel</code> Active connections"},{"location":"networking/cloudflare-tunnel/#troubleshooting","title":"Troubleshooting","text":""},{"location":"networking/cloudflare-tunnel/#check-tunnel-status","title":"Check Tunnel Status","text":"<pre><code># Check if cloudflared pods are running\nkubectl get pods -n networking -l app.kubernetes.io/name=cloudflared\n\n# Check readiness\nkubectl get pods -n networking -l app.kubernetes.io/name=cloudflared -o wide\n\n# View logs\nkubectl logs -n networking -l app.kubernetes.io/name=cloudflared --tail=50\n</code></pre>"},{"location":"networking/cloudflare-tunnel/#verify-tunnel-connectivity","title":"Verify Tunnel Connectivity","text":"<pre><code># Check the /ready endpoint\nkubectl port-forward -n networking svc/cloudflared 8080:8080 &amp;\ncurl http://localhost:8080/ready\n\n# Check metrics\ncurl http://localhost:8080/metrics | grep cloudflared_tunnel\n</code></pre>"},{"location":"networking/cloudflare-tunnel/#test-end-to-end","title":"Test End-to-End","text":"<pre><code># From outside the network, test a proxied service\ncurl -v https://myapp.example.com\n\n# Check the cf-ray header (confirms traffic went through Cloudflare)\ncurl -sI https://myapp.example.com | grep cf-ray\n</code></pre>"},{"location":"networking/cloudflare-tunnel/#common-issues","title":"Common Issues","text":"<p>Tunnel Disconnects</p> <p>If both cloudflared replicas lose connection to Cloudflare, all proxied services become unreachable. Check:</p> <ul> <li>Node network connectivity</li> <li>Cloudflare status page</li> <li>cloudflared logs for reconnection attempts</li> </ul> <p>Origin Certificate Errors</p> <p>If cloudflared cannot connect to nginx, check:</p> <ul> <li>The <code>originServerName</code> matches the TLS certificate on nginx</li> <li>The <code>wildcard-production-tls</code> secret exists in the networking namespace</li> <li>The certificate is not expired (<code>kubectl get certificate -n networking</code>)</li> </ul>"},{"location":"networking/dns-management/","title":"DNS Management","text":"<p>DNS in the cluster involves several layers: Cloudflare as the authoritative DNS provider, Talos Linux's built-in host DNS forwarding (CoreDNS is disabled), and a critical workaround for the Ubiquiti router's DNS interception behavior. This page covers the full DNS architecture and the solutions developed to handle each challenge.</p>"},{"location":"networking/dns-management/#dns-architecture","title":"DNS Architecture","text":"<pre><code>flowchart TB\n    subgraph Internet\n        CF[Cloudflare DNS&lt;br/&gt;Authoritative for example.com]\n    end\n\n    subgraph Router[\"Ubiquiti Router\"]\n        DNAT[\"DNAT Rule&lt;br/&gt;Intercepts port 53 traffic&lt;br/&gt;Redirects to router DNS\"]\n    end\n\n    subgraph Cluster[\"Cluster\"]\n        subgraph Pods\n            App[Application Pod]\n            RRDA[RRDA Pod]\n            DNSProxy[\"dnsproxy sidecar&lt;br/&gt;DoH to 1.1.1.1\"]\n        end\n\n        subgraph DNS[\"Cluster DNS\"]\n            HostDNS[\"Talos hostDNS&lt;br/&gt;(forwarding proxy)\"]\n        end\n\n        EDNS[external-dns&lt;br/&gt;Updates Cloudflare records]\n    end\n\n    App --&gt;|\"DNS query\\n(port 53)\"| HostDNS\n    HostDNS --&gt;|\"port 53\"| DNAT\n    DNAT --&gt;|\"Intercepted!\"| DNAT\n    DNAT -.-&gt;|\"Answers from\\nrouter cache\"| HostDNS\n\n    RRDA --&gt;|\"DNS query\\n(port 53)\"| DNSProxy\n    DNSProxy --&gt;|\"DoH (port 443)\\nBypasses interception\"| CF\n\n    EDNS --&gt;|\"API calls\"| CF\n\n    classDef problem fill:#ef4444,stroke:#b91c1c,color:#fff\n    classDef solution fill:#22c55e,stroke:#16a34a,color:#fff\n    class DNAT problem\n    class DNSProxy solution</code></pre>"},{"location":"networking/dns-management/#cloudflare-as-authoritative-dns","title":"Cloudflare as Authoritative DNS","text":"<p>The domain <code>example.com</code> is managed by Cloudflare. All DNS records are created and updated automatically by external-dns, which runs in the cluster and uses the Cloudflare API.</p> <p>Key DNS records:</p> Record Type Target Proxied <code>external.example.com</code> CNAME <code>&lt;tunnel-id&gt;.cfargotunnel.com</code> No (tunnel routing) <code>*.example.com</code> (external apps) CNAME <code>external.example.com</code> Yes"},{"location":"networking/dns-management/#cluster-dns-talos-hostdns","title":"Cluster DNS: Talos hostDNS","text":"<p>The cluster does not run CoreDNS. Instead, Talos Linux provides a built-in host DNS forwarding proxy that handles cluster DNS resolution. This is configured at the Talos machine config level and forwards queries to upstream resolvers.</p> <p>Why Not CoreDNS?</p> <p>While CoreDNS is deployed (at <code>192.168.0.220</code>), it serves as a fallback. The primary cluster DNS is Talos's built-in <code>hostDNS</code> feature, which provides a lightweight forwarding DNS proxy directly on each node. This avoids the overhead of running CoreDNS pods and reduces single-point-of-failure risk.</p>"},{"location":"networking/dns-management/#the-dns-interception-problem","title":"The DNS Interception Problem","text":"<p>Critical Gotcha: Ubiquiti Router Intercepts Port 53</p> <p>The Ubiquiti router performs transparent DNS interception (DNAT) on all outbound traffic to port 53. This means any DNS query from the cluster that exits the node toward an external DNS server (like <code>1.1.1.1</code> or <code>8.8.8.8</code>) gets intercepted and answered by the router's own DNS resolver instead.</p> <p>This has several consequences:</p> <ol> <li>Stale records: The router's DNS cache may serve outdated records for <code>example.com</code> subdomains</li> <li>Incorrect answers: The router resolves against its own upstream, which may not reflect recent Cloudflare changes</li> <li>Verification impossible: You cannot verify Cloudflare DNS records from within the network using standard <code>dig</code> commands</li> </ol>"},{"location":"networking/dns-management/#how-to-verify-actual-cloudflare-records","title":"How to Verify Actual Cloudflare Records","text":"<p>Since standard DNS queries on port 53 get intercepted, you must use DNS-over-HTTPS (DoH) to verify what Cloudflare actually returns:</p> <pre><code># Verify a record via Cloudflare DoH (bypasses router interception)\ncurl -s \"https://1.1.1.1/dns-query?name=app.example.com&amp;type=A\" \\\n  -H \"Accept: application/dns-json\" | jq .\n\n# Compare with what the router returns (intercepted)\ndig app.example.com @1.1.1.1 +short\n\n# These may return DIFFERENT results!\n</code></pre> <p>Quick DoH Check</p> <pre><code># One-liner to check if a CNAME exists on Cloudflare\ncurl -s \"https://1.1.1.1/dns-query?name=myapp.example.com&amp;type=CNAME\" \\\n  -H \"Accept: application/dns-json\" | jq '.Answer[].data'\n</code></pre>"},{"location":"networking/dns-management/#the-doh-sidecar-solution","title":"The DoH Sidecar Solution","text":"<p>For applications that must resolve DNS records accurately against Cloudflare (not the router's intercepted version), a DNS-over-HTTPS sidecar is deployed alongside the application.</p>"},{"location":"networking/dns-management/#rrda-dnsproxy","title":"RRDA + dnsproxy","text":"<p>The RRDA application is a REST API for DNS lookups. It needs to query actual authoritative DNS servers to return correct results. Since standard port 53 queries get intercepted by the Ubiquiti router, RRDA runs a <code>dnsproxy</code> sidecar that provides a local DNS server (on port 53 within the pod) that forwards all queries via DoH.</p> pitower/kubernetes/apps/selfhosted/rrda/values.yaml<pre><code>controllers:\n  rrda:\n    strategy: RollingUpdate\n    containers:\n      app:\n        image:\n          repository: ghcr.io/swibrow/rrda\n          tag: 1.4.1\n        probes:\n          liveness: &amp;probes\n            enabled: true\n            custom: true\n            spec:\n              httpGet:\n                path: /127.0.0.1:53/example.com/A\n                port: &amp;port 8080\n              initialDelaySeconds: 5\n              periodSeconds: 30\n              timeoutSeconds: 5\n          readiness: *probes\n      dns-over-https:\n        image:\n          repository: docker.io/adguard/dnsproxy\n          tag: v0.78.2\n        args:\n          - --listen=0.0.0.0\n          - --port=53\n          - --upstream=https://1.1.1.1/dns-query\n          - --upstream=https://1.0.0.1/dns-query\n          - --bootstrap=9.9.9.9:53\n</code></pre>"},{"location":"networking/dns-management/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    subgraph Pod[\"RRDA Pod\"]\n        RRDA[rrda app&lt;br/&gt;port 8080]\n        DNSProxy[\"dnsproxy sidecar&lt;br/&gt;port 53\"]\n    end\n\n    Client[Client] --&gt;|\"HTTP GET /1.1.1.1:53/example.com/A\"| RRDA\n    RRDA --&gt;|\"DNS query to&lt;br/&gt;127.0.0.1:53\"| DNSProxy\n    DNSProxy --&gt;|\"DoH (HTTPS/443)&lt;br/&gt;to 1.1.1.1\"| Cloudflare[Cloudflare DNS]\n    Cloudflare --&gt;|\"Response\"| DNSProxy\n    DNSProxy --&gt;|\"DNS response\"| RRDA\n    RRDA --&gt;|\"JSON response\"| Client</code></pre> <ol> <li>RRDA receives an HTTP request asking for a DNS record</li> <li>RRDA performs a standard DNS query to <code>127.0.0.1:53</code> (the pod's localhost)</li> <li>The <code>dnsproxy</code> sidecar receives the query on port 53</li> <li><code>dnsproxy</code> forwards the query via DNS-over-HTTPS to <code>https://1.1.1.1/dns-query</code> (port 443)</li> <li>Since the query goes over HTTPS (port 443), the Ubiquiti router does not intercept it</li> <li>The actual Cloudflare response comes back through the DoH tunnel</li> </ol>"},{"location":"networking/dns-management/#dnsproxy-configuration","title":"dnsproxy Configuration","text":"Flag Purpose <code>--listen=0.0.0.0</code> Listen on all interfaces within the pod <code>--port=53</code> Standard DNS port <code>--upstream=https://1.1.1.1/dns-query</code> Primary DoH upstream (Cloudflare) <code>--upstream=https://1.0.0.1/dns-query</code> Secondary DoH upstream (Cloudflare backup) <code>--bootstrap=9.9.9.9:53</code> Bootstrap DNS to resolve the DoH upstream hostnames <p>Bootstrap DNS</p> <p>The <code>--bootstrap</code> flag is needed because <code>dnsproxy</code> needs to resolve <code>1.1.1.1</code> and <code>1.0.0.1</code> before it can use DoH. Since these are IP addresses (not hostnames), the bootstrap is technically only needed if DoH upstreams were specified as hostnames. It is included as a safety net using Quad9 (<code>9.9.9.9</code>).</p>"},{"location":"networking/dns-management/#why-not-use-doh-cluster-wide","title":"Why Not Use DoH Cluster-Wide?","text":"<p>Running DoH for the entire cluster would bypass the router's DNS entirely, which is actually desirable in some cases. However:</p> <ol> <li>Performance: DoH adds latency compared to plain DNS for every query</li> <li>Complexity: Would require changing Talos's hostDNS configuration on all nodes</li> <li>Scope: Only RRDA actually needs authoritative Cloudflare results -- most apps just need standard resolution</li> <li>Router features: The Ubiquiti router provides useful features like local DNS entries and DHCP-based hostname resolution that would be lost</li> </ol> <p>The sidecar approach solves the problem surgically for the apps that need it.</p>"},{"location":"networking/dns-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"networking/dns-management/#check-what-the-router-returns-vs-cloudflare","title":"Check What the Router Returns vs. Cloudflare","text":"<pre><code># What the router returns (after interception)\ndig +short myapp.example.com\n\n# What Cloudflare actually has\ncurl -s \"https://1.1.1.1/dns-query?name=myapp.example.com&amp;type=CNAME\" \\\n  -H \"Accept: application/dns-json\" | jq .\n\n# If these differ, the router's DNS cache is stale\n</code></pre>"},{"location":"networking/dns-management/#flush-router-dns-cache","title":"Flush Router DNS Cache","text":"<p>If the Ubiquiti router is serving stale DNS records, you may need to flush its cache. Refer to your Ubiquiti documentation for the specific command, as it varies by firmware version.</p>"},{"location":"networking/dns-management/#check-rrdas-dns-resolution","title":"Check RRDA's DNS Resolution","text":"<pre><code># Test RRDA's DNS resolution (uses the DoH sidecar)\ncurl -s https://rrda.example.com/1.1.1.1:53/example.com/A | jq .\n\n# Test a example.com domain through RRDA\ncurl -s https://rrda.example.com/1.1.1.1:53/myapp.example.com/CNAME | jq .\n</code></pre>"},{"location":"networking/dns-management/#verify-dnsproxy-sidecar-is-running","title":"Verify dnsproxy Sidecar Is Running","text":"<pre><code># Check pod containers\nkubectl get pods -n selfhosted -l app.kubernetes.io/name=rrda -o jsonpath='{.items[*].spec.containers[*].name}'\n# Should output: app dns-over-https\n\n# Check dnsproxy logs\nkubectl logs -n selfhosted -l app.kubernetes.io/name=rrda -c dns-over-https\n</code></pre>"},{"location":"networking/dns-management/#test-doh-directly","title":"Test DoH Directly","text":"<pre><code># Test DNS-over-HTTPS from your workstation\ncurl -s \"https://1.1.1.1/dns-query?name=example.com&amp;type=A\" \\\n  -H \"Accept: application/dns-json\"\n\n# Test with a specific record type\ncurl -s \"https://1.1.1.1/dns-query?name=example.com&amp;type=TXT\" \\\n  -H \"Accept: application/dns-json\" | jq '.Answer'\n</code></pre>"},{"location":"networking/envoy-gateway/","title":"Envoy Gateway","text":"<p>The cluster uses Envoy Gateway as its primary ingress controller, implementing the Kubernetes Gateway API. The architecture deploys two separate Gateway resources -- <code>envoy-external</code> and <code>envoy-internal</code> -- each serving a distinct traffic path with its own IP address, DNS target, and access policy.</p>"},{"location":"networking/envoy-gateway/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart LR\n    subgraph External Traffic\n        CF[Cloudflare Tunnel] --&gt; NX[nginx-external&lt;br/&gt;192.168.0.231]\n        NX --&gt; EE\n    end\n\n    subgraph Internal Traffic\n        LAN[LAN / Tailscale] --&gt; EI\n    end\n\n    subgraph Gateways\n        EE[envoy-external&lt;br/&gt;192.168.0.239&lt;br/&gt;external.example.com]\n        EI[envoy-internal&lt;br/&gt;192.168.0.238&lt;br/&gt;internal.example.com]\n    end\n\n    EE --&gt; Apps[Applications]\n    EI --&gt; Apps\n\n    classDef gw fill:#7c3aed,stroke:#5b21b6,color:#fff\n    class EE,EI gw</code></pre>"},{"location":"networking/envoy-gateway/#gateway-comparison","title":"Gateway Comparison","text":"Property envoy-external envoy-internal IP Address 192.168.0.239 192.168.0.238 DNS Target external.example.com internal.example.com Traffic Source Cloudflare tunnel (via nginx) LAN / Tailscale VPN Cloudflare Proxied Yes (via tunnel) N/A external-dns Label <code>enabled: \"true\"</code> Not set DNS Records Created Yes No Listeners HTTP (80), HTTPS (443) HTTP (80), HTTPS (443) Allowed Namespaces (HTTPS) All All Allowed Namespaces (HTTP) Same (redirect only) Same (redirect only) Use Case Public web apps Admin dashboards, internal tools"},{"location":"networking/envoy-gateway/#shared-configuration","title":"Shared Configuration","text":"<p>Both gateways share a common <code>GatewayClass</code>, <code>EnvoyProxy</code>, <code>BackendTrafficPolicy</code>, and <code>ClientTrafficPolicy</code>.</p>"},{"location":"networking/envoy-gateway/#envoyproxy","title":"EnvoyProxy","text":"<p>Defines the Envoy data plane configuration -- replicas, resources, shutdown behavior, and Prometheus telemetry:</p> pitower/kubernetes/apps/networking/envoy-gateway/envoy.yaml<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyProxy\nmetadata:\n  name: envoy\nspec:\n  logging:\n    level:\n      default: info\n  provider:\n    type: Kubernetes\n    kubernetes:\n      envoyDeployment:\n        replicas: 2\n        container:\n          image: mirror.gcr.io/envoyproxy/envoy:v1.35.3\n          resources:\n            requests:\n              cpu: 100m\n            limits:\n              memory: 1Gi\n      envoyService:\n        externalTrafficPolicy: Cluster\n  shutdown:\n    drainTimeout: 180s\n  telemetry:\n    metrics:\n      prometheus:\n        compression:\n          type: Gzip\n</code></pre> <p>Replicas and Drain Timeout</p> <p>Each gateway gets 2 Envoy replicas with a 180-second drain timeout. This ensures zero-downtime during rolling updates -- existing connections have 3 minutes to complete before the old pod is terminated.</p>"},{"location":"networking/envoy-gateway/#gatewayclass","title":"GatewayClass","text":"<p>Links the <code>envoy</code> GatewayClass to the EnvoyProxy configuration:</p> pitower/kubernetes/apps/networking/envoy-gateway/envoy.yaml<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: envoy\nspec:\n  controllerName: gateway.envoyproxy.io/gatewayclass-controller\n  parametersRef:\n    group: gateway.envoyproxy.io\n    kind: EnvoyProxy\n    name: envoy\n    namespace: networking\n</code></pre>"},{"location":"networking/envoy-gateway/#backendtrafficpolicy-compression","title":"BackendTrafficPolicy (Compression)","text":"<p>Enables Brotli and Gzip response compression across all gateways:</p> pitower/kubernetes/apps/networking/envoy-gateway/envoy.yaml<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: BackendTrafficPolicy\nmetadata:\n  name: envoy\nspec:\n  targetSelectors:\n    - group: gateway.networking.k8s.io\n      kind: Gateway\n  compression:\n    - type: Brotli\n    - type: Gzip\n</code></pre>"},{"location":"networking/envoy-gateway/#clienttrafficpolicy-http3-tls","title":"ClientTrafficPolicy (HTTP/3, TLS)","text":"<p>Configures HTTP/3 support, TLS settings, and client IP detection:</p> pitower/kubernetes/apps/networking/envoy-gateway/envoy.yaml<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: ClientTrafficPolicy\nmetadata:\n  name: envoy\nspec:\n  clientIPDetection:\n    xForwardedFor:\n      numTrustedHops: 1\n  http3: {}\n  targetSelectors:\n    - group: gateway.networking.k8s.io\n      kind: Gateway\n  tls:\n    minVersion: \"1.2\"\n    alpnProtocols:\n      - h2\n      - http/1.1\n</code></pre> <p>HTTP/3 Support</p> <p>The empty <code>http3: {}</code> block enables HTTP/3 (QUIC) on all HTTPS listeners. Clients that support it will automatically upgrade to HTTP/3 via the <code>Alt-Svc</code> header. The <code>alpnProtocols</code> list ensures h2 and http/1.1 fallback for clients that do not support HTTP/3.</p> <p>Client IP Detection</p> <p><code>numTrustedHops: 1</code> tells Envoy to trust the rightmost IP in the <code>X-Forwarded-For</code> header (from the immediate upstream proxy, such as Cloudflare via nginx).</p>"},{"location":"networking/envoy-gateway/#tls-certificate","title":"TLS Certificate","text":"<p>All gateways share a wildcard certificate from Let's Encrypt:</p> pitower/kubernetes/apps/networking/envoy-gateway/certificate.yaml<pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: \"wildcard-production\"\n  namespace: networking\nspec:\n  secretName: \"wildcard-production-tls\"\n  issuerRef:\n    name: letsencrypt-production\n    kind: ClusterIssuer\n  commonName: \"example.com\"\n  dnsNames:\n    - \"example.com\"\n    - \"*.example.com\"\n</code></pre> <p>The certificate covers both <code>example.com</code> and <code>*.example.com</code>, issued via DNS-01 challenge through Cloudflare.</p>"},{"location":"networking/envoy-gateway/#gateway-definitions","title":"Gateway Definitions","text":""},{"location":"networking/envoy-gateway/#envoy-external","title":"envoy-external","text":"<p>Receives all Cloudflare-proxied traffic. The <code>external-dns.alpha.kubernetes.io/enabled: \"true\"</code> label tells external-dns to create DNS records for routes attached to this gateway.</p> pitower/kubernetes/apps/networking/envoy-gateway/external.yaml<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-external\n  labels:\n    external-dns.alpha.kubernetes.io/enabled: \"true\"\n  annotations:\n    external-dns.alpha.kubernetes.io/target: &amp;hostname external.example.com\nspec:\n  gatewayClassName: envoy\n  infrastructure:\n    annotations:\n      external-dns.alpha.kubernetes.io/hostname: *hostname\n      lbipam.cilium.io/ips: \"192.168.0.239\"\n  listeners:\n    - name: http\n      protocol: HTTP\n      port: 80\n      allowedRoutes:\n        namespaces:\n          from: Same\n    - name: https\n      protocol: HTTPS\n      port: 443\n      allowedRoutes:\n        namespaces:\n          from: All\n      tls:\n        certificateRefs:\n          - group: ''\n            kind: Secret\n            name: wildcard-production-tls\n</code></pre> <p>HTTP Listener Scope</p> <p>The HTTP listener (port 80) uses <code>from: Same</code> -- only routes in the <code>networking</code> namespace can attach to it. This is because port 80 is only used for the HTTP-to-HTTPS redirect route, not for application traffic.</p>"},{"location":"networking/envoy-gateway/#envoy-internal","title":"envoy-internal","text":"<p>Internal-only gateway. Notice it has no <code>external-dns.alpha.kubernetes.io/enabled</code> label, so external-dns ignores it entirely.</p> pitower/kubernetes/apps/networking/envoy-gateway/internal.yaml<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-internal\n  annotations:\n    external-dns.alpha.kubernetes.io/target: &amp;hostname internal.example.com\nspec:\n  gatewayClassName: envoy\n  infrastructure:\n    annotations:\n      external-dns.alpha.kubernetes.io/hostname: *hostname\n      lbipam.cilium.io/ips: \"192.168.0.238\"\n  listeners:\n    - name: http\n      protocol: HTTP\n      port: 80\n      allowedRoutes:\n        namespaces:\n          from: Same\n    - name: https\n      protocol: HTTPS\n      port: 443\n      allowedRoutes:\n        namespaces:\n          from: All\n      tls:\n        certificateRefs:\n          - group: ''\n            kind: Secret\n            name: wildcard-production-tls\n</code></pre> <p>Accessing Internal Services</p> <p>Internal services are accessible when your DNS resolves <code>*.example.com</code> to <code>192.168.0.238</code>. This happens automatically on the LAN (via split DNS) or through Tailscale (which advertises the <code>192.168.0.0/24</code> subnet).</p>"},{"location":"networking/envoy-gateway/#http-to-https-redirect","title":"HTTP-to-HTTPS Redirect","text":"<p>Each gateway has an accompanying HTTPRoute that redirects all HTTP traffic to HTTPS with a 301 status code:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: envoy-external\n  annotations:\n    external-dns.alpha.kubernetes.io/controller: none  # (1)!\nspec:\n  parentRefs:\n    - group: gateway.networking.k8s.io\n      kind: Gateway\n      name: envoy-external\n      namespace: networking\n      sectionName: http  # (2)!\n  rules:\n    - filters:\n        - requestRedirect:\n            scheme: https\n            statusCode: 301\n          type: RequestRedirect\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n</code></pre> <ol> <li>The <code>controller: none</code> annotation prevents external-dns from creating DNS records for this redirect route.</li> <li><code>sectionName: http</code> attaches this route only to the HTTP (port 80) listener.</li> </ol>"},{"location":"networking/envoy-gateway/#how-apps-attach-to-gateways","title":"How Apps Attach to Gateways","text":"<p>Applications create HTTPRoute resources that reference a gateway via <code>parentRefs</code>. Example from the RRDA application:</p> <pre><code>route:\n  app:\n    enabled: true\n    hostnames:\n      - rrda.example.com\n    parentRefs:\n      - name: envoy-external      # (1)!\n        namespace: networking\n        sectionName: https         # (2)!\n</code></pre> <ol> <li>Choose which gateway to attach to: <code>envoy-external</code> or <code>envoy-internal</code>.</li> <li>Always attach to the <code>https</code> section for application traffic.</li> </ol> <p>Moving an App Between Gateways</p> <p>To move an app from external to internal access (or vice versa), change the <code>parentRefs</code> to reference the desired gateway.</p>"},{"location":"networking/envoy-gateway/#envoy-gateway-helm-values","title":"Envoy Gateway Helm Values","text":"<p>The Envoy Gateway controller itself is deployed with minimal configuration:</p> pitower/kubernetes/apps/networking/envoy-gateway/values.yaml<pre><code>global:\n  imageRegistry: mirror.gcr.io\n\ncertgen:\n  job:\n    args:\n      - certgen\n      - --disable-topology-injector\n\nconfig:\n  envoyGateway:\n    provider:\n      type: Kubernetes\n      kubernetes:\n        deploy:\n          type: GatewayNamespace\n</code></pre> <p>The <code>GatewayNamespace</code> deploy type means Envoy proxy pods are created in the same namespace as the Gateway resource (the <code>networking</code> namespace).</p>"},{"location":"networking/envoy-gateway/#troubleshooting","title":"Troubleshooting","text":""},{"location":"networking/envoy-gateway/#check-gateway-status","title":"Check Gateway Status","text":"<pre><code># List all gateways and their conditions\nkubectl get gateways -n networking\n\n# Detailed status of a specific gateway\nkubectl describe gateway envoy-external -n networking\n</code></pre>"},{"location":"networking/envoy-gateway/#check-envoy-proxy-pods","title":"Check Envoy Proxy Pods","text":"<pre><code># List Envoy proxy pods (one deployment per gateway)\nkubectl get pods -n networking -l gateway.envoyproxy.io/owning-gateway-name\n\n# Check logs for a specific gateway's Envoy pods\nkubectl logs -n networking -l gateway.envoyproxy.io/owning-gateway-name=envoy-external\n</code></pre>"},{"location":"networking/envoy-gateway/#verify-httproutes","title":"Verify HTTPRoutes","text":"<pre><code># List all HTTPRoutes across all namespaces\nkubectl get httproutes -A\n\n# Check if a route is accepted by its gateway\nkubectl describe httproute &lt;route-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"networking/envoy-gateway/#test-connectivity","title":"Test Connectivity","text":"<pre><code># Test external gateway (via Cloudflare)\ncurl -v https://app.example.com\n\n# Test internal gateway (from LAN)\ncurl -v --resolve app.example.com:443:192.168.0.238 https://app.example.com\n</code></pre>"},{"location":"networking/external-dns/","title":"External DNS","text":"<p>external-dns automatically creates and manages Cloudflare DNS records based on Kubernetes resources. It watches Gateway API HTTPRoutes, Ingress resources, and custom DNSEndpoint CRDs, then synchronizes the corresponding DNS records in Cloudflare.</p>"},{"location":"networking/external-dns/#configuration","title":"Configuration","text":"pitower/kubernetes/apps/networking/external-dns/values.yaml<pre><code>fullnameOverride: external-dns\nprovider: cloudflare\nenv:\n  - name: CF_API_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: external-dns-secret\n        key: api-token\nextraArgs:\n  - --ingress-class=external\n  - --cloudflare-proxied\n  - --crd-source-apiversion=externaldns.k8s.io/v1alpha1\n  - --crd-source-kind=DNSEndpoint\n  - --gateway-label-filter=external-dns.alpha.kubernetes.io/enabled=true\npolicy: sync\nsources:\n  - crd\n  - ingress\n  - gateway-httproute\ntxtPrefix: k8s.\ntxtOwnerId: default\ndomainFilters: [\"example.com\"]\nserviceMonitor:\n  enabled: true\npodAnnotations:\n  secret.reloader.stakater.com/reload: external-dns-secret\n</code></pre>"},{"location":"networking/external-dns/#configuration-breakdown","title":"Configuration Breakdown","text":"Setting Value Purpose <code>provider</code> <code>cloudflare</code> Use the Cloudflare DNS API <code>--ingress-class=external</code> Only process Ingress resources with class <code>external</code> <code>--cloudflare-proxied</code> Default to Cloudflare-proxied (orange cloud) for records <code>--crd-source-apiversion</code> <code>externaldns.k8s.io/v1alpha1</code> Enable the DNSEndpoint CRD source <code>--crd-source-kind</code> <code>DNSEndpoint</code> Watch DNSEndpoint resources <code>--gateway-label-filter</code> <code>external-dns.alpha.kubernetes.io/enabled=true</code> Only process gateways with this label <code>policy</code> <code>sync</code> Delete records that are no longer in Kubernetes (vs. <code>upsert-only</code>) <code>sources</code> <code>crd</code>, <code>ingress</code>, <code>gateway-httproute</code> Watch these three resource types <code>txtPrefix</code> <code>k8s.</code> Prefix for TXT ownership records <code>txtOwnerId</code> <code>default</code> Identifies this external-dns instance's records <code>domainFilters</code> <code>[\"example.com\"]</code> Only manage records under <code>example.com</code>"},{"location":"networking/external-dns/#the-gateway-label-filter-pattern","title":"The Gateway Label Filter Pattern","text":"<p>The most important configuration detail is <code>--gateway-label-filter</code>. This controls which gateways external-dns considers when creating DNS records from HTTPRoutes.</p>"},{"location":"networking/external-dns/#how-it-works","title":"How It Works","text":"<pre><code>flowchart TD\n    EDNS[external-dns] --&gt;|\"Checks label\"| GW{Gateway has&lt;br/&gt;enabled=true?}\n    GW --&gt;|Yes| Process[Process attached HTTPRoutes]\n    GW --&gt;|No| Skip[Skip gateway entirely]\n\n    Process --&gt; CreateDNS[Create DNS records&lt;br/&gt;in Cloudflare]\n\n    subgraph Gateways\n        EE[\"envoy-external&lt;br/&gt;label: enabled=true&lt;br/&gt;Records created\"]\n        EI[\"envoy-internal&lt;br/&gt;NO label&lt;br/&gt;No records\"]\n    end\n\n    classDef active fill:#22c55e,stroke:#16a34a,color:#fff\n    classDef inactive fill:#6b7280,stroke:#4b5563,color:#fff\n    class EE active\n    class EI inactive</code></pre> <p>Only gateways with the label <code>external-dns.alpha.kubernetes.io/enabled: \"true\"</code> are processed:</p> <ul> <li>envoy-external: Has the label -- DNS records are created for its HTTPRoutes</li> <li>envoy-internal: Does NOT have the label -- completely ignored by external-dns</li> </ul>"},{"location":"networking/external-dns/#annotation-patterns","title":"Annotation Patterns","text":"<p>Understanding which annotations go on which resource is critical and a frequent source of confusion.</p>"},{"location":"networking/external-dns/#annotations-on-gateway-resources","title":"Annotations on Gateway Resources","text":"Annotation Placement Purpose <code>external-dns.alpha.kubernetes.io/target</code> Gateway <code>.metadata.annotations</code> Sets the CNAME/A target for DNS records <code>external-dns.alpha.kubernetes.io/hostname</code> Gateway <code>.spec.infrastructure.annotations</code> Sets the hostname for the LoadBalancer Service <code>external-dns.alpha.kubernetes.io/enabled</code> Gateway <code>.metadata.labels</code> Enables external-dns processing (label, not annotation) <code>external-dns.alpha.kubernetes.io/cloudflare-proxied</code> Gateway <code>.metadata.annotations</code> Controls Cloudflare proxy status (orange/grey cloud) <code>lbipam.cilium.io/ips</code> Gateway <code>.spec.infrastructure.annotations</code> Requests a specific IP from Cilium LBIPAM"},{"location":"networking/external-dns/#annotations-on-httproute-resources","title":"Annotations on HTTPRoute Resources","text":"Annotation Placement Purpose <code>external-dns.alpha.kubernetes.io/controller</code> HTTPRoute <code>.metadata.annotations</code> Set to <code>none</code> to prevent DNS record creation <code>external-dns.alpha.kubernetes.io/cloudflare-proxied</code> HTTPRoute <code>.metadata.annotations</code> Override Cloudflare proxy status for this route"},{"location":"networking/external-dns/#example-external-gateway","title":"Example: External Gateway","text":"<pre><code># Gateway definition\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-external\n  labels:\n    external-dns.alpha.kubernetes.io/enabled: \"true\"    # (1)!\n  annotations:\n    external-dns.alpha.kubernetes.io/target: external.example.com  # (2)!\nspec:\n  infrastructure:\n    annotations:\n      lbipam.cilium.io/ips: \"192.168.0.239\"             # (3)!\n</code></pre> <ol> <li>Label (not annotation) that enables external-dns for this gateway.</li> <li>All HTTPRoutes attached to this gateway will create CNAME records pointing to <code>external.example.com</code>.</li> <li>Requests IP <code>192.168.0.239</code> from Cilium LBIPAM for the underlying Service.</li> </ol>"},{"location":"networking/external-dns/#example-http-redirect-route","title":"Example: HTTP Redirect Route","text":"<pre><code># This route must NOT create DNS records\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: envoy-external\n  annotations:\n    external-dns.alpha.kubernetes.io/controller: none    # (1)!\nspec:\n  parentRefs:\n    - name: envoy-external\n      sectionName: http\n</code></pre> <ol> <li><code>controller: none</code> tells external-dns to completely ignore this route. Without this, external-dns would create a DNS record for the redirect route.</li> </ol>"},{"location":"networking/external-dns/#crd-source-dnsendpoint","title":"CRD Source: DNSEndpoint","text":"<p>The <code>crd</code> source allows creating DNS records that are not tied to any Ingress or HTTPRoute. This is used for the Cloudflare tunnel CNAME:</p> pitower/kubernetes/apps/networking/cloudflared/dnsendpoint.yaml<pre><code>apiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  name: cloudflared\n  namespace: networking\nspec:\n  endpoints:\n    - dnsName: \"external.example.com\"\n      recordType: CNAME\n      targets: [\"7ee9277a-e2f3-45ae-a0ac-4e85d39fc334.cfargotunnel.com\"]\n</code></pre> <p>This creates a CNAME record: <code>external.example.com</code> -&gt; <code>&lt;tunnel-id&gt;.cfargotunnel.com</code>.</p> <p>Without this, Cloudflare would not know how to route traffic for <code>external.example.com</code> to the tunnel.</p>"},{"location":"networking/external-dns/#record-creation-flow","title":"Record Creation Flow","text":"<p>Here is how a DNS record gets created when you deploy an app:</p> <pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant K8s as Kubernetes API\n    participant EDNS as external-dns\n    participant CF as Cloudflare API\n\n    Dev-&gt;&gt;K8s: Create HTTPRoute&lt;br/&gt;(hostname: myapp.example.com,&lt;br/&gt;parentRef: envoy-external)\n    K8s-&gt;&gt;EDNS: Watch event: new HTTPRoute\n    EDNS-&gt;&gt;EDNS: Check parent Gateway label&lt;br/&gt;(enabled=true? Yes)\n    EDNS-&gt;&gt;EDNS: Get target from Gateway annotation&lt;br/&gt;(external.example.com)\n    EDNS-&gt;&gt;EDNS: Check cloudflare-proxied&lt;br/&gt;(default: true)\n    EDNS-&gt;&gt;CF: Create CNAME record&lt;br/&gt;myapp.example.com \u2192 external.example.com&lt;br/&gt;(proxied: true)\n    EDNS-&gt;&gt;CF: Create TXT record&lt;br/&gt;k8s.myapp.example.com&lt;br/&gt;(ownership record)\n    CF--&gt;&gt;EDNS: Records created</code></pre>"},{"location":"networking/external-dns/#gotchas","title":"Gotchas","text":"<p>Target Annotation Only Works on Gateways</p> <p>The <code>external-dns.alpha.kubernetes.io/target</code> annotation is only read from Gateway resources, not from HTTPRoutes. If you add this annotation to an HTTPRoute, it will be ignored.</p> <pre><code># WRONG - target on HTTPRoute is ignored\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/target: external.example.com  # Ignored!\n</code></pre> <p>Cloudflare-Proxied Annotation on Routes</p> <p>The <code>cloudflare-proxied</code> annotation is read from Route resources to override the default proxy behavior. On the Gateway, it sets the default for all routes attached to that gateway.</p> <pre><code># On Gateway: sets default for all attached routes\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/cloudflare-proxied: \"false\"\n\n# On HTTPRoute: overrides per-route\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/cloudflare-proxied: \"false\"\n</code></pre> <p>Always Add <code>controller: none</code> to Redirect Routes</p> <p>The HTTP-to-HTTPS redirect HTTPRoutes must have <code>external-dns.alpha.kubernetes.io/controller: none</code>. Otherwise, external-dns will create DNS records for the redirect route, which is not desired since the HTTPS route already creates the record.</p> <p>Sync Policy Deletes Records</p> <p>The <code>policy: sync</code> setting means external-dns will delete DNS records from Cloudflare when the corresponding Kubernetes resource is removed. Be careful when deleting HTTPRoutes or Gateways -- the DNS records will be removed within the sync interval.</p> <p>TXT Ownership Records</p> <p>Every DNS record created by external-dns has an accompanying TXT record (prefixed with <code>k8s.</code>) that marks ownership. This prevents external-dns from modifying records it did not create. If you see <code>k8s.myapp.example.com</code> TXT records in Cloudflare, those are ownership markers.</p>"},{"location":"networking/external-dns/#scheduling","title":"Scheduling","text":"<p>external-dns runs on control plane nodes with tolerations for the master taint:</p> <pre><code>nodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n      - matchExpressions:\n          - key: node-role.kubernetes.io/master\n            operator: Exists\ntolerations:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\n    operator: Exists\n</code></pre>"},{"location":"networking/external-dns/#troubleshooting","title":"Troubleshooting","text":""},{"location":"networking/external-dns/#check-external-dns-logs","title":"Check external-dns Logs","text":"<pre><code># View recent logs\nkubectl logs -n networking deploy/external-dns --tail=100\n\n# Follow logs for real-time record creation\nkubectl logs -n networking deploy/external-dns -f\n\n# Filter for specific domain\nkubectl logs -n networking deploy/external-dns | grep \"myapp.example.com\"\n</code></pre>"},{"location":"networking/external-dns/#check-what-records-external-dns-sees","title":"Check What Records external-dns Sees","text":"<pre><code># List all DNS records external-dns manages (via Cloudflare API)\n# Look for \"CREATE\", \"UPDATE\", or \"DELETE\" log entries\nkubectl logs -n networking deploy/external-dns | grep -E \"(CREATE|UPDATE|DELETE)\"\n</code></pre>"},{"location":"networking/external-dns/#verify-gateway-label","title":"Verify Gateway Label","text":"<pre><code># Check if a gateway has the external-dns label\nkubectl get gateway -n networking envoy-external -o jsonpath='{.metadata.labels}'\n\n# Should include: external-dns.alpha.kubernetes.io/enabled: \"true\"\n</code></pre>"},{"location":"networking/external-dns/#check-dnsendpoint-resources","title":"Check DNSEndpoint Resources","text":"<pre><code># List all DNSEndpoint CRDs\nkubectl get dnsendpoints -A\n\n# Inspect the cloudflared tunnel CNAME\nkubectl describe dnsendpoint cloudflared -n networking\n</code></pre>"},{"location":"networking/external-dns/#verify-records-in-cloudflare-via-doh","title":"Verify Records in Cloudflare via DoH","text":"<pre><code># Check the actual Cloudflare record (bypasses router DNS interception)\ncurl -s \"https://1.1.1.1/dns-query?name=myapp.example.com&amp;type=CNAME\" \\\n  -H \"Accept: application/dns-json\" | jq '.Answer'\n\n# Check TXT ownership record\ncurl -s \"https://1.1.1.1/dns-query?name=k8s.myapp.example.com&amp;type=TXT\" \\\n  -H \"Accept: application/dns-json\" | jq '.Answer'\n</code></pre>"},{"location":"networking/external-dns/#force-external-dns-sync","title":"Force external-dns Sync","text":"<p>external-dns syncs periodically (default: every 1 minute). To force a sync, restart the pod:</p> <pre><code>kubectl rollout restart deploy/external-dns -n networking\n</code></pre>"},{"location":"networking/load-balancers/","title":"Load Balancers","text":"<p>Load balancing in the cluster is handled entirely by Cilium's LoadBalancer IP Address Management (LBIPAM). There is no MetalLB -- Cilium's L2 announcements provide the same functionality with fewer components and tighter integration with the CNI.</p>"},{"location":"networking/load-balancers/#how-cilium-lbipam-works","title":"How Cilium LBIPAM Works","text":"<p>When a Kubernetes Service of type <code>LoadBalancer</code> is created, Cilium assigns it an IP address from the configured pool and responds to ARP requests for that IP on all Linux nodes. This allows clients on the LAN to reach the service at the assigned IP.</p> <pre><code>flowchart TB\n    subgraph Pool[\"CiliumLoadBalancerIPPool&lt;br/&gt;192.168.0.220 - 192.168.0.239\"]\n        IP1[.220 - CoreDNS]\n        IP2[.221 - nginx-internal]\n        IP3[.222 - LLDAP]\n        IP4[.226 - Mosquitto]\n        IP5[.227 - Home Assistant]\n        IP6[.228 - Matter Server]\n        IP7[.229 - Jellyfin]\n        IP8[.230 - OTBR]\n        IP9[.231 - nginx-external]\n        IP10[.238 - envoy-internal]\n        IP11[.239 - envoy-external]\n    end\n\n    subgraph Cilium[\"Cilium Agent (per node)\"]\n        L2[L2 Announcement&lt;br/&gt;ARP Responder]\n    end\n\n    Client((LAN Client)) --&gt;|\"ARP: Who has&lt;br/&gt;192.168.0.238?\"| L2\n    L2 --&gt;|\"ARP Reply:&lt;br/&gt;Node MAC\"| Client\n    Client --&gt;|\"IP Traffic\"| IP10\n\n    classDef pool fill:#00b894,stroke:#00a381,color:#fff\n    class IP1,IP2,IP3,IP4,IP5,IP6,IP7,IP8,IP9,IP10,IP11 pool</code></pre>"},{"location":"networking/load-balancers/#ip-assignment-methods","title":"IP Assignment Methods","text":"<p>There are two ways a Service gets an IP from the pool:</p> <p>Automatic assignment: If no specific IP is requested, Cilium picks the next available IP from the pool.</p> <p>Explicit assignment: Services can request a specific IP using the <code>lbipam.cilium.io/ips</code> annotation:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    lbipam.cilium.io/ips: \"192.168.0.238\"\nspec:\n  type: LoadBalancer\n</code></pre> <p>Gateway Infrastructure Annotations</p> <p>For Envoy Gateway, the <code>lbipam.cilium.io/ips</code> annotation is placed in the Gateway's <code>spec.infrastructure.annotations</code> block (not on the Gateway metadata). Envoy Gateway copies these annotations to the Service it creates.</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-internal\nspec:\n  infrastructure:\n    annotations:\n      lbipam.cilium.io/ips: \"192.168.0.238\"\n</code></pre>"},{"location":"networking/load-balancers/#ip-pool-configuration","title":"IP Pool Configuration","text":"<p>The LBIPAM pool is defined by two Cilium CRDs:</p> pitower/kubernetes/apps/kube-system/cilium/config/cilium-l2.yaml<pre><code>apiVersion: cilium.io/v2alpha1\nkind: CiliumL2AnnouncementPolicy\nmetadata:\n  name: policy\nspec:\n  loadBalancerIPs: true\n  nodeSelector:\n    matchLabels:\n      kubernetes.io/os: linux\n---\napiVersion: cilium.io/v2alpha1\nkind: CiliumLoadBalancerIPPool\nmetadata:\n  name: pool\nspec:\n  allowFirstLastIPs: \"Yes\"\n  blocks:\n    - start: 192.168.0.220\n      stop: 192.168.0.239\n</code></pre> Setting Value Purpose <code>loadBalancerIPs</code> <code>true</code> Announce LoadBalancer IPs via L2 (ARP) <code>nodeSelector</code> <code>kubernetes.io/os: linux</code> All Linux nodes participate in L2 announcements <code>allowFirstLastIPs</code> <code>Yes</code> Allow <code>.220</code> and <code>.239</code> to be used (not reserved) <code>blocks</code> <code>.220</code> - <code>.239</code> 20 IP addresses available"},{"location":"networking/load-balancers/#ip-allocation-table","title":"IP Allocation Table","text":"<p>Current LoadBalancer IP assignments across the cluster:</p> IP Address Service Namespace Type Notes <code>192.168.0.220</code> CoreDNS kube-system ClusterDNS Fallback DNS <code>192.168.0.221</code> nginx-internal networking Ingress Controller Internal ingress (legacy) <code>192.168.0.222</code> LLDAP security LDAP Service LDAP on port 389 <code>192.168.0.223</code> -- -- -- Available <code>192.168.0.224</code> -- -- -- Available <code>192.168.0.225</code> -- -- -- Available <code>192.168.0.226</code> Mosquitto home-automation MQTT Broker MQTT on port 1883 <code>192.168.0.227</code> Home Assistant home-automation Home Automation HTTP on port 8123 <code>192.168.0.228</code> Matter Server home-automation Matter Protocol Matter on port 5580 <code>192.168.0.229</code> Jellyfin media Media Server HTTP on port 8096 <code>192.168.0.230</code> OTBR home-automation Thread Border Router Thread/mDNS services <code>192.168.0.231</code> nginx-external networking Ingress Controller Cloudflare tunnel target <code>192.168.0.232</code> -- -- -- Available <code>192.168.0.233</code> -- -- -- Available <code>192.168.0.234</code> -- -- -- Available <code>192.168.0.235</code> -- -- -- Available <code>192.168.0.236</code> -- -- -- Available <code>192.168.0.237</code> -- -- -- Available <code>192.168.0.238</code> envoy-internal networking Gateway Internal services <code>192.168.0.239</code> envoy-external networking Gateway Cloudflare tunnel ingress <p>IP Assignment Convention</p> <p>Networking infrastructure uses the upper end of the pool (<code>.238-.239</code>), home automation services cluster around <code>.226-.230</code>, and utility services use the lower end (<code>.220-.222</code>). The middle range (<code>.232-.237</code>) is available for future services.</p>"},{"location":"networking/load-balancers/#why-certain-services-need-loadbalancer-ips","title":"Why Certain Services Need LoadBalancer IPs","text":"<p>Not all services need a dedicated LoadBalancer IP. Most applications use HTTPRoute/Ingress and are accessible through the Envoy gateways. However, some services require their own IP because:</p> Reason Services Explanation Non-HTTP protocols Mosquitto, LLDAP, Home Assistant, Matter Server MQTT, LDAP, and Matter use TCP protocols that cannot be routed through HTTP gateways mDNS / discovery OTBR, Home Assistant Need to be directly reachable on the LAN for device discovery Media streaming Jellyfin Benefits from direct access for DLNA and client apps Gateway infrastructure envoy-external/internal, nginx-external/internal These ARE the gateways -- they need stable IPs Cluster DNS CoreDNS Must have a known, stable IP for DNS resolution"},{"location":"networking/load-balancers/#adding-a-new-loadbalancer-service","title":"Adding a New LoadBalancer Service","text":"<p>To assign a specific IP to a new LoadBalancer service:</p>"},{"location":"networking/load-balancers/#for-a-standard-service","title":"For a Standard Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  annotations:\n    lbipam.cilium.io/ips: \"192.168.0.223\"  # Pick an available IP\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 8080\n      targetPort: 8080\n</code></pre>"},{"location":"networking/load-balancers/#for-an-envoy-gateway","title":"For an Envoy Gateway","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: my-gateway\nspec:\n  gatewayClassName: envoy\n  infrastructure:\n    annotations:\n      lbipam.cilium.io/ips: \"192.168.0.223\"\n  listeners:\n    - name: https\n      protocol: HTTPS\n      port: 443\n</code></pre>"},{"location":"networking/load-balancers/#for-an-app-template-bjw-s-service","title":"For an App-Template (bjw-s) Service","text":"<pre><code>service:\n  app:\n    controller: my-app\n    type: LoadBalancer\n    annotations:\n      lbipam.cilium.io/ips: \"192.168.0.223\"\n    ports:\n      http:\n        port: 8080\n</code></pre>"},{"location":"networking/load-balancers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"networking/load-balancers/#check-ip-pool-status","title":"Check IP Pool Status","text":"<pre><code># View the IP pool and available IPs\nkubectl get ciliumloadbalancerippools\n\n# Check detailed pool status\nkubectl describe ciliumloadbalancerippool pool\n</code></pre>"},{"location":"networking/load-balancers/#list-all-loadbalancer-services","title":"List All LoadBalancer Services","text":"<pre><code># List all LoadBalancer services with their external IPs\nkubectl get svc -A --field-selector spec.type=LoadBalancer\n</code></pre>"},{"location":"networking/load-balancers/#check-l2-announcement-status","title":"Check L2 Announcement Status","text":"<pre><code># Verify the L2 announcement policy\nkubectl get ciliuml2announcementpolicies\n\n# Check which node is announcing a specific IP\nkubectl get leases -n kube-system | grep cilium-l2\n</code></pre>"},{"location":"networking/load-balancers/#ip-conflict-troubleshooting","title":"IP Conflict Troubleshooting","text":"<p>IP Conflicts</p> <p>If two services try to use the same IP, Cilium will assign it to whichever service was created first. The second service will remain in <code>Pending</code> state. Check for conflicts:</p> <pre><code># Find services stuck in Pending\nkubectl get svc -A --field-selector spec.type=LoadBalancer | grep Pending\n\n# Check events for IP allocation issues\nkubectl events -A --field-selector reason=IPAllocationFailed\n</code></pre>"},{"location":"networking/load-balancers/#verify-arp-responses","title":"Verify ARP Responses","text":"<pre><code># From a LAN machine, check ARP for a LoadBalancer IP\narp -n 192.168.0.238\n\n# Or use arping to test\narping -c 3 192.168.0.238\n</code></pre>"},{"location":"networking/tailscale/","title":"Tailscale","text":"<p>Tailscale provides secure remote access to the cluster through a WireGuard-based mesh VPN. The Tailscale operator runs in the cluster and acts as a subnet router (exposing cluster and LAN networks) and an exit node (allowing all traffic to route through the cluster).</p>"},{"location":"networking/tailscale/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Remote\n        User[Remote User&lt;br/&gt;Tailscale Client]\n    end\n\n    subgraph Tailscale Network\n        Coord[Tailscale&lt;br/&gt;Coordination Server]\n        DERP[DERP Relay&lt;br/&gt;fallback]\n    end\n\n    subgraph Cluster[\"Cluster\"]\n        TSO[Tailscale Operator]\n        TSC[Tailscale Connector&lt;br/&gt;Subnet Router + Exit Node]\n\n        subgraph Networks\n            PodNet[\"Pod Network&lt;br/&gt;10.244.0.0/16\"]\n            SvcNet[\"Service Network&lt;br/&gt;10.96.0.0/12\"]\n            LAN[\"LAN&lt;br/&gt;192.168.0.0/24\"]\n        end\n\n        EI[envoy-internal&lt;br/&gt;192.168.0.238]\n    end\n\n    User --&gt;|\"WireGuard&lt;br/&gt;(direct or DERP)\"| Coord\n    Coord --&gt; TSC\n    User -.-&gt;|\"fallback\"| DERP\n    DERP -.-&gt; TSC\n\n    TSC --&gt; PodNet\n    TSC --&gt; SvcNet\n    TSC --&gt; LAN\n    LAN --&gt; EI\n\n    classDef ts fill:#4f46e5,stroke:#3730a3,color:#fff\n    class TSO,TSC ts</code></pre>"},{"location":"networking/tailscale/#operator-configuration","title":"Operator Configuration","text":"<p>The Tailscale operator is deployed via Helm with API server proxy mode enabled:</p> pitower/kubernetes/apps/networking/tailscale/operator/values.yaml<pre><code>operatorConfig:\n  hostname: tailscale-operator\napiServerProxyConfig:\n  mode: \"true\"\n</code></pre>"},{"location":"networking/tailscale/#api-server-proxy","title":"API Server Proxy","text":"<p>With <code>apiServerProxyConfig.mode: \"true\"</code>, the Tailscale operator acts as a proxy for the Kubernetes API server. This allows you to access <code>kubectl</code> commands remotely through Tailscale without exposing the API server publicly.</p> <pre><code># From a remote machine connected to Tailscale\nkubectl --server=https://tailscale-operator:443 get pods -A\n</code></pre> <p>kubectl via Tailscale</p> <p>Configure your kubeconfig to use the Tailscale operator as the API server endpoint. This provides authenticated, encrypted access to the cluster API from anywhere in your Tailscale network.</p>"},{"location":"networking/tailscale/#connector-subnet-router-exit-node","title":"Connector (Subnet Router + Exit Node)","text":"<p>The Tailscale Connector resource configures the subnet router and exit node:</p> pitower/kubernetes/apps/networking/tailscale/connectors/connector.yaml<pre><code>apiVersion: tailscale.com/v1alpha1\nkind: Connector\nmetadata:\n  name: home-ops\nspec:\n  hostname: home-ops\n  subnetRouter:\n    advertiseRoutes:\n      - 10.244.0.0/16\n      - 10.96.0.0/12\n      - 192.168.0.0/24\n  exitNode: true\n</code></pre>"},{"location":"networking/tailscale/#advertised-routes","title":"Advertised Routes","text":"CIDR Network Purpose <code>10.244.0.0/16</code> Pod network Direct access to pod IPs from remote machines <code>10.96.0.0/12</code> Service network Access ClusterIP services by their service IP <code>192.168.0.0/24</code> LAN Access LAN devices including LoadBalancer IPs (192.168.0.220-239) <p>LAN Access</p> <p>The <code>192.168.0.0/24</code> route is the most important for daily use. It allows remote access to:</p> <ul> <li>envoy-internal (<code>192.168.0.238</code>) -- internal-only web services</li> <li>envoy-external (<code>192.168.0.239</code>) -- external services without going through Cloudflare</li> <li>Any other LAN device (NAS, router admin, etc.)</li> </ul>"},{"location":"networking/tailscale/#exit-node","title":"Exit Node","text":"<p>The <code>exitNode: true</code> setting allows Tailscale clients to route all their internet traffic through the cluster. This is useful for:</p> <ul> <li>Accessing services that are geo-restricted to the cluster's location</li> <li>Using the cluster's DNS configuration for all queries</li> <li>Routing all traffic through the home network when traveling</li> </ul> <p>Exit Node Bandwidth</p> <p>When using the exit node, all internet traffic from your device passes through the cluster's internet connection. Be mindful of bandwidth constraints, especially on residential connections.</p>"},{"location":"networking/tailscale/#secrets","title":"Secrets","text":"<p>The Tailscale operator requires authentication credentials stored in a secret (managed via External Secrets from 1Password):</p> <ul> <li>Client ID and Secret: OAuth client credentials for the Tailscale API</li> <li>Auth Key: Pre-authentication key for automatic device registration</li> </ul>"},{"location":"networking/tailscale/#typical-usage","title":"Typical Usage","text":""},{"location":"networking/tailscale/#access-internal-services-remotely","title":"Access Internal Services Remotely","text":"<ol> <li>Connect to Tailscale on your device</li> <li>Navigate to <code>https://hubble.example.com</code> (or any internal service)</li> <li>Traffic routes: Device -&gt; Tailscale -&gt; the subnet router -&gt; 192.168.0.238 -&gt; envoy-internal -&gt; app</li> </ol>"},{"location":"networking/tailscale/#access-kubernetes-api","title":"Access Kubernetes API","text":"<pre><code># Configure kubectl to use Tailscale proxy\nexport KUBERNETES_SERVICE_HOST=tailscale-operator\nexport KUBERNETES_SERVICE_PORT=443\n\n# Or use --server flag\nkubectl --server=https://tailscale-operator:443 get nodes\n</code></pre>"},{"location":"networking/tailscale/#use-as-exit-node","title":"Use as Exit Node","text":"<ol> <li>Enable exit node on your Tailscale client</li> <li>Select the exit node</li> <li>All traffic now routes through the cluster</li> </ol>"},{"location":"networking/tailscale/#troubleshooting","title":"Troubleshooting","text":""},{"location":"networking/tailscale/#check-tailscale-operator-status","title":"Check Tailscale Operator Status","text":"<pre><code># Check operator pod\nkubectl get pods -n networking -l app.kubernetes.io/name=tailscale-operator\n\n# Check operator logs\nkubectl logs -n networking -l app.kubernetes.io/name=tailscale-operator --tail=50\n</code></pre>"},{"location":"networking/tailscale/#check-connector-status","title":"Check Connector Status","text":"<pre><code># Check connector resource\nkubectl get connectors\n\n# Describe for detailed status\nkubectl describe connector home-ops\n</code></pre>"},{"location":"networking/tailscale/#verify-subnet-routes","title":"Verify Subnet Routes","text":"<p>From the Tailscale admin console (<code>https://login.tailscale.com/admin/machines</code>):</p> <ol> <li>Find the the device</li> <li>Check that subnet routes <code>10.244.0.0/16</code>, <code>10.96.0.0/12</code>, and <code>192.168.0.0/24</code> are approved</li> <li>Verify exit node is enabled</li> </ol> <p>Route Approval Required</p> <p>Subnet routes and exit node must be approved in the Tailscale admin console after the connector registers. If routes are advertised but not approved, remote clients will not be able to reach the cluster networks.</p>"},{"location":"networking/tailscale/#test-connectivity","title":"Test Connectivity","text":"<pre><code># From a remote Tailscale-connected device\n# Test LAN access\nping 192.168.0.238\n\n# Test internal service\ncurl -k https://192.168.0.238\n\n# Test pod network access\ncurl http://10.244.x.x:port\n\n# Test service network access\ncurl http://10.96.x.x:port\n</code></pre>"},{"location":"operations/","title":"Operations","text":"<p>Day-to-day operations for the the Kubernetes cluster running on Talos Linux.</p>"},{"location":"operations/#overview","title":"Overview","text":"<p>Cluster operations are automated through a combination of justfile recipes and <code>talosctl</code> commands. The justfile lives at <code>pitower/talos/justfile</code> and provides high-level recipes for common tasks like configuration generation, node management, upgrades, and diagnostics.</p> <pre><code>flowchart TD\n    Operator((Operator))\n    Just[justfile recipes]\n    Talosctl[talosctl]\n    Kubectl[kubectl]\n    ArgoCD[ArgoCD]\n\n    Operator --&gt; Just\n    Operator --&gt; Talosctl\n    Operator --&gt; Kubectl\n\n    Just --&gt;|config, patch, apply| Talosctl\n    Just --&gt;|addons| Kubectl\n    ArgoCD --&gt;|sync| Kubectl\n\n    Talosctl --&gt; Nodes[Talos Nodes]\n    Kubectl --&gt; API[Kubernetes API]</code></pre>"},{"location":"operations/#quick-reference","title":"Quick Reference","text":"Task Command Details Generate configs <code>just config</code> Decrypt secrets and generate Talos machine configs Apply to control planes <code>just apply-controlplanes</code> Push configs to 192.168.0.201-203 Apply to workers <code>just apply-workers</code> Push configs to all worker nodes Reboot control planes <code>just reboot-controlplanes</code> Sequential reboot with wait Upgrade control planes <code>just upgrade-controlplanes</code> ARM image upgrade with --preserve Check cluster health <code>talosctl health</code> Verify etcd, kubelet, and API server View dashboard <code>talosctl dashboard</code> Interactive node dashboard"},{"location":"operations/#sections","title":"Sections","text":"Page Description Justfile Recipes Complete reference of all justfile recipes grouped by category Talos Commands Common <code>talosctl</code> commands for health checks, logs, and debugging Troubleshooting Common issues and their resolutions Upgrades Procedures for upgrading Talos, Kubernetes, and applications"},{"location":"operations/#node-layout","title":"Node Layout","text":"IP Address Hostname Role Architecture 192.168.0.201 worker-01 Control Plane ARM64 (Pi 4) 192.168.0.202 worker-02 Control Plane ARM64 (Pi 4) 192.168.0.203 worker-03 Control Plane AMD64 192.168.0.204 worker-04 Worker AMD64 (Intel) 192.168.0.211 worker-pi-01 Worker ARM64 (Pi) 192.168.0.212 worker-pi-02 Worker ARM64 (Pi) 192.168.0.213 worker-pi-03 Worker ARM64 (Pi) <p>Talos Version</p> <p>The cluster runs Talos Linux v1.12.4 with per-architecture factory images that include system extensions (Intel GPU, AMD, RPi PoE).</p>"},{"location":"operations/justfile-recipes/","title":"Justfile Recipes","text":"<p>Complete reference for all justfile recipes in <code>pitower/talos/justfile</code>. Run recipes from the <code>pitower/talos/</code> directory with <code>just &lt;recipe&gt;</code>.</p> <p>Running Recipes</p> <pre><code>cd pitower/talos\njust &lt;recipe-name&gt;\n</code></pre>"},{"location":"operations/justfile-recipes/#configuration-generation","title":"Configuration Generation","text":"<p>Recipes for generating and patching Talos machine configurations from encrypted secrets and patch files.</p> Recipe Description <code>config</code> Generate Talos config from SOPS-encrypted secrets <code>patch</code> Patch machine configs for all nodes using per-node patches"},{"location":"operations/justfile-recipes/#just-config","title":"<code>just config</code>","text":"<p>Decrypts <code>secrets.sops.yaml</code> with SOPS, then runs <code>talosctl gen config</code> to produce base control plane and worker configs in <code>clusterconfig/</code>.</p> <pre><code>just config\n</code></pre> <p>Internally runs:</p> <pre><code>sops -d ./secrets.sops.yaml &gt; ./secrets.yaml\ntalosctl gen config \\\n    pitower \\\n    https://192.168.0.200:6443 \\\n    --with-secrets ./secrets.yaml \\\n    --with-examples=true \\\n    --config-patch-control-plane @patches/controlplane.patch \\\n    --config-patch @patches/general.patch \\\n    --output ./clusterconfig \\\n    --force\n</code></pre>"},{"location":"operations/justfile-recipes/#just-patch","title":"<code>just patch</code>","text":"<p>Takes the base <code>controlplane.yaml</code> and <code>worker.yaml</code> from <code>clusterconfig/</code> and applies per-node patches to produce individual node configs.</p> <pre><code>just patch\n</code></pre> <p>This generates files like <code>worker-01.yaml</code>, <code>worker-04.yaml</code>, <code>worker-pi-01.yaml</code>, etc.</p>"},{"location":"operations/justfile-recipes/#deployment","title":"Deployment","text":"<p>Recipes for applying configurations to nodes and bootstrapping the cluster.</p> Recipe Description <code>bootstrap</code> Bootstrap the Talos cluster (first-time setup) <code>apply-controlplanes</code> Apply configs to control plane nodes (192.168.0.201-203) <code>apply-workers</code> Apply configs to all worker nodes <code>apply-worker &lt;name&gt;</code> Apply config to a specific worker by name suffix <code>addons</code> Build and apply kustomize addons (CNI, kubelet-csr-approver)"},{"location":"operations/justfile-recipes/#just-bootstrap","title":"<code>just bootstrap</code>","text":"<p>First-time cluster bootstrap. Bootstraps etcd on node 201 and fetches the kubeconfig.</p> <pre><code>just bootstrap\n</code></pre> <p>One-Time Operation</p> <p>Only run <code>bootstrap</code> once during initial cluster setup. Running it again can corrupt the cluster.</p>"},{"location":"operations/justfile-recipes/#just-apply-controlplanes","title":"<code>just apply-controlplanes</code>","text":"<p>Applies machine configs to all three control plane nodes.</p> <pre><code>just apply-controlplanes\n</code></pre>"},{"location":"operations/justfile-recipes/#just-apply-workers","title":"<code>just apply-workers</code>","text":"<p>Applies machine configs to all worker nodes (Intel and Raspberry Pi).</p> <pre><code>just apply-workers\n</code></pre>"},{"location":"operations/justfile-recipes/#just-apply-worker-name","title":"<code>just apply-worker &lt;name&gt;</code>","text":"<p>Applies config to a single worker by name suffix. Automatically resolves the node's IP address from Talos member list.</p> <pre><code># Apply to worker-04 (Intel)\njust apply-worker 04\n\n# Apply to worker-pi-01 (Raspberry Pi)\njust apply-worker pi-01\n</code></pre>"},{"location":"operations/justfile-recipes/#just-addons","title":"<code>just addons</code>","text":"<p>Builds kustomize addons with Helm support and applies them to the cluster. Used for CNI (Cilium) and kubelet-csr-approver during bootstrap.</p> <pre><code>just addons\n</code></pre>"},{"location":"operations/justfile-recipes/#reboot","title":"Reboot","text":"<p>Recipes for rebooting nodes sequentially with wait between each node.</p> Recipe Description <code>reboot-controlplanes</code> Reboot control plane nodes one at a time <code>reboot-workers</code> Reboot worker nodes one at a time"},{"location":"operations/justfile-recipes/#just-reboot-controlplanes","title":"<code>just reboot-controlplanes</code>","text":"<p>Reboots control plane nodes 192.168.0.201 through 203 sequentially, waiting for each node to come back before rebooting the next.</p> <pre><code>just reboot-controlplanes\n</code></pre>"},{"location":"operations/justfile-recipes/#just-reboot-workers","title":"<code>just reboot-workers</code>","text":"<p>Reboots worker nodes 192.168.0.211 through 214 sequentially with wait.</p> <pre><code>just reboot-workers\n</code></pre> <p>Sequential Reboot</p> <p>Both reboot recipes use <code>--wait</code> to ensure each node is fully back online before proceeding to the next. This maintains cluster availability throughout the process.</p>"},{"location":"operations/justfile-recipes/#reset","title":"Reset","text":"Recipe Description <code>reset &lt;suffix&gt;</code> Reset a Talos node by its last IP octet"},{"location":"operations/justfile-recipes/#just-reset-suffix","title":"<code>just reset &lt;suffix&gt;</code>","text":"<p>Resets a node by wiping its ephemeral and meta partitions, then reboots. Use the last two digits of the node's IP address.</p> <pre><code># Reset node at 192.168.0.204\njust reset 04\n\n# Reset node at 192.168.0.211\njust reset 11\n</code></pre> <p>Destructive Operation</p> <p>This wipes the node's ephemeral storage and metadata. The node will need to be re-joined to the cluster. Uses <code>--graceful=false</code> for a forced reset.</p>"},{"location":"operations/justfile-recipes/#upgrade","title":"Upgrade","text":"<p>Recipes for upgrading Talos on nodes, grouped by architecture. All upgrades use <code>--preserve</code> to retain ephemeral data and <code>--wait</code> for sequential processing.</p> Recipe Description <code>upgrade-controlplanes</code> Upgrade control plane nodes with ARM image <code>upgrade-controlplanes-amd</code> Upgrade control plane nodes with AMD image <code>upgrade-workers-intel</code> Upgrade Intel/AMD worker nodes <code>upgrade-workers-rpi</code> Upgrade Raspberry Pi worker nodes"},{"location":"operations/justfile-recipes/#just-upgrade-controlplanes","title":"<code>just upgrade-controlplanes</code>","text":"<p>Upgrades ARM-based control plane nodes (201-203) to the current Talos version.</p> <pre><code>just upgrade-controlplanes\n</code></pre>"},{"location":"operations/justfile-recipes/#just-upgrade-controlplanes-amd","title":"<code>just upgrade-controlplanes-amd</code>","text":"<p>Upgrades AMD-based control plane nodes (currently node 203 only).</p> <pre><code>just upgrade-controlplanes-amd\n</code></pre>"},{"location":"operations/justfile-recipes/#just-upgrade-workers-intel","title":"<code>just upgrade-workers-intel</code>","text":"<p>Upgrades Intel/AMD worker nodes (currently node 204).</p> <pre><code>just upgrade-workers-intel\n</code></pre>"},{"location":"operations/justfile-recipes/#just-upgrade-workers-rpi","title":"<code>just upgrade-workers-rpi</code>","text":"<p>Upgrades Raspberry Pi worker nodes (211-213).</p> <pre><code>just upgrade-workers-rpi\n</code></pre> <p>Factory Images</p> <p>Each architecture uses a different factory image with tailored system extensions:</p> <ul> <li>ARM control plane: includes base extensions for Pi 4</li> <li>AMD control plane: includes AMD-specific extensions</li> <li>Intel worker: includes Intel GPU and related extensions</li> <li>RPi worker: includes RPi PoE hat extensions</li> </ul>"},{"location":"operations/justfile-recipes/#diagnostics","title":"Diagnostics","text":"<p>Recipes for inspecting node images and disk usage.</p> Recipe Description <code>image-list &lt;node&gt;</code> List container images on a node sorted by size <code>image-usage</code> Show image count and disk usage per node <code>image-id</code> Print Talos factory image IDs from extension files"},{"location":"operations/justfile-recipes/#just-image-list-node","title":"<code>just image-list &lt;node&gt;</code>","text":"<p>Lists all container images on a specific node, sorted by size (smallest first).</p> <pre><code>just image-list 192.168.0.201\n</code></pre>"},{"location":"operations/justfile-recipes/#just-image-usage","title":"<code>just image-usage</code>","text":"<p>Shows a summary of image count and containerd disk usage for all nodes.</p> <pre><code>just image-usage\n</code></pre> <p>Example output:</p> <pre><code>192.168.0.201      87 images  2.1 GB\n192.168.0.202      85 images  2.0 GB\n192.168.0.203      92 images  2.4 GB\n192.168.0.204     104 images  3.8 GB\n</code></pre>"},{"location":"operations/justfile-recipes/#just-image-id","title":"<code>just image-id</code>","text":"<p>Queries the Talos factory API to get schematic IDs for each architecture's extension file.</p> <pre><code>just image-id\n</code></pre> <p>Example output:</p> <pre><code>amd: f19ad7b4a5d29151f3a59ef2d9c581cf89e77142e52f0abb5022e8f0b95ad0b9\nintel: 97bf8e92fc6bba0f03928b859c08295d7615737b29db06a97be51dc63004e403\nrpi-poe: a862538d0862e8ad5b17fadc2b56599677101537b3f75926085d8cbff4a411b9\n</code></pre>"},{"location":"operations/justfile-recipes/#complete-recipe-summary","title":"Complete Recipe Summary","text":"Recipe Arguments Category <code>config</code> -- Configuration <code>patch</code> -- Configuration <code>bootstrap</code> -- Deployment <code>apply-controlplanes</code> -- Deployment <code>apply-workers</code> -- Deployment <code>apply-worker</code> <code>&lt;name&gt;</code> (e.g., <code>04</code>, <code>pi-01</code>) Deployment <code>addons</code> -- Deployment <code>reset</code> <code>&lt;suffix&gt;</code> (last IP octet) Reset <code>reboot-controlplanes</code> -- Reboot <code>reboot-workers</code> -- Reboot <code>upgrade-controlplanes</code> -- Upgrade <code>upgrade-controlplanes-amd</code> -- Upgrade <code>upgrade-workers-intel</code> -- Upgrade <code>upgrade-workers-rpi</code> -- Upgrade <code>image-list</code> <code>&lt;node&gt;</code> (IP address) Diagnostics <code>image-usage</code> -- Diagnostics <code>image-id</code> -- Diagnostics"},{"location":"operations/talos-commands/","title":"Talos Commands","text":"<p>Common <code>talosctl</code> commands for managing and debugging the Talos Linux cluster. These commands complement the justfile recipes for lower-level operations.</p> <p>Default Endpoint</p> <p>Most commands default to the endpoint configured in <code>~/.talos/config</code>. Set the default node/endpoint with: <pre><code>talosctl config endpoint 192.168.0.201\ntalosctl config node 192.168.0.201\n</code></pre></p>"},{"location":"operations/talos-commands/#health-checks","title":"Health Checks","text":""},{"location":"operations/talos-commands/#cluster-health","title":"Cluster Health","text":"<p>Check the overall health of the cluster, including etcd, kubelet, and API server status.</p> <pre><code>talosctl health\n</code></pre> <p>Target a specific node:</p> <pre><code>talosctl health --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#node-dashboard","title":"Node Dashboard","text":"<p>Open an interactive dashboard showing real-time CPU, memory, and service status for a node.</p> <pre><code>talosctl dashboard\n</code></pre> <pre><code>talosctl dashboard --nodes 192.168.0.204\n</code></pre>"},{"location":"operations/talos-commands/#member-list","title":"Member List","text":"<p>List all cluster members as seen by Talos.</p> <pre><code>talosctl get members\n</code></pre> <p>JSON output (useful for scripting):</p> <pre><code>talosctl get members -o json | jq '.spec'\n</code></pre>"},{"location":"operations/talos-commands/#service-management","title":"Service Management","text":""},{"location":"operations/talos-commands/#list-services","title":"List Services","text":"<p>Show the status of all Talos services on a node.</p> <pre><code>talosctl services --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#service-logs","title":"Service Logs","text":"<p>View logs for a specific Talos service (e.g., <code>etcd</code>, <code>kubelet</code>, <code>apid</code>, <code>containerd</code>).</p> <pre><code># etcd logs\ntalosctl logs etcd --nodes 192.168.0.201\n\n# kubelet logs\ntalosctl logs kubelet --nodes 192.168.0.201\n\n# Follow logs in real time\ntalosctl logs etcd --nodes 192.168.0.201 -f\n</code></pre>"},{"location":"operations/talos-commands/#service-status","title":"Service Status","text":"<p>Get detailed status for a specific service.</p> <pre><code>talosctl service etcd --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#etcd-operations","title":"etcd Operations","text":""},{"location":"operations/talos-commands/#membership","title":"Membership","text":"<p>List etcd cluster members and their status.</p> <pre><code>talosctl etcd members --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#etcd-status","title":"Etcd Status","text":"<p>Check etcd health and leadership.</p> <pre><code>talosctl etcd status --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#remove-a-member","title":"Remove a Member","text":"<p>Remove a failed etcd member (use with caution).</p> <pre><code>talosctl etcd remove-member &lt;member-id&gt; --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#etcd-snapshot","title":"Etcd Snapshot","text":"<p>Take a snapshot of the etcd database.</p> <pre><code>talosctl etcd snapshot etcd-backup.snapshot --nodes 192.168.0.201\n</code></pre> <p>etcd Quorum</p> <p>With a 3-node control plane, losing more than 1 etcd member will break quorum. Always verify etcd membership before performing maintenance.</p>"},{"location":"operations/talos-commands/#kubeconfig","title":"Kubeconfig","text":""},{"location":"operations/talos-commands/#refresh-kubeconfig","title":"Refresh Kubeconfig","text":"<p>Generate or refresh the kubeconfig file for kubectl access.</p> <pre><code>talosctl kubeconfig --nodes 192.168.0.200\n</code></pre> <p>Write to a specific path:</p> <pre><code>talosctl kubeconfig ~/.kube/config --nodes 192.168.0.200\n</code></pre> <p>VIP Address</p> <p>Use the VIP address (192.168.0.200) for kubeconfig generation to ensure HA access to the API server.</p>"},{"location":"operations/talos-commands/#configuration-inspection","title":"Configuration Inspection","text":""},{"location":"operations/talos-commands/#view-running-config","title":"View Running Config","text":"<p>Inspect the current machine configuration running on a node.</p> <pre><code>talosctl get machineconfig --nodes 192.168.0.201 -o yaml\n</code></pre>"},{"location":"operations/talos-commands/#compare-configs","title":"Compare Configs","text":"<p>Diff the running config against a file.</p> <pre><code>talosctl apply-config --nodes 192.168.0.201 --file ./clusterconfig/worker-01.yaml --dry-run\n</code></pre>"},{"location":"operations/talos-commands/#version-information","title":"Version Information","text":"<p>Check the Talos version on a node.</p> <pre><code>talosctl version --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#resource-inspection","title":"Resource Inspection","text":""},{"location":"operations/talos-commands/#kernel-logs-dmesg","title":"Kernel Logs (dmesg)","text":"<p>View kernel messages from a node.</p> <pre><code>talosctl dmesg --nodes 192.168.0.204\n</code></pre> <p>Follow kernel messages:</p> <pre><code>talosctl dmesg --nodes 192.168.0.204 -f\n</code></pre>"},{"location":"operations/talos-commands/#process-list","title":"Process List","text":"<p>List running processes on a node.</p> <pre><code>talosctl processes --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#disk-usage","title":"Disk Usage","text":"<p>Check disk usage on a node.</p> <pre><code>talosctl usage /var --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#container-images","title":"Container Images","text":"<p>List all container images on a node.</p> <pre><code>talosctl image list --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#network-interfaces","title":"Network Interfaces","text":"<p>Show network interfaces and addresses.</p> <pre><code>talosctl get addresses --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#routes","title":"Routes","text":"<p>Show routing table.</p> <pre><code>talosctl get routes --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#node-operations","title":"Node Operations","text":""},{"location":"operations/talos-commands/#reboot","title":"Reboot","text":"<p>Reboot a node (with wait for it to come back).</p> <pre><code>talosctl reboot --nodes 192.168.0.204 --wait\n</code></pre>"},{"location":"operations/talos-commands/#shutdown","title":"Shutdown","text":"<p>Shut down a node.</p> <pre><code>talosctl shutdown --nodes 192.168.0.204\n</code></pre>"},{"location":"operations/talos-commands/#reset","title":"Reset","text":"<p>Reset a node, wiping its state.</p> <pre><code>talosctl reset \\\n    --system-labels-to-wipe=EPHEMERAL \\\n    --system-labels-to-wipe=META \\\n    --reboot \\\n    --graceful=false \\\n    --nodes 192.168.0.204\n</code></pre>"},{"location":"operations/talos-commands/#upgrade","title":"Upgrade","text":"<p>Upgrade Talos on a node to a new version.</p> <pre><code>talosctl upgrade \\\n    --image factory.talos.dev/installer/&lt;schematic-id&gt;:v1.12.4 \\\n    --nodes 192.168.0.201 \\\n    --preserve \\\n    --wait\n</code></pre>"},{"location":"operations/talos-commands/#troubleshooting-commands","title":"Troubleshooting Commands","text":""},{"location":"operations/talos-commands/#check-pod-cidr-and-service-cidr","title":"Check Pod CIDR and Service CIDR","text":"<pre><code>talosctl get clusterconfig --nodes 192.168.0.201 -o yaml | grep -A5 clusterNetwork\n</code></pre>"},{"location":"operations/talos-commands/#check-certificate-validity","title":"Check Certificate Validity","text":"<pre><code>talosctl get certificate --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#read-machine-config-patches","title":"Read Machine Config Patches","text":"<pre><code>talosctl get machineconfig --nodes 192.168.0.201 -o yaml\n</code></pre>"},{"location":"operations/talos-commands/#check-time-sync","title":"Check Time Sync","text":"<pre><code>talosctl get timestatus --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/talos-commands/#useful-aliases","title":"Useful Aliases","text":"<p>Consider adding these aliases to your shell configuration:</p> <pre><code>alias tc='talosctl'\nalias tcd='talosctl dashboard'\nalias tch='talosctl health'\nalias tcl='talosctl logs'\nalias tcs='talosctl services'\nalias tcm='talosctl get members'\n</code></pre>"},{"location":"operations/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and their resolutions for the cluster.</p>"},{"location":"operations/troubleshooting/#node-not-joining-cluster","title":"Node Not Joining Cluster","text":"<p>Symptoms: Node shows as not ready, or does not appear in <code>kubectl get nodes</code>.</p>"},{"location":"operations/troubleshooting/#check-talos-health","title":"Check Talos Health","text":"<pre><code>talosctl health --nodes &lt;node-ip&gt;\n</code></pre> <p>Look for failures in etcd, kubelet, or API server connectivity.</p>"},{"location":"operations/troubleshooting/#check-etcd-membership","title":"Check etcd Membership","text":"<pre><code>talosctl etcd members --nodes 192.168.0.201\n</code></pre> <p>If the node was previously part of the cluster and was reset, its stale etcd member entry may need to be removed:</p> <pre><code>talosctl etcd remove-member &lt;member-id&gt; --nodes 192.168.0.201\n</code></pre>"},{"location":"operations/troubleshooting/#verify-machine-config","title":"Verify Machine Config","text":"<p>Ensure the node has the correct machine config applied:</p> <pre><code>talosctl apply-config --nodes &lt;node-ip&gt; --file ./clusterconfig/&lt;node-config&gt;.yaml --dry-run\n</code></pre>"},{"location":"operations/troubleshooting/#check-kubelet-csr-approver","title":"Check kubelet-csr-approver","text":"<p>New nodes need their CSRs approved. Verify the kubelet-csr-approver is running:</p> <pre><code>kubectl get pods -n kube-system -l app.kubernetes.io/name=kubelet-csr-approver\nkubectl get csr\n</code></pre> <p>Bootstrap Addons</p> <p>If kubelet-csr-approver is not running, apply the bootstrap addons: <pre><code>cd pitower/talos &amp;&amp; just addons\n</code></pre></p>"},{"location":"operations/troubleshooting/#pod-stuck-in-pending-state","title":"Pod Stuck in Pending State","text":"<p>Symptoms: Pod stays in <code>Pending</code> status and never gets scheduled.</p>"},{"location":"operations/troubleshooting/#check-node-resources","title":"Check Node Resources","text":"<pre><code>kubectl describe node &lt;node-name&gt; | grep -A10 \"Allocated resources\"\nkubectl top nodes\n</code></pre>"},{"location":"operations/troubleshooting/#check-storage","title":"Check Storage","text":"<p>If the pod requires a PVC, verify the storage class and available capacity:</p> <pre><code>kubectl get pvc -n &lt;namespace&gt;\nkubectl describe pvc &lt;pvc-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>For Rook Ceph:</p> <pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph status\nkubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd df\n</code></pre> <p>For OpenEBS (local PV):</p> <pre><code>kubectl get blockdevice -n openebs\n</code></pre>"},{"location":"operations/troubleshooting/#check-pod-events","title":"Check Pod Events","text":"<pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\nkubectl get events -n &lt;namespace&gt; --sort-by='.lastTimestamp' | tail -20\n</code></pre>"},{"location":"operations/troubleshooting/#check-node-taints","title":"Check Node Taints","text":"<pre><code>kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\n</code></pre>"},{"location":"operations/troubleshooting/#dns-not-resolving","title":"DNS Not Resolving","text":"<p>Symptoms: Services cannot resolve DNS names, or external DNS records are not created.</p>"},{"location":"operations/troubleshooting/#ubiquiti-dns-interception","title":"Ubiquiti DNS Interception","text":"<p>Port 53 Interception</p> <p>The Ubiquiti router intercepts all DNS traffic on port 53. This means standard DNS lookups may return the router's cached results rather than actual Cloudflare records.</p>"},{"location":"operations/troubleshooting/#verify-with-doh-dns-over-https","title":"Verify with DoH (DNS over HTTPS)","text":"<p>To check actual Cloudflare DNS records, bypass the router's interception using DoH:</p> <pre><code># Using curl to query Cloudflare DoH\ncurl -sH 'accept: application/dns-json' \\\n  'https://cloudflare-dns.com/dns-query?name=echo.example.com&amp;type=A' | jq\n\n# Using dig with DoH (if supported)\ndig @1.1.1.1 echo.example.com +https\n</code></pre>"},{"location":"operations/troubleshooting/#check-coredns","title":"Check CoreDNS","text":"<pre><code>kubectl get pods -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=kube-dns --tail=50\n</code></pre>"},{"location":"operations/troubleshooting/#check-external-dns","title":"Check external-dns","text":"<pre><code>kubectl get pods -n networking -l app.kubernetes.io/name=external-dns\nkubectl logs -n networking -l app.kubernetes.io/name=external-dns --tail=50\n</code></pre> <p>Verify external-dns is watching the correct gateways:</p> <pre><code>kubectl get gateways -A -l external-dns.alpha.kubernetes.io/enabled=true\n</code></pre> <p>Gateway Label Filter</p> <p>external-dns uses <code>--gateway-label-filter=external-dns.alpha.kubernetes.io/enabled=true</code> to select which gateways to process. Ensure the target gateway has this label.</p>"},{"location":"operations/troubleshooting/#check-httproute-and-gateway","title":"Check HTTPRoute and Gateway","text":"<pre><code>kubectl get httproutes -A\nkubectl get gateways -A\n</code></pre>"},{"location":"operations/troubleshooting/#certificate-issues","title":"Certificate Issues","text":"<p>Symptoms: TLS errors, expired certificates, or certificates not being issued.</p>"},{"location":"operations/troubleshooting/#check-cert-manager","title":"Check cert-manager","text":"<pre><code>kubectl get pods -n cert-manager\nkubectl logs -n cert-manager deploy/cert-manager --tail=50\n</code></pre>"},{"location":"operations/troubleshooting/#check-certificate-status","title":"Check Certificate Status","text":"<pre><code>kubectl get certificates -A\nkubectl get certificaterequests -A\nkubectl get orders.acme.cert-manager.io -A\nkubectl get challenges.acme.cert-manager.io -A\n</code></pre>"},{"location":"operations/troubleshooting/#check-clusterissuer","title":"Check ClusterIssuer","text":"<pre><code>kubectl get clusterissuers\nkubectl describe clusterissuer letsencrypt-production\n</code></pre>"},{"location":"operations/troubleshooting/#force-certificate-renewal","title":"Force Certificate Renewal","text":"<p>Delete the certificate to trigger re-issuance:</p> <pre><code>kubectl delete certificate &lt;cert-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>DNS-01 Challenges</p> <p>If using DNS-01 challenges with Cloudflare, verify the API token has the correct permissions and the DNS zone is accessible.</p>"},{"location":"operations/troubleshooting/#service-not-accessible","title":"Service Not Accessible","text":"<p>Symptoms: Cannot reach a service via its URL, connection timeouts, or 404 errors.</p>"},{"location":"operations/troubleshooting/#check-gateway-status","title":"Check Gateway Status","text":"<pre><code>kubectl get gateways -n networking\nkubectl describe gateway envoy-external -n networking\nkubectl describe gateway envoy-internal -n networking\n</code></pre>"},{"location":"operations/troubleshooting/#check-httproute","title":"Check HTTPRoute","text":"<pre><code>kubectl get httproutes -A\nkubectl describe httproute &lt;route-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Verify the route's <code>parentRefs</code> point to the correct gateway:</p> <ul> <li>envoy-external: For services accessed via Cloudflare tunnel (proxied)</li> <li>envoy-internal: For services accessed via Tailscale/LAN</li> </ul>"},{"location":"operations/troubleshooting/#check-cilium-l2-announcements","title":"Check Cilium L2 Announcements","text":"<p>Verify LoadBalancer IPs are being announced:</p> <pre><code>kubectl get svc -A | grep LoadBalancer\ncilium status\n</code></pre> <p>Check that the Cilium L2 announcement policy is active:</p> <pre><code>kubectl get ciliuml2announcementpolicies\nkubectl get ciliumbgppeeringpolicies\n</code></pre>"},{"location":"operations/troubleshooting/#check-cloudflare-tunnel","title":"Check Cloudflare Tunnel","text":"<p>For externally exposed services:</p> <pre><code>kubectl get pods -n networking -l app.kubernetes.io/name=cloudflared\nkubectl logs -n networking -l app.kubernetes.io/name=cloudflared --tail=50\n</code></pre>"},{"location":"operations/troubleshooting/#check-nginx-reverse-proxy","title":"Check nginx Reverse Proxy","text":"<pre><code>kubectl get pods -n networking -l app.kubernetes.io/name=nginx\nkubectl get svc -n networking | grep nginx\n</code></pre>"},{"location":"operations/troubleshooting/#end-to-end-request-flow","title":"End-to-End Request Flow","text":"<pre><code>flowchart LR\n    Client --&gt; CF[Cloudflare]\n    CF --&gt; Tunnel[cloudflared]\n    Tunnel --&gt; Nginx[nginx]\n    Nginx --&gt; EE[envoy-external&lt;br/&gt;192.168.0.239]\n    EE --&gt; Route[HTTPRoute]\n    Route --&gt; Svc[Service]\n    Svc --&gt; Pod[Pod]</code></pre> <p>Verify each hop in the chain to isolate where the failure occurs.</p>"},{"location":"operations/troubleshooting/#argocd-sync-failed","title":"ArgoCD Sync Failed","text":"<p>Symptoms: Application shows <code>OutOfSync</code>, <code>Degraded</code>, or <code>Unknown</code> in ArgoCD.</p>"},{"location":"operations/troubleshooting/#check-application-status","title":"Check Application Status","text":"<pre><code>kubectl get applications -n argocd\nkubectl describe application &lt;app-name&gt; -n argocd\n</code></pre>"},{"location":"operations/troubleshooting/#argocd-cli","title":"ArgoCD CLI","text":"<pre><code>argocd app list\nargocd app get &lt;app-name&gt;\nargocd app diff &lt;app-name&gt;\n</code></pre>"},{"location":"operations/troubleshooting/#common-sync-failures","title":"Common Sync Failures","text":""},{"location":"operations/troubleshooting/#resource-already-exists","title":"Resource Already Exists","text":"<p>If a resource was manually created, ArgoCD may fail to adopt it:</p> <pre><code>argocd app sync &lt;app-name&gt; --force\n</code></pre>"},{"location":"operations/troubleshooting/#schema-validation-errors","title":"Schema Validation Errors","text":"<p>CRDs may not be installed yet when the app tries to sync:</p> <pre><code># Check if CRDs exist\nkubectl get crds | grep &lt;crd-name&gt;\n\n# Sync CRDs first if needed\nargocd app sync &lt;crd-app-name&gt;\n</code></pre>"},{"location":"operations/troubleshooting/#health-check-failures","title":"Health Check Failures","text":"<p>Check pod health and events:</p> <pre><code>kubectl get pods -n &lt;namespace&gt; -l app.kubernetes.io/name=&lt;app&gt;\nkubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\nkubectl get events -n &lt;namespace&gt; --sort-by='.lastTimestamp'\n</code></pre>"},{"location":"operations/troubleshooting/#helm-template-errors","title":"Helm Template Errors","text":"<p>For apps using Helm, test rendering locally:</p> <pre><code>cd pitower/kubernetes/apps/&lt;category&gt;/&lt;app&gt;\nkustomize build . --enable-helm\n</code></pre>"},{"location":"operations/troubleshooting/#storage-issues","title":"Storage Issues","text":""},{"location":"operations/troubleshooting/#rook-ceph-degraded","title":"Rook Ceph Degraded","text":"<pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph status\nkubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph health detail\nkubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd tree\n</code></pre>"},{"location":"operations/troubleshooting/#pvc-stuck-in-pending","title":"PVC Stuck in Pending","text":"<pre><code>kubectl get pvc -A | grep Pending\nkubectl describe pvc &lt;pvc-name&gt; -n &lt;namespace&gt;\nkubectl get sc\n</code></pre>"},{"location":"operations/troubleshooting/#volsync-backup-failures","title":"VolSync Backup Failures","text":"<pre><code>kubectl get replicationsources -A\nkubectl describe replicationsource &lt;name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"operations/troubleshooting/#network-issues","title":"Network Issues","text":""},{"location":"operations/troubleshooting/#pod-to-pod-communication","title":"Pod-to-Pod Communication","text":"<pre><code># Test from a debug pod\nkubectl run -it --rm debug --image=busybox -- sh\n# Inside the pod:\nwget -qO- http://&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local:&lt;port&gt;\n</code></pre>"},{"location":"operations/troubleshooting/#cilium-connectivity","title":"Cilium Connectivity","text":"<pre><code>cilium connectivity test\ncilium status --verbose\n</code></pre>"},{"location":"operations/troubleshooting/#envoy-gateway-logs","title":"Envoy Gateway Logs","text":"<pre><code>kubectl logs -n envoy-gateway-system deploy/envoy-gateway --tail=50\n</code></pre>"},{"location":"operations/upgrades/","title":"Upgrades","text":"<p>Procedures for upgrading Talos Linux, Kubernetes, and applications in the cluster.</p>"},{"location":"operations/upgrades/#talos-linux-upgrades","title":"Talos Linux Upgrades","text":"<p>Talos upgrades are performed per-architecture using factory images that include the appropriate system extensions. The cluster currently runs Talos v1.12.4.</p>"},{"location":"operations/upgrades/#architecture-specific-images","title":"Architecture-Specific Images","text":"<p>The justfile defines four factory images, each tailored to a specific hardware type:</p> Variable Architecture Nodes Extensions <code>cp_image</code> ARM64 192.168.0.201-202 Base Pi 4 extensions <code>cp_amd_image</code> AMD64 192.168.0.203 AMD extensions <code>worker_intel_image</code> AMD64 192.168.0.204 Intel GPU extensions <code>worker_rpi_image</code> ARM64 192.168.0.211-213 RPi PoE hat extensions"},{"location":"operations/upgrades/#pre-upgrade-checklist","title":"Pre-Upgrade Checklist","text":"<p>Before upgrading Talos:</p> <ul> <li> Verify cluster health: <code>talosctl health</code></li> <li> Check etcd membership: <code>talosctl etcd members --nodes 192.168.0.201</code></li> <li> Back up etcd: <code>talosctl etcd snapshot etcd-backup.snapshot --nodes 192.168.0.201</code></li> <li> Review the Talos release notes for breaking changes</li> <li> Update the factory image IDs if extensions have changed: <code>just image-id</code></li> <li> Update the image variables in the justfile</li> </ul>"},{"location":"operations/upgrades/#upgrade-procedure","title":"Upgrade Procedure","text":"<p>Sequential Upgrades</p> <p>All upgrade recipes process nodes one at a time with <code>--wait</code> and <code>--preserve</code>. Never upgrade all nodes simultaneously.</p>"},{"location":"operations/upgrades/#step-1-upgrade-control-plane-nodes","title":"Step 1: Upgrade Control Plane Nodes","text":"<p>Upgrade ARM-based control plane nodes first:</p> <pre><code>cd pitower/talos\njust upgrade-controlplanes\n</code></pre> <p>Then upgrade AMD control plane node(s):</p> <pre><code>just upgrade-controlplanes-amd\n</code></pre> <p>Each node upgrade:</p> <ol> <li>Downloads the new Talos image</li> <li>Stages the upgrade</li> <li>Reboots the node</li> <li>Waits for the node to come back with the new version</li> <li>Proceeds to the next node</li> </ol>"},{"location":"operations/upgrades/#step-2-upgrade-worker-nodes","title":"Step 2: Upgrade Worker Nodes","text":"<p>Upgrade Intel/AMD workers:</p> <pre><code>just upgrade-workers-intel\n</code></pre> <p>Upgrade Raspberry Pi workers:</p> <pre><code>just upgrade-workers-rpi\n</code></pre>"},{"location":"operations/upgrades/#step-3-verify","title":"Step 3: Verify","text":"<pre><code>talosctl version --nodes 192.168.0.201\ntalosctl health\nkubectl get nodes -o wide\n</code></pre>"},{"location":"operations/upgrades/#the-preserve-flag","title":"The <code>--preserve</code> Flag","text":"<p>All upgrade commands use <code>--preserve</code>, which retains the node's ephemeral partition data across the upgrade. This means:</p> <ul> <li>Container images are preserved (faster restart)</li> <li>Local PVs are retained</li> <li>The node rejoins the cluster faster</li> </ul> <p>When NOT to Preserve</p> <p>Omit <code>--preserve</code> only when you want a clean slate (e.g., after major version upgrades that require a fresh state).</p>"},{"location":"operations/upgrades/#updating-factory-images","title":"Updating Factory Images","text":"<p>When Talos releases a new version or you change system extensions:</p> <ol> <li> <p>Update extension files in <code>pitower/talos/extensions/</code> (e.g., <code>amd.yaml</code>, <code>intel.yaml</code>, <code>rpi-poe.yaml</code>)</p> </li> <li> <p>Get new schematic IDs:</p> <pre><code>just image-id\n</code></pre> </li> <li> <p>Update the image variables at the top of the justfile with the new schematic IDs and version tag</p> </li> <li> <p>Proceed with the upgrade</p> </li> </ol>"},{"location":"operations/upgrades/#kubernetes-version-upgrades","title":"Kubernetes Version Upgrades","text":"<p>Kubernetes version is managed by Talos. When upgrading Talos, check whether the new Talos release includes a Kubernetes version bump.</p>"},{"location":"operations/upgrades/#check-current-kubernetes-version","title":"Check Current Kubernetes Version","text":"<pre><code>kubectl version\ntalosctl get machineconfig --nodes 192.168.0.201 -o yaml | grep kubernetesVersion\n</code></pre>"},{"location":"operations/upgrades/#upgrade-kubernetes","title":"Upgrade Kubernetes","text":"<p>If you need to upgrade Kubernetes independently of Talos:</p> <pre><code>talosctl upgrade-k8s --to &lt;version&gt; --nodes 192.168.0.201\n</code></pre> <p>Talos-Managed Kubernetes</p> <p>Talos manages the Kubernetes control plane components (API server, controller manager, scheduler, etcd). A Talos upgrade often includes a Kubernetes version bump. Check the Talos release notes for the bundled Kubernetes version.</p>"},{"location":"operations/upgrades/#post-upgrade-verification","title":"Post-Upgrade Verification","text":"<pre><code>kubectl get nodes -o wide\nkubectl get pods -A | grep -v Running | grep -v Completed\ntalosctl health\n</code></pre>"},{"location":"operations/upgrades/#application-upgrades","title":"Application Upgrades","text":"<p>Application upgrades are handled automatically by Renovate and deployed via ArgoCD.</p>"},{"location":"operations/upgrades/#renovate-workflow","title":"Renovate Workflow","text":"<pre><code>flowchart LR\n    Renovate[Renovate Bot] --&gt;|Creates PR| GitHub[GitHub]\n    GitHub --&gt;|Auto-merge&lt;br/&gt;digest/patch| Main[main branch]\n    GitHub --&gt;|Manual review&lt;br/&gt;minor/major| Review[Review]\n    Review --&gt;|Merge| Main\n    Main --&gt;|Webhook| ArgoCD[ArgoCD]\n    ArgoCD --&gt;|Sync| Cluster[Cluster]</code></pre>"},{"location":"operations/upgrades/#auto-merge-rules","title":"Auto-Merge Rules","text":"<p>Renovate automatically merges certain update types:</p> Update Type Auto-Merge Docker digest updates Yes Docker patch versions Yes Docker pin/pinDigest Yes Helm patch versions Yes Helm digest/pin Yes KPS minor/patch Yes GitHub Actions digest/patch Yes Docker minor versions No (manual review) Docker major versions No (manual review) Helm minor versions No (manual review)"},{"location":"operations/upgrades/#manual-application-upgrade","title":"Manual Application Upgrade","text":"<p>To manually upgrade an application:</p> <ol> <li> <p>Update the image tag or chart version in the app's <code>values.yaml</code> or <code>kustomization.yaml</code>:</p> <pre><code># kustomization.yaml\nhelmCharts:\n  - name: app-template\n    repo: oci://ghcr.io/bjw-s-labs/helm\n    version: 4.6.2  # Update this\n</code></pre> <pre><code># values.yaml\ncontrollers:\n  app-name:\n    containers:\n      app:\n        image:\n          repository: ghcr.io/example/app\n          tag: 2.0.0  # Update this\n</code></pre> </li> <li> <p>Commit and push to <code>main</code></p> </li> <li> <p>ArgoCD will automatically detect the change and sync</p> </li> </ol>"},{"location":"operations/upgrades/#helm-chart-upgrades","title":"Helm Chart Upgrades","text":"<p>For bjw-s app-template chart upgrades:</p> <ol> <li>Check the release notes for breaking changes</li> <li>Update the <code>version</code> field in all <code>kustomization.yaml</code> files</li> <li> <p>Test locally:</p> <pre><code>cd pitower/kubernetes/apps/&lt;category&gt;/&lt;app&gt;\nkustomize build . --enable-helm\n</code></pre> </li> <li> <p>Push to <code>main</code> for ArgoCD to pick up</p> </li> </ol>"},{"location":"operations/upgrades/#upgrade-order-of-operations","title":"Upgrade Order of Operations","text":"<p>For a full stack upgrade, follow this order:</p> <ol> <li>Talos Linux -- Foundation must be upgraded first</li> <li>Kubernetes -- Usually bundled with Talos</li> <li>CNI (Cilium) -- Network layer before workloads</li> <li>Storage (Rook Ceph, OpenEBS) -- Storage layer before workloads</li> <li>Core Infrastructure -- cert-manager, external-secrets, ArgoCD</li> <li>Applications -- Workloads last</li> </ol> <p>Never Skip Major Versions</p> <p>Always upgrade incrementally. Do not skip major versions of Talos, Kubernetes, or critical infrastructure components.</p>"},{"location":"operations/upgrades/#rollback-procedures","title":"Rollback Procedures","text":""},{"location":"operations/upgrades/#talos-rollback","title":"Talos Rollback","text":"<p>Talos keeps the previous version available. If an upgrade fails:</p> <pre><code>talosctl rollback --nodes &lt;node-ip&gt;\n</code></pre>"},{"location":"operations/upgrades/#application-rollback","title":"Application Rollback","text":"<p>Use ArgoCD to roll back to a previous sync:</p> <pre><code>argocd app rollback &lt;app-name&gt;\n</code></pre> <p>Or revert the Git commit:</p> <pre><code>git revert &lt;commit-hash&gt;\ngit push origin main\n</code></pre> <p>ArgoCD will automatically sync the reverted state.</p>"},{"location":"reference/","title":"Reference","text":"<p>Quick-reference tables and catalogs for the cluster.</p>"},{"location":"reference/#sections","title":"Sections","text":"Page Description IP Allocation Complete IP address allocation table and DNS records App Catalog Full catalog of all applications deployed in the cluster"},{"location":"reference/#quick-links","title":"Quick Links","text":""},{"location":"reference/#network","title":"Network","text":"<ul> <li>Domain: <code>example.com</code> (Cloudflare-managed)</li> <li>Cluster VIP: <code>192.168.0.200</code></li> <li>LoadBalancer Pool: <code>192.168.0.220-239</code> (Cilium LBIPAM)</li> <li>Control Plane Nodes: <code>192.168.0.201-203</code></li> <li>Worker Nodes: <code>192.168.0.204</code>, <code>192.168.0.211-213</code></li> </ul>"},{"location":"reference/#gateways","title":"Gateways","text":"Gateway IP Purpose envoy-external 192.168.0.239 Cloudflare tunnel traffic (<code>external.example.com</code>) envoy-internal 192.168.0.238 Internal/VPN traffic (<code>internal.example.com</code>)"},{"location":"reference/#key-versions","title":"Key Versions","text":"Component Version Talos Linux v1.12.4 app-template 4.6.2"},{"location":"reference/app-catalog/","title":"App Catalog","text":"<p>Complete catalog of all applications deployed in the cluster, organized by category.</p>"},{"location":"reference/app-catalog/#summary","title":"Summary","text":"Category App Count AI 1 Banking 2 Cert-Manager 2 CloudNativePG 3 Home Automation 5 Kube-System 3 Media 7 Monitoring 4 Networking 6 OpenEBS 1 Rook Ceph 3 Security 5 Self-Hosted 12 System 6 Total 60"},{"location":"reference/app-catalog/#ai","title":"AI","text":"Name Chart/Image Gateway URL Description kagent app-template -- -- AI agent workload"},{"location":"reference/app-catalog/#banking","title":"Banking","text":"Name Chart/Image Gateway URL Description firefly app-template external <code>firefly.example.com</code> Firefly III personal finance manager firefly-importer app-template external <code>firefly-importer.example.com</code> Data importer for Firefly III"},{"location":"reference/app-catalog/#cert-manager","title":"Cert-Manager","text":"Name Chart/Image Gateway URL Description cert-manager cert-manager (Helm) -- -- X.509 certificate management for Kubernetes issuers Kustomize manifests -- -- ClusterIssuer resources for Let's Encrypt"},{"location":"reference/app-catalog/#cloudnativepg","title":"CloudNativePG","text":"Name Chart/Image Gateway URL Description operator cloudnative-pg (Helm) -- -- CloudNativePG operator for PostgreSQL cluster Kustomize manifests -- -- PostgreSQL cluster instance clustersecretstore Kustomize manifests -- -- Secret store for database credentials"},{"location":"reference/app-catalog/#home-automation","title":"Home Automation","text":"Name Chart/Image Gateway URL Description home-assistant app-template external <code>hass.example.com</code> Home automation platform matter-server app-template -- -- Matter protocol server for smart home devices mosquitto app-template -- -- MQTT broker for IoT messaging otbr app-template -- -- OpenThread Border Router for Thread devices zigbee2mqtt app-template external <code>z2m.example.com</code> Zigbee to MQTT bridge"},{"location":"reference/app-catalog/#kube-system","title":"Kube-System","text":"Name Chart/Image Gateway URL Description cilium cilium (Helm) -- -- eBPF-based CNI with L2 announcements, DSR, Maglev coredns coredns (Helm) -- -- Cluster DNS server metrics-server metrics-server (Helm) -- -- Resource metrics for HPA and kubectl top"},{"location":"reference/app-catalog/#media","title":"Media","text":"Name Chart/Image Gateway URL Description autobrr app-template external <code>autobrr.example.com</code> Automation for torrent/usenet indexers jellyfin app-template external <code>jellyfin.example.com</code> Open-source media server prowlarr app-template external <code>prowlarr.example.com</code> Indexer manager for Sonarr/Radarr qbittorrent app-template external <code>qbit.example.com</code> BitTorrent client radarr app-template external <code>radarr.example.com</code> Movie collection manager sabnzbd app-template external <code>sabnzbd.example.com</code> Usenet download client sonarr app-template external <code>sonarr.example.com</code> TV series collection manager"},{"location":"reference/app-catalog/#monitoring","title":"Monitoring","text":"Name Chart/Image Gateway URL Description fluent-bit fluent-bit (Helm) -- -- Log collector and forwarder grafana grafana (Helm) external <code>grafana.example.com</code> Metrics visualization and dashboards kube-prometheus-stack kube-prometheus-stack (Helm) external <code>prometheus.example.com</code> Prometheus, Alertmanager, and node exporters loki loki (Helm) -- -- Log aggregation system"},{"location":"reference/app-catalog/#networking","title":"Networking","text":"Name Chart/Image Gateway URL Description cloudflared app-template -- -- Cloudflare tunnel for secure external access envoy-gateway envoy-gateway (Helm) -- -- Gateway API implementation (2 gateways) external-dns external-dns (Helm) -- -- Automated DNS record management nginx nginx (Helm) -- -- Reverse proxy (external + internal) openspeedtest app-template internal <code>speedtest.example.com</code> Network speed test tool tailscale tailscale (Helm) -- -- VPN mesh for remote access"},{"location":"reference/app-catalog/#openebs","title":"OpenEBS","text":"Name Chart/Image Gateway URL Description openebs openebs (Helm) -- -- Local persistent volume provisioner"},{"location":"reference/app-catalog/#rook-ceph","title":"Rook Ceph","text":"Name Chart/Image Gateway URL Description operator rook-ceph (Helm) -- -- Rook Ceph operator for distributed storage cluster Kustomize manifests -- -- Ceph cluster configuration add-ons Kustomize manifests external <code>ceph.example.com</code> Dashboard and toolbox"},{"location":"reference/app-catalog/#security","title":"Security","text":"Name Chart/Image Gateway URL Description 1password-connect app-template -- -- 1Password Connect server for External Secrets authelia app-template external <code>auth.example.com</code> SSO and 2FA authentication portal aws-identity-webhook Kustomize manifests -- -- AWS IAM identity webhook external-secrets external-secrets (Helm) -- -- Operator for syncing external secrets lldap app-template external <code>lldap.example.com</code> Lightweight LDAP directory"},{"location":"reference/app-catalog/#self-hosted","title":"Self-Hosted","text":"Name Chart/Image Gateway URL Description cryptgeon app-template external <code>cryptgeon.example.com</code> Encrypted secret sharing echo-server app-template external <code>echo.example.com</code> HTTP echo server for testing excalidraw app-template external <code>draw.example.com</code> Collaborative whiteboard glance app-template external <code>glance.example.com</code> Dashboard / start page homepage app-template external <code>home.example.com</code> Application dashboard house-hunter app-template external <code>house.example.com</code> Property search aggregator miniflux app-template external <code>rss.example.com</code> Minimalist RSS reader n8n app-template external <code>n8n.example.com</code> Workflow automation platform rrda app-template external <code>rrda.example.com</code> DNS lookup API sharkord app-template external <code>sharkord.example.com</code> Custom application tandoor app-template external <code>recipes.example.com</code> Recipe manager whoami app-template external <code>whoami.example.com</code> Simple HTTP request echo"},{"location":"reference/app-catalog/#system","title":"System","text":"Name Chart/Image Gateway URL Description intel-device-plugins Kustomize manifests -- -- Intel GPU device plugin and operator kubelet-csr-approver kubelet-csr-approver (Helm) -- -- Automatic CSR approval for kubelets node-feature-discovery node-feature-discovery (Helm) -- -- Hardware feature detection and labeling reloader reloader (Helm) -- -- Auto-restart pods on ConfigMap/Secret changes snapshot-controller snapshot-controller (Helm) -- -- Volume snapshot controller volsync volsync (Helm) -- -- Persistent volume replication and backup"},{"location":"reference/app-catalog/#gateway-distribution","title":"Gateway Distribution","text":"Gateway Count Services envoy-external ~26 Most user-facing apps envoy-internal ~2 Internal tools (speedtest, etc.) None ~32 Infrastructure, operators, system services"},{"location":"reference/app-catalog/#chart-distribution","title":"Chart Distribution","text":"Chart Type Count Description app-template (bjw-s) ~30 Standard application deployment Dedicated Helm chart ~15 Infrastructure components with their own charts Kustomize manifests ~15 CRDs, operators, and custom resources"},{"location":"reference/ip-allocation/","title":"IP Allocation","text":"<p>Complete IP address allocation and DNS record tables for the cluster.</p>"},{"location":"reference/ip-allocation/#node-addresses","title":"Node Addresses","text":"IP Address Hostname Role Architecture 192.168.0.200 -- Talos VIP (unused) Virtual IP 192.168.0.201 worker-01 Control Plane ARM64 (Pi 4) 192.168.0.202 worker-02 Control Plane ARM64 (Pi 4) 192.168.0.203 worker-03 Control Plane AMD64 192.168.0.204 worker-04 Worker (Intel) AMD64 192.168.0.211 worker-pi-01 Worker (Pi) ARM64 192.168.0.212 worker-pi-02 Worker (Pi) ARM64 192.168.0.213 worker-pi-03 Worker (Pi) ARM64"},{"location":"reference/ip-allocation/#loadbalancer-ip-pool","title":"LoadBalancer IP Pool","text":"<p>The Cilium LBIPAM pool allocates LoadBalancer service IPs from the range <code>192.168.0.220-239</code>.</p> IP Address Service Type 192.168.0.220-237 Available Cilium LBIPAM Pool 192.168.0.238 envoy-internal Gateway 192.168.0.239 envoy-external Gateway"},{"location":"reference/ip-allocation/#gateway-details","title":"Gateway Details","text":"Gateway IP Target Domain Purpose envoy-external 192.168.0.239 external.example.com Receives traffic from Cloudflare tunnel via nginx envoy-internal 192.168.0.238 internal.example.com Receives traffic from LAN and Tailscale VPN"},{"location":"reference/ip-allocation/#network-diagram","title":"Network Diagram","text":"<pre><code>flowchart TB\n    subgraph Internet\n        CF[Cloudflare&lt;br/&gt;*.example.com]\n    end\n\n    subgraph Router[\"Ubiquiti Router\"]\n        DHCP[DHCP / DNS]\n    end\n\n    subgraph Cluster[\"Cluster (192.168.0.0/24)\"]\n        subgraph ControlPlane[\"Control Plane\"]\n            CP1[192.168.0.201&lt;br/&gt;worker-01]\n            CP2[192.168.0.202&lt;br/&gt;worker-02]\n            CP3[192.168.0.203&lt;br/&gt;worker-03]\n        end\n        subgraph Workers[\"Workers\"]\n            W4[192.168.0.204&lt;br/&gt;worker-04]\n            WP1[192.168.0.211&lt;br/&gt;worker-pi-01]\n            WP2[192.168.0.212&lt;br/&gt;worker-pi-02]\n            WP3[192.168.0.213&lt;br/&gt;worker-pi-03]\n        end\n        subgraph LB[\"LoadBalancers (Cilium L2)\"]\n            EE[192.168.0.239&lt;br/&gt;envoy-external]\n            EI[192.168.0.238&lt;br/&gt;envoy-internal]\n        end\n    end\n\n    CF --&gt;|Tunnel| EE\n    DHCP --&gt; ControlPlane\n    DHCP --&gt; Workers</code></pre>"},{"location":"reference/ip-allocation/#dns-records","title":"DNS Records","text":""},{"location":"reference/ip-allocation/#cloudflare-managed-records","title":"Cloudflare-Managed Records","text":"Record Type Target Proxied Purpose <code>*.example.com</code> CNAME <code>external.example.com</code> Yes Wildcard for all services <code>external.example.com</code> CNAME <code>&lt;tunnel-id&gt;.cfargotunnel.com</code> Yes Cloudflare tunnel endpoint <code>internal.example.com</code> A <code>192.168.0.238</code> No Internal gateway"},{"location":"reference/ip-allocation/#traffic-flow-by-record","title":"Traffic Flow by Record","text":"<pre><code>flowchart LR\n    subgraph External[\"External Access\"]\n        W1[\"*.example.com\"] --&gt;|CNAME| E1[\"external.example.com\"]\n        E1 --&gt;|CNAME| T1[\"tunnel.cfargotunnel.com\"]\n        T1 --&gt;|Tunnel| N1[nginx]\n        N1 --&gt; EE1[envoy-external&lt;br/&gt;192.168.0.239]\n    end\n\n    subgraph Internal[\"Internal Access\"]\n        W3[\"internal.example.com\"] --&gt;|A record| EI1[envoy-internal&lt;br/&gt;192.168.0.238]\n    end</code></pre>"},{"location":"reference/ip-allocation/#address-space-summary","title":"Address Space Summary","text":"Range Purpose Count 192.168.0.200 Talos VIP 1 192.168.0.201-203 Control plane nodes 3 192.168.0.204 Intel/AMD worker 1 192.168.0.205-210 Reserved (future workers) 6 192.168.0.211-213 Raspberry Pi workers 3 192.168.0.214-219 Reserved (future Pi workers) 6 192.168.0.220-239 Cilium LBIPAM pool 20 <p>DNS Interception</p> <p>The Ubiquiti router intercepts all DNS traffic on port 53. To verify actual Cloudflare DNS records, use DNS over HTTPS (DoH): <pre><code>curl -sH 'accept: application/dns-json' \\\n  'https://cloudflare-dns.com/dns-query?name=echo.example.com&amp;type=A' | jq\n</code></pre></p>"},{"location":"security/","title":"Security","text":"<p>The cluster follows a defense-in-depth approach, layering multiple security controls from the network edge to individual application secrets. External traffic is filtered by Cloudflare before reaching the cluster, authenticated by Authelia SSO, and served over TLS certificates issued by cert-manager. Secrets are encrypted at rest in Git with SOPS and synced into the cluster via External Secrets Operator.</p>"},{"location":"security/#request-flow","title":"Request Flow","text":"<pre><code>flowchart LR\n    User((User)) --&gt;|HTTPS| CF[Cloudflare\\nWAF + DDoS Protection]\n    CF --&gt;|Tunnel| CFL[cloudflared]\n    CFL --&gt; EE[Envoy External\\nGateway]\n    EE --&gt;|Auth check| AUTH[Authelia\\nSSO Portal]\n    AUTH --&gt;|LDAP lookup| LLDAP[LLDAP\\nUser Directory]\n    AUTH --&gt;|Authenticated| APP[Application]\n    EE --&gt; APP</code></pre>"},{"location":"security/#secrets-flow","title":"Secrets Flow","text":"<pre><code>flowchart LR\n    OP[1Password Vault] --&gt;|Connect API| OPC[1Password Connect\\nIn-Cluster]\n    INF[Infisical\\nCloud] --&gt;|Universal Auth| ESO\n    OPC --&gt; ESO[External Secrets\\nOperator]\n    ESO --&gt;|Sync| KS[Kubernetes\\nSecrets]\n    KS --&gt; APP[Application\\nPods]\n\n    SOPS[SOPS + age\\nEncrypted in Git] --&gt;|Decrypt at apply| KS</code></pre>"},{"location":"security/#security-layers","title":"Security Layers","text":"Layer Technology Purpose Edge protection Cloudflare WAF DDoS mitigation, bot protection, web application firewall Transport Cloudflare Tunnel Encrypted tunnel without exposing ports to the internet TLS termination cert-manager + Let's Encrypt Automated wildcard certificates for <code>*.example.com</code> Authentication Authelia Single sign-on portal with OIDC provider capabilities User directory LLDAP Lightweight LDAP server for user and group management Secret management External Secrets + 1Password Automated sync from 1Password vault to Kubernetes secrets Secret management External Secrets + Infisical Automated sync from Infisical cloud to Kubernetes secrets Encryption at rest SOPS + age Git-committed secrets encrypted with age keys Network policy Cilium eBPF-based network policies for pod-to-pod traffic control"},{"location":"security/#components","title":"Components","text":"Component Namespace Description Authelia <code>security</code> SSO portal and OIDC provider LLDAP <code>security</code> Lightweight LDAP user directory External Secrets <code>security</code> Operator that syncs secrets from external stores 1Password Connect <code>security</code> In-cluster 1Password API server SOPS N/A (CLI tool) Encrypts secrets in the Git repository cert-manager <code>cert-manager</code> TLS certificate management via ACME"},{"location":"security/#sections","title":"Sections","text":"Page Description Authelia SSO portal, OIDC configuration, access control policies LLDAP Lightweight LDAP server, user and group management External Secrets 1Password Connect and Infisical backends, ClusterSecretStore patterns SOPS age encryption, <code>.sops.yaml</code> configuration, encrypt/decrypt workflows cert-manager Let's Encrypt ACME, DNS-01 challenge, ClusterIssuer configuration"},{"location":"security/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Two secret backends -- 1Password Connect serves application secrets from the the 1Password vault, while Infisical handles infrastructure secrets (Cloudflare tokens, Tailscale keys). This separation of concerns allows different access controls per category.</li> <li>SOPS for bootstrap secrets -- Secrets needed before External Secrets is running (e.g., 1Password Connect credentials, Infisical auth) are encrypted with SOPS and committed to Git.</li> <li>DNS-01 challenges -- cert-manager uses Cloudflare DNS-01 challenges instead of HTTP-01, enabling wildcard certificates and working behind the Cloudflare tunnel without exposing challenge endpoints.</li> <li>LDAP over embedded users -- LLDAP provides a central user directory that both Authelia and other services can query, avoiding duplicated user management.</li> </ul>"},{"location":"security/authelia/","title":"Authelia","text":"<p>Authelia is the single sign-on (SSO) portal for the cluster. It protects internal services behind authentication, provides an OpenID Connect (OIDC) identity provider for compatible applications, and integrates with LLDAP as its user directory backend.</p>"},{"location":"security/authelia/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    User((User)) --&gt;|HTTPS| AUTH[Authelia\\nauth.example.com]\n    AUTH --&gt;|LDAP bind| LLDAP[LLDAP\\nUser Directory]\n    AUTH --&gt;|Session cookie| APP[Protected\\nApplication]\n    AUTH --&gt;|OIDC tokens| OIDC[OIDC Clients\\nJellyfin, Grafana]\n\n    subgraph Secret Sources\n        OP[1Password\\nvia ExternalSecret]\n        INF[Infisical\\nvia ExternalSecret]\n    end\n\n    OP --&gt;|authelia-secret| AUTH\n    INF --&gt;|authelia-secrets| AUTH</code></pre>"},{"location":"security/authelia/#deployment","title":"Deployment","text":"<p>Authelia is deployed via the app-template Helm chart (v4.6.2) in the <code>security</code> namespace:</p> Setting Value Image <code>ghcr.io/authelia/authelia:4.39.15</code> Replicas 1 Listen port 8080 Metrics port 9000 Theme Dark URL <code>https://auth.example.com</code> <p>The application is exposed via the external Envoy gateway, making it accessible through the Cloudflare tunnel:</p> values.yaml (route section)<pre><code>route:\n  app:\n    enabled: true\n    hostnames:\n      - auth.example.com\n    parentRefs:\n      - name: envoy-external\n        namespace: networking\n        sectionName: https\n</code></pre>"},{"location":"security/authelia/#ldap-backend","title":"LDAP Backend","text":"<p>Authelia authenticates users against the LLDAP server running in the same namespace:</p> configuration.yaml (authentication_backend section)<pre><code>authentication_backend:\n  ldap:\n    address: ldap://lldap.security.svc.cluster.local:5389\n    implementation: custom\n    timeout: 5s\n    start_tls: false\n    base_dn: dc=example,dc=com\n    additional_users_dn: ou=people\n    users_filter: \"(&amp;({username_attribute}={input})(objectClass=person))\"\n    additional_groups_dn: ou=groups\n    groups_filter: \"(member={dn})\"\n    attributes:\n      username: uid\n      display_name: displayName\n      mail: mail\n      member_of: memberOf\n      group_name: cn\n</code></pre> <p>The LDAP bind credentials (<code>LLDAP_BIND_USER_DN</code> and <code>LLDAP_BIND_PASSWORD</code>) are injected as environment variables from Infisical via an ExternalSecret.</p>"},{"location":"security/authelia/#access-control","title":"Access Control","text":"<p>The default access control policy is <code>one_factor</code>, meaning all protected services require at least a username and password:</p> configuration.yaml (access_control section)<pre><code>access_control:\n  default_policy: one_factor\n  rules:\n    - domain:\n        - actual.example.com\n      subject:\n        - [\"group:auth_users\"]\n      policy: one_factor\n    - domain:\n        - jellyfin.example.com\n      subject:\n        - [\"group:auth_users\"]\n      policy: one_factor\n</code></pre> <p>Policy levels</p> <p>Authelia supports three policy levels:</p> <ul> <li><code>bypass</code> -- No authentication required</li> <li><code>one_factor</code> -- Username and password</li> <li><code>two_factor</code> -- Username, password, and a second factor (TOTP, WebAuthn, or Duo)</li> </ul>"},{"location":"security/authelia/#oidc-provider","title":"OIDC Provider","text":"<p>Authelia acts as an OpenID Connect identity provider, allowing applications to delegate authentication:</p>"},{"location":"security/authelia/#configured-clients","title":"Configured Clients","text":"Client Client ID Redirect URI Scopes Jellyfin (from 1Password) <code>https://jellyfin.example.com/sso/OID/redirect/authelia</code> openid, profile, groups Grafana <code>grafana</code> <code>https://grafana.example.com/login/generic_oauth</code> openid, profile, groups, email <p>Both clients use PKCE with S256 challenge method for enhanced security.</p> <p>Adding a new OIDC client</p> <p>To add a new application as an OIDC client:</p> <ol> <li>Generate a client ID and secret, store them in 1Password</li> <li>Add the client configuration to <code>authelia/config/configuration.yaml</code> under <code>identity_providers.oidc.clients</code></li> <li>Configure the application to use Authelia as its OIDC provider with the redirect URI</li> </ol>"},{"location":"security/authelia/#session-configuration","title":"Session Configuration","text":"configuration.yaml (session section)<pre><code>session:\n  same_site: lax\n  inactivity: 5m\n  expiration: 1h\n  remember_me: 1M\n  cookies:\n    - name: session\n      domain: example.com\n      authelia_url: https://auth.example.com\n      default_redirection_url: https://example.com\n</code></pre> <p>The session cookie is scoped to the <code>example.com</code> domain, enabling SSO across all subdomains.</p>"},{"location":"security/authelia/#storage","title":"Storage","text":"<p>Authelia uses a local SQLite database for session and configuration storage:</p> <pre><code>storage:\n  local:\n    path: \"/config/db.sqlite3\"\n</code></pre> <p>The database is persisted via a PVC to survive pod restarts.</p>"},{"location":"security/authelia/#secrets-management","title":"Secrets Management","text":"<p>Authelia receives secrets from two external sources:</p> Secret Source Contains <code>authelia-secret</code> 1Password (via ExternalSecret) OIDC keys, session secret, storage encryption key, JWT secret, client secrets <code>authelia-secrets</code> Infisical (via ExternalSecret) LLDAP bind credentials <p>The Authelia configuration file itself is rendered as a template by External Secrets, with secret values injected at sync time using <code>templateFrom</code>:</p> externalsecret.yaml (template rendering)<pre><code>spec:\n  target:\n    name: authelia-secret\n    template:\n      templateFrom:\n        - configMap:\n            name: authelia-configmap\n            items:\n              - key: configuration.yaml\n                templateAs: Values\n          target: Data\n</code></pre> <p>Reloader integration</p> <p>The deployment has <code>reloader.stakater.com/auto: \"true\"</code> annotation, so it automatically restarts when the referenced secrets or configmaps change.</p>"},{"location":"security/authelia/#monitoring","title":"Monitoring","text":"<p>Authelia exposes Prometheus metrics on port 9000, scraped by a ServiceMonitor:</p> <pre><code>serviceMonitor:\n  app:\n    serviceName: authelia\n    endpoints:\n      - port: metrics\n        scheme: http\n        path: /metrics\n        interval: 1m\n</code></pre>"},{"location":"security/cert-manager/","title":"cert-manager","text":"<p>cert-manager automates TLS certificate management for the cluster. It obtains certificates from Let's Encrypt using ACME DNS-01 challenges via Cloudflare, enabling wildcard certificates for <code>*.example.com</code> without exposing any HTTP challenge endpoints.</p>"},{"location":"security/cert-manager/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    CM[cert-manager] --&gt;|ACME DNS-01| LE[Let's Encrypt]\n    CM --&gt;|Create TXT record| CF[Cloudflare DNS\\nexample.com]\n    LE --&gt;|Verify TXT record| CF\n    LE --&gt;|Issue certificate| CM\n    CM --&gt;|Store| SEC[Kubernetes Secret\\nTLS cert + key]\n    SEC --&gt; GW[Envoy Gateway\\nTLS termination]</code></pre>"},{"location":"security/cert-manager/#deployment","title":"Deployment","text":"<p>cert-manager is deployed via its official Helm chart in the <code>cert-manager</code> namespace:</p> cert-manager/values.yaml<pre><code>global:\n  leaderElection:\n    namespace: cert-manager\ncrds:\n  enabled: true\ndns01RecursiveNameservers: https://1.1.1.1:443/dns-query,https://1.0.0.1:443/dns-query\ndns01RecursiveNameserversOnly: true\nprometheus:\n  enabled: true\n  servicemonitor:\n    enabled: true\n</code></pre>"},{"location":"security/cert-manager/#key-configuration","title":"Key Configuration","text":"Setting Value Purpose <code>dns01RecursiveNameservers</code> Cloudflare DoH Bypasses local DNS interception for ACME verification <code>dns01RecursiveNameserversOnly</code> <code>true</code> Forces cert-manager to use only the specified resolvers <code>crds.enabled</code> <code>true</code> CRDs managed by the Helm chart <p>Why DoH nameservers?</p> <p>The Ubiquiti router intercepts DNS traffic on port 53. By configuring cert-manager to use Cloudflare's DNS-over-HTTPS endpoints (<code>1.1.1.1:443/dns-query</code>), DNS-01 challenge verification queries bypass the local DNS interception and reach Cloudflare directly.</p>"},{"location":"security/cert-manager/#clusterissuers","title":"ClusterIssuers","text":"<p>Two ClusterIssuers are configured -- production and staging:</p>"},{"location":"security/cert-manager/#production","title":"Production","text":"issuers/issuers.yaml<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-production\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: sam.wibrow.wa@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt-production\n    solvers:\n      - dns01:\n          cloudflare:\n            apiTokenSecretRef:\n              name: cert-manager-secret\n              key: api-token\n        selector:\n          dnsZones:\n            - \"example.com\"\n</code></pre>"},{"location":"security/cert-manager/#staging","title":"Staging","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    email: sam.wibrow.wa@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    solvers:\n      - dns01:\n          cloudflare:\n            apiTokenSecretRef:\n              name: cert-manager-secret\n              key: api-token\n        selector:\n          dnsZones:\n            - \"example.com\"\n</code></pre> <p>Use staging first</p> <p>When testing new certificate configurations, use <code>letsencrypt-staging</code> to avoid hitting Let's Encrypt production rate limits. Staging certificates are not trusted by browsers but validate the entire ACME flow.</p>"},{"location":"security/cert-manager/#cloudflare-api-token","title":"Cloudflare API Token","text":"<p>The Cloudflare API token used for DNS-01 challenges is synced from Infisical via an ExternalSecret:</p> issuers/externalsecret.yaml<pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: cert-manager-secret\nspec:\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: infisical-cert-manager\n  target:\n    name: cert-manager-secret\n  dataFrom:\n    - find:\n        name:\n          regexp: .*\n</code></pre> <p>The token requires <code>Zone:DNS:Edit</code> permissions for the <code>example.com</code> zone in Cloudflare.</p>"},{"location":"security/cert-manager/#how-dns-01-challenge-works","title":"How DNS-01 Challenge Works","text":"<pre><code>sequenceDiagram\n    participant CM as cert-manager\n    participant LE as Let's Encrypt\n    participant CF as Cloudflare DNS\n\n    CM-&gt;&gt;LE: Request certificate for *.example.com\n    LE--&gt;&gt;CM: Return challenge token\n    CM-&gt;&gt;CF: Create TXT record _acme-challenge.example.com\n    CM-&gt;&gt;LE: Notify challenge is ready\n    LE-&gt;&gt;CF: Query TXT record\n    CF--&gt;&gt;LE: Return challenge token\n    LE--&gt;&gt;CM: Issue certificate\n    CM-&gt;&gt;CF: Delete TXT record\n    CM-&gt;&gt;CM: Store cert in Kubernetes Secret</code></pre> <p>The DNS-01 challenge method:</p> <ol> <li>Proves domain ownership by creating a DNS TXT record</li> <li>Supports wildcards -- unlike HTTP-01, DNS-01 can issue <code>*.example.com</code> certificates</li> <li>Works behind tunnels -- no need to expose port 80 or 443 for challenge verification</li> </ol>"},{"location":"security/cert-manager/#requesting-a-certificate","title":"Requesting a Certificate","text":""},{"location":"security/cert-manager/#wildcard-certificate","title":"Wildcard Certificate","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: wildcard-cert\n  namespace: networking\nspec:\n  secretName: wildcard-cert-tls\n  issuerRef:\n    name: letsencrypt-production\n    kind: ClusterIssuer\n  dnsNames:\n    - \"example.com\"\n    - \"*.example.com\"\n</code></pre>"},{"location":"security/cert-manager/#single-domain-certificate","title":"Single-Domain Certificate","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: my-app-cert\n  namespace: my-app\nspec:\n  secretName: my-app-tls\n  issuerRef:\n    name: letsencrypt-production\n    kind: ClusterIssuer\n  dnsNames:\n    - \"my-app.example.com\"\n</code></pre>"},{"location":"security/cert-manager/#internal-pki","title":"Internal PKI","text":"<p>In addition to Let's Encrypt certificates, cert-manager manages internal self-signed certificates for cluster components. For example, the Snapshot Controller webhook uses a self-signed CA chain:</p> <pre><code># Self-signed root issuer\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: snapshot-controller-webhook-selfsign\nspec:\n  selfSigned: {}\n\n# CA certificate (5-year duration)\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: snapshot-controller-webhook-ca\nspec:\n  secretName: snapshot-controller-webhook-ca\n  duration: 43800h\n  isCA: true\n  issuerRef:\n    name: snapshot-controller-webhook-selfsign\n\n# CA issuer for signing webhook certs\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: snapshot-controller-webhook-ca\nspec:\n  ca:\n    secretName: snapshot-controller-webhook-ca\n</code></pre>"},{"location":"security/cert-manager/#troubleshooting","title":"Troubleshooting","text":""},{"location":"security/cert-manager/#check-certificate-status","title":"Check Certificate Status","text":"<pre><code># List all certificates and their status\nkubectl get certificates -A\n\n# Describe a specific certificate for detailed status\nkubectl describe certificate wildcard-cert -n networking\n\n# Check certificate requests\nkubectl get certificaterequests -A\n\n# Check ACME orders and challenges\nkubectl get orders -A\nkubectl get challenges -A\n</code></pre>"},{"location":"security/cert-manager/#common-issues","title":"Common Issues","text":"Symptom Likely Cause Fix Challenge stuck in <code>pending</code> Cloudflare API token lacks <code>Zone:DNS:Edit</code> Verify token permissions in Cloudflare dashboard Challenge fails DNS propagation Local DNS interception Verify <code>dns01RecursiveNameserversOnly: true</code> is set Rate limit hit Too many production certificate requests Use <code>letsencrypt-staging</code> for testing, wait for rate limit to reset Certificate not renewing cert-manager pod not running Check <code>cert-manager</code> namespace for pod health <p>Automatic renewal</p> <p>cert-manager automatically renews certificates before they expire (default: 30 days before expiry). No manual intervention is needed for routine renewals.</p>"},{"location":"security/cert-manager/#monitoring","title":"Monitoring","text":"<p>cert-manager exports Prometheus metrics and is scraped via a ServiceMonitor:</p> <pre><code>prometheus:\n  enabled: true\n  servicemonitor:\n    enabled: true\n</code></pre> <p>Key metrics to monitor:</p> <ul> <li><code>certmanager_certificate_expiration_timestamp_seconds</code> -- time until certificate expiry</li> <li><code>certmanager_certificate_ready_status</code> -- whether certificates are in ready state</li> <li><code>certmanager_http_acme_client_request_count</code> -- ACME API call volume</li> </ul>"},{"location":"security/external-secrets/","title":"External Secrets","text":"<p>External Secrets Operator (ESO) syncs secrets from external providers into Kubernetes <code>Secret</code> resources. The cluster uses two secret backends -- 1Password Connect for application secrets and Infisical for infrastructure secrets -- connected through <code>ClusterSecretStore</code> resources.</p>"},{"location":"security/external-secrets/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    subgraph External Providers\n        OP[1Password Vault]\n        INF[Infisical\\nhome-lab project]\n    end\n\n    subgraph Cluster - security namespace\n        OPC[1Password Connect\\nIn-Cluster API]\n        CSS1[ClusterSecretStore\\nonepassword-connect]\n        CSS2[ClusterSecretStore\\ninfisical]\n        CSS3[ClusterSecretStore\\ninfisical-cert-manager]\n        CSS4[ClusterSecretStore\\ninfisical-networking-*]\n    end\n\n    subgraph Application Namespaces\n        ES1[ExternalSecret] --&gt; KS1[Kubernetes Secret]\n        ES2[ExternalSecret] --&gt; KS2[Kubernetes Secret]\n    end\n\n    OP --&gt;|Connect API| OPC\n    OPC --&gt; CSS1\n    INF --&gt;|Universal Auth| CSS2\n    INF --&gt; CSS3\n    INF --&gt; CSS4\n    CSS1 --&gt; ES1\n    CSS2 --&gt; ES2</code></pre>"},{"location":"security/external-secrets/#components","title":"Components","text":""},{"location":"security/external-secrets/#external-secrets-operator","title":"External Secrets Operator","text":"<p>The operator is deployed via Helm in the <code>security</code> namespace:</p> operator/values.yaml<pre><code>installCRDs: true\nreplicaCount: 1\nleaderElect: true\ngrafana:\n  enabled: true\nserviceMonitor:\n  enabled: true\n  interval: 1m\n</code></pre> <p>All sub-controllers (background, cleanup, reports) have ServiceMonitors enabled for Prometheus scraping.</p>"},{"location":"security/external-secrets/#1password-connect","title":"1Password Connect","text":"<p>1Password Connect runs as an in-cluster API server that provides authenticated access to the 1Password vault:</p> 1password-connect/values.yaml<pre><code>connect:\n  create: true\noperator:\n  create: false\n</code></pre> <p>It is deployed via the official 1Password Connect Helm chart (v2.2.1). The connect server credentials (<code>1password-credentials.json</code> and token) are stored as SOPS-encrypted secrets in the repository.</p> <p>Why in-cluster?</p> <p>Running 1Password Connect inside the cluster avoids external API calls for every secret sync. The connect server caches vault data locally and serves it to External Secrets over the cluster network.</p>"},{"location":"security/external-secrets/#clustersecretstores","title":"ClusterSecretStores","text":""},{"location":"security/external-secrets/#1password-store","title":"1Password Store","text":"<p>The <code>onepassword-connect</code> ClusterSecretStore connects to the in-cluster 1Password Connect server:</p> stores/onepassword/clustersecretstore.yaml<pre><code>apiVersion: external-secrets.io/v1\nkind: ClusterSecretStore\nmetadata:\n  name: onepassword-connect\nspec:\n  provider:\n    onepassword:\n      connectHost: http://onepassword-connect:8080\n      vaults:\n        pitower: 1\n      auth:\n        secretRef:\n          connectTokenSecretRef:\n            name: onepassword-connect-token\n            key: token\n            namespace: security\n</code></pre>"},{"location":"security/external-secrets/#infisical-stores","title":"Infisical Stores","text":"<p>Multiple Infisical ClusterSecretStores provide scoped access to different secret paths:</p> Store Name Secrets Path Used By <code>infisical</code> <code>/</code> (root, recursive) General application secrets <code>infisical-cert-manager</code> <code>/cert-manager</code> Cloudflare API token for DNS-01 challenges <code>infisical-networking-cloudflared</code> <code>/networking/cloudflared</code> Cloudflare tunnel credentials <code>infisical-networking-external-dns</code> <code>/networking/external-dns</code> External DNS Cloudflare token <code>infisical-networking-tailscale</code> <code>/networking/tailscale</code> Tailscale auth key <p>All Infisical stores authenticate using Universal Auth credentials stored in the <code>universal-auth-credentials</code> secret (SOPS-encrypted in Git):</p> stores/infisical/clustersecretstore.yaml (example)<pre><code>apiVersion: external-secrets.io/v1\nkind: ClusterSecretStore\nmetadata:\n  name: infisical\nspec:\n  provider:\n    infisical:\n      hostAPI: https://eu.infisical.com\n      auth:\n        universalAuthCredentials:\n          clientId:\n            key: clientId\n            namespace: security\n            name: universal-auth-credentials\n          clientSecret:\n            key: clientSecret\n            namespace: security\n            name: universal-auth-credentials\n      secretsScope:\n        projectSlug: home-lab-iwi-y\n        environmentSlug: prod\n        secretsPath: /\n        recursive: true\n</code></pre>"},{"location":"security/external-secrets/#externalsecret-pattern","title":"ExternalSecret Pattern","text":"<p>Applications reference a <code>ClusterSecretStore</code> to sync secrets into their namespace. Here is the typical pattern:</p>"},{"location":"security/external-secrets/#simple-key-extraction","title":"Simple Key Extraction","text":"<p>Pull all fields from a 1Password item into a Kubernetes secret:</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: my-app\n  namespace: my-app\nspec:\n  refreshInterval: 5m\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: onepassword-connect\n  target:\n    name: my-app-secret\n    creationPolicy: Owner\n  dataFrom:\n    - extract:\n        key: my-app  # 1Password item name\n</code></pre>"},{"location":"security/external-secrets/#templated-secrets","title":"Templated Secrets","text":"<p>Combine external secret values with templates to produce configuration files:</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: my-app-config\nspec:\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: onepassword-connect\n  target:\n    name: my-app-config\n    template:\n      data:\n        config.yaml: |\n          database_url: postgres://{{ .db_user }}:{{ .db_pass }}@db:5432/myapp\n          api_key: {{ .api_key }}\n  dataFrom:\n    - extract:\n        key: my-app\n</code></pre>"},{"location":"security/external-secrets/#rewriting-keys","title":"Rewriting Keys","text":"<p>Prefix keys from external sources to avoid naming collisions:</p> <pre><code>dataFrom:\n  - extract:\n      key: my-app\n    rewrite:\n      - regexp:\n          source: \"(.*)\"\n          target: \"myapp_$1\"\n</code></pre>"},{"location":"security/external-secrets/#secret-lifecycle","title":"Secret Lifecycle","text":"<pre><code>sequenceDiagram\n    participant OP as 1Password / Infisical\n    participant ESO as External Secrets Operator\n    participant KS as Kubernetes Secret\n    participant Pod as Application Pod\n    participant RL as Reloader\n\n    ESO-&gt;&gt;OP: Poll for changes (refreshInterval)\n    OP--&gt;&gt;ESO: Return secret data\n    ESO-&gt;&gt;KS: Create/Update Secret\n    RL-&gt;&gt;KS: Detect change\n    RL-&gt;&gt;Pod: Trigger rolling restart\n    Pod-&gt;&gt;KS: Mount updated secret</code></pre> <p>Refresh intervals</p> <p>Most ExternalSecrets use a <code>refreshInterval</code> of <code>5m</code>. This means changes in 1Password or Infisical propagate to the cluster within 5 minutes. For critical secrets, this can be reduced to <code>1m</code>.</p>"},{"location":"security/external-secrets/#monitoring","title":"Monitoring","text":"<p>The operator exposes Grafana dashboards and Prometheus metrics:</p> <ul> <li>ServiceMonitor on the main operator and all sub-controllers</li> <li>Grafana dashboard auto-provisioned (<code>grafana.enabled: true</code>)</li> <li>Alerts can be configured based on <code>externalsecret_sync_calls_error</code> and <code>externalsecret_status_condition</code> metrics</li> </ul>"},{"location":"security/lldap/","title":"LLDAP","text":"<p>LLDAP is a lightweight LDAP server that provides the user directory for the cluster. It serves as the authentication backend for Authelia and any other service that needs to look up users and groups.</p>"},{"location":"security/lldap/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    AUTH[Authelia\\nSSO Portal] --&gt;|LDAP bind\\nport 5389| LLDAP[LLDAP\\n2 replicas]\n    LLDAP --&gt;|User/group data| PG[(PostgreSQL\\nCloudNativePG)]\n    ADMIN((Admin)) --&gt;|HTTPS\\nlldap.example.com| LLDAP\n    OP[1Password\\nvia ExternalSecret] --&gt;|JWT, password, key seed| LLDAP\n    INF[Infisical\\nvia ClusterSecretStore] --&gt;|DB credentials| LLDAP</code></pre>"},{"location":"security/lldap/#deployment","title":"Deployment","text":"<p>LLDAP is deployed via the app-template Helm chart (v4.6.2) in the <code>security</code> namespace:</p> Setting Value Image <code>ghcr.io/lldap/lldap:v0.6.2</code> Replicas 2 Strategy RollingUpdate HTTP port 8080 LDAP port 5389 Base DN <code>dc=example,dc=com</code> Timezone <code>Europe/Zurich</code>"},{"location":"security/lldap/#high-availability","title":"High Availability","text":"<p>LLDAP runs with 2 replicas and a topology spread constraint to ensure pods land on different nodes:</p> <pre><code>defaultPodOptions:\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: kubernetes.io/hostname\n      whenUnsatisfiable: DoNotSchedule\n      labelSelector:\n        matchLabels:\n          app.kubernetes.io/name: lldap\n</code></pre>"},{"location":"security/lldap/#network-access","title":"Network Access","text":"<p>LLDAP is exposed in two ways:</p>"},{"location":"security/lldap/#loadbalancer-service","title":"LoadBalancer Service","text":"<p>A Cilium L2-announced LoadBalancer provides direct LDAP access on the network:</p> <pre><code>service:\n  app:\n    type: LoadBalancer\n    annotations:\n      external-dns.alpha.kubernetes.io/hostname: lldap.example.com\n      lbipam.cilium.io/ips: \"192.168.0.222\"\n    ports:\n      http:\n        port: 8080\n      ldap:\n        port: 5389\n</code></pre> Protocol Address Port Purpose LDAP 192.168.0.222 5389 LDAP queries from Authelia and other services HTTP 192.168.0.222 8080 Web admin UI"},{"location":"security/lldap/#internal-gateway","title":"Internal Gateway","text":"<p>The web admin interface is also available via the internal Envoy gateway at <code>https://lldap.example.com</code>:</p> <pre><code>route:\n  app:\n    hostnames:\n      - lldap.example.com\n    parentRefs:\n      - name: envoy-internal\n        namespace: networking\n        sectionName: https\n</code></pre>"},{"location":"security/lldap/#database","title":"Database","text":"<p>LLDAP uses PostgreSQL (via CloudNativePG) as its persistent backend, replacing the default SQLite:</p> <pre><code>LLDAP_DATABASE_URL: postgres://user:password@host.cloudnative-pg.svc.cluster.local/lldap\n</code></pre> <p>Database credentials are sourced from the <code>cnpg-secrets</code> ClusterSecretStore via an ExternalSecret:</p> externalsecret.yaml (database credentials)<pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: lldap-db\nspec:\n  secretStoreRef:\n    name: cnpg-secrets\n    kind: ClusterSecretStore\n  target:\n    name: lldap-db\n    template:\n      data:\n        LLDAP_DATABASE_URL: &gt;-\n          postgres://{{ index . \"user\" }}:{{ index . \"password\" }}@\n          {{ index . \"host\" }}.cloudnative-pg.svc.cluster.local/lldap\n  dataFrom:\n    - extract:\n        key: lldap-superuser\n</code></pre>"},{"location":"security/lldap/#ldap-schema","title":"LDAP Schema","text":"<p>LLDAP uses a simplified LDAP schema with the following structure:</p> <pre><code>dc=example,dc=com\n\u251c\u2500\u2500 ou=people          # User entries\n\u2502   \u251c\u2500\u2500 uid=admin\n\u2502   \u251c\u2500\u2500 uid=user1\n\u2502   \u2514\u2500\u2500 uid=user2\n\u2514\u2500\u2500 ou=groups          # Group entries\n    \u251c\u2500\u2500 cn=auth_users\n    \u2514\u2500\u2500 cn=admin_group\n</code></pre>"},{"location":"security/lldap/#key-attributes","title":"Key Attributes","text":"Attribute Description <code>uid</code> Username (login identifier) <code>displayName</code> Full display name <code>mail</code> Email address <code>memberOf</code> Groups the user belongs to <code>cn</code> Group common name"},{"location":"security/lldap/#secrets","title":"Secrets","text":"<p>LLDAP receives secrets from two sources:</p> Secret Source Contains <code>lldap-secret</code> 1Password (via ExternalSecret) JWT secret, admin password, server key seed <code>lldap-db</code> CloudNativePG (via ClusterSecretStore) PostgreSQL connection string externalsecret.yaml (application secrets)<pre><code>spec:\n  target:\n    name: lldap-secret\n    template:\n      data:\n        LLDAP_JWT_SECRET: \"{{ .lldap_jwt_secret }}\"\n        LLDAP_LDAP_USER_PASS: \"{{ .lldap_password }}\"\n        LLDAP_KEY_SEED: \"{{ .lldap_server_key_seed }}\"\n  dataFrom:\n    - extract:\n        key: lldap\n</code></pre>"},{"location":"security/lldap/#integration-with-authelia","title":"Integration with Authelia","text":"<p>Authelia connects to LLDAP using the following configuration (see Authelia docs for full details):</p> Setting Value Address <code>ldap://lldap.security.svc.cluster.local:5389</code> Base DN <code>dc=example,dc=com</code> Users DN <code>ou=people</code> Groups DN <code>ou=groups</code> Users filter <code>(&amp;({username_attribute}={input})(objectClass=person))</code> Groups filter <code>(member={dn})</code>"},{"location":"security/lldap/#administration","title":"Administration","text":"<p>Access the LLDAP web UI at <code>https://lldap.example.com</code> to manage users and groups.</p> <p>Common tasks</p> <ul> <li>Add a user: Navigate to Users, click Create, fill in username/email/display name, set a password</li> <li>Add to group: Navigate to Groups, select the group, add the user as a member</li> <li>Reset password: Select the user, use the password reset option (or set <code>LLDAP_FORCE_LDAP_USER_PASS_RESET: \"true\"</code> to force reset on next login)</li> </ul>"},{"location":"security/sops/","title":"SOPS","text":"<p>SOPS (Secrets OPerationS) encrypts sensitive values in YAML files so they can be safely committed to Git. The cluster uses SOPS with age as the encryption backend, ensuring that secrets like 1Password credentials, Infisical auth tokens, and Talos machine secrets are protected at rest in the repository.</p>"},{"location":"security/sops/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    PLAIN[Plaintext YAML\\n*.sops.yaml] --&gt;|sops --encrypt| ENC[Encrypted YAML\\nValues encrypted\\nKeys in cleartext]\n    ENC --&gt;|git commit + push| GIT[(Git Repository)]\n    GIT --&gt;|git pull| LOCAL[Local Clone]\n    LOCAL --&gt;|sops --decrypt| PLAIN2[Plaintext YAML]\n\n    AGE[age private key\\n~/.config/sops/age/keys.txt] -.-&gt;|Used by| ENC\n    AGE -.-&gt;|Used by| PLAIN2</code></pre> <p>SOPS encrypts only the values in YAML files (controlled by <code>encrypted_regex</code>), leaving keys and structure visible. This allows diffs to show which fields changed without revealing the actual secret data.</p>"},{"location":"security/sops/#configuration","title":"Configuration","text":"<p>The <code>.sops.yaml</code> file at the repository root defines encryption rules for different file paths:</p> .sops.yaml<pre><code>---\ncreation_rules:\n  - path_regex: kubernetes/pitower/.*\\.sops\\.ya?ml\n    encrypted_regex: \"^(data|stringData)$\"\n    key_groups:\n      - age:\n          - \"age1tkaddc3hgjx0eagjl6mqpxvzzkerd44e34rua6gzucv6emr5f5fs4mlu67\"\n  - path_regex: pitower/talos/.*\\.sops\\.ya?ml\n    encrypted_regex: \"^(crt|id|token|key|secret|stringData|secretboxencryptionsecret|bootstraptoken)$\"\n    key_groups:\n      - age:\n          - \"age1tkaddc3hgjx0eagjl6mqpxvzzkerd44e34rua6gzucv6emr5f5fs4mlu67\"\n  - path_regex: /dev/stdin\n    key_groups:\n      - age:\n          - \"age1tkaddc3hgjx0eagjl6mqpxvzzkerd44e34rua6gzucv6emr5f5fs4mlu67\"\n  - path_regex: .*\\.sops\\.ya?ml$\n    key_groups:\n      - age:\n          - \"age1tkaddc3hgjx0eagjl6mqpxvzzkerd44e34rua6gzucv6emr5f5fs4mlu67\"\n</code></pre>"},{"location":"security/sops/#rule-breakdown","title":"Rule Breakdown","text":"Path Pattern <code>encrypted_regex</code> Purpose <code>kubernetes/pitower/.*\\.sops\\.ya?ml</code> <code>^(data\\|stringData)$</code> Kubernetes Secret manifests -- encrypts only <code>data</code> and <code>stringData</code> fields <code>pitower/talos/.*\\.sops\\.ya?ml</code> <code>^(crt\\|id\\|token\\|key\\|secret\\|...)$</code> Talos machine secrets -- encrypts certificates, tokens, keys, and bootstrap data <code>/dev/stdin</code> (all values) Piped input for ad-hoc encryption <code>.*\\.sops\\.ya?ml$</code> (all values) Catch-all for any other <code>.sops.yaml</code> files <p>File naming convention</p> <p>SOPS-encrypted files must use the <code>.sops.yaml</code> or <code>.sops.yml</code> extension. This convention makes it immediately clear which files contain encrypted data and ensures the correct <code>.sops.yaml</code> rules are matched.</p>"},{"location":"security/sops/#age-encryption","title":"age Encryption","text":"<p>The cluster uses a single age public key for encryption:</p> <pre><code>age1tkaddc3hgjx0eagjl6mqpxvzzkerd44e34rua6gzucv6emr5f5fs4mlu67\n</code></pre> <p>The corresponding private key must be available locally for decryption:</p> <pre><code>export SOPS_AGE_KEY_FILE=~/.config/sops/age/keys.txt\n</code></pre> <p>Protect the private key</p> <p>The age private key is the master key for all encrypted secrets in the repository. It must never be committed to Git. Store it securely and ensure it is backed up.</p>"},{"location":"security/sops/#encrypt-decrypt-workflow","title":"Encrypt / Decrypt Workflow","text":""},{"location":"security/sops/#encrypting-a-new-secret","title":"Encrypting a New Secret","text":"<ol> <li> <p>Create the plaintext file with the <code>.sops.yaml</code> extension:</p> my-secret.sops.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\n  namespace: my-app\nstringData:\n  api-key: \"super-secret-value\"\n  password: \"another-secret\"\n</code></pre> </li> <li> <p>Encrypt the file in place:</p> <pre><code>sops --encrypt --in-place my-secret.sops.yaml\n</code></pre> </li> <li> <p>Verify -- the file now has encrypted values but readable keys:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\n  namespace: my-app\nstringData:\n  api-key: ENC[AES256_GCM,data:...,type:str]\n  password: ENC[AES256_GCM,data:...,type:str]\nsops:\n  age:\n    - recipient: age1tkaddc3hgjx0eagjl6mqpxvzzkerd44e34rua6gzucv6emr5f5fs4mlu67\n      enc: |\n        -----BEGIN AGE ENCRYPTED FILE-----\n        ...\n        -----END AGE ENCRYPTED FILE-----\n  lastmodified: \"2025-01-01T00:00:00Z\"\n  mac: ENC[AES256_GCM,data:...,type:str]\n  version: 3.9.0\n</code></pre> </li> <li> <p>Commit the encrypted file to Git.</p> </li> </ol>"},{"location":"security/sops/#decrypting-a-secret","title":"Decrypting a Secret","text":"<pre><code>sops --decrypt my-secret.sops.yaml\n</code></pre>"},{"location":"security/sops/#editing-an-encrypted-secret","title":"Editing an Encrypted Secret","text":"<p>SOPS can open encrypted files in your editor, decrypting on open and re-encrypting on save:</p> <pre><code>sops my-secret.sops.yaml\n</code></pre> <p>This opens the file in <code>$EDITOR</code> with plaintext values. When you save and close, SOPS re-encrypts the changed values automatically.</p>"},{"location":"security/sops/#talos-secrets","title":"Talos Secrets","text":"<p>Talos machine secrets (certificates, tokens, encryption keys) are stored as SOPS-encrypted files under <code>pitower/talos/</code>. The <code>encrypted_regex</code> for this path is broader than for Kubernetes secrets, covering fields specific to Talos:</p> <ul> <li><code>crt</code> -- TLS certificates</li> <li><code>id</code> -- Machine identifiers</li> <li><code>token</code> -- Bootstrap and join tokens</li> <li><code>key</code> -- Private keys</li> <li><code>secret</code> -- General secrets</li> <li><code>secretboxencryptionsecret</code> -- Kubernetes secret encryption key</li> <li><code>bootstraptoken</code> -- Cluster bootstrap token</li> </ul> <pre><code># Decrypt Talos secrets for inspection\nsops --decrypt pitower/talos/secrets.sops.yaml\n\n# Edit Talos secrets\nsops pitower/talos/secrets.sops.yaml\n</code></pre>"},{"location":"security/sops/#integration-with-argocd","title":"Integration with ArgoCD","text":"<p>ArgoCD does not natively decrypt SOPS files. Instead, bootstrap secrets (like 1Password Connect credentials and Infisical auth) are:</p> <ol> <li>Encrypted with SOPS and committed to Git</li> <li>Decrypted locally and applied directly with <code>kubectl</code> during cluster bootstrap</li> <li>Referenced by External Secrets Operator, which then handles syncing all other secrets</li> </ol> <p>This means SOPS is used primarily for bootstrap secrets -- the minimal set of credentials needed to start External Secrets Operator, which then takes over secret management for all applications.</p>"},{"location":"security/sops/#best-practices","title":"Best Practices","text":"<p>SOPS best practices</p> <ul> <li>Always use <code>encrypted_regex</code> -- encrypt only the fields that contain sensitive data, not the entire file. This makes diffs meaningful and review easier.</li> <li>Use the <code>.sops.yaml</code> extension -- this ensures SOPS rules are applied correctly and makes encrypted files easy to identify.</li> <li>Never commit the age private key -- store it in a password manager or secure vault, separate from the repository.</li> <li>Rotate secrets periodically -- when rotating the age key, re-encrypt all SOPS files with the new key and update <code>.sops.yaml</code>.</li> <li>Verify before committing -- run <code>sops --decrypt</code> on the file to ensure it was encrypted correctly before pushing.</li> </ul>"},{"location":"storage/","title":"Storage","text":"<p>The cluster uses a layered storage architecture to match workload requirements with the right storage backend. Distributed block storage, node-local volumes, and NAS-backed NFS mounts each serve a distinct purpose.</p>"},{"location":"storage/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    Apps[Applications] --&gt;|PersistentVolumeClaim| SC{StorageClass}\n\n    SC --&gt;|ceph-block| RC[(Rook Ceph\\n3x 512GB NVMe\\nReplicated Block)]\n    SC --&gt;|openebs-hostpath| OE[(OpenEBS\\nLocal PV\\nNode-Local Disk)]\n    SC --&gt;|nfs| SYN[(Synology NAS\\n4-Bay 8TB\\nNFS Shares)]\n\n    RC --&gt;|CephBlockPool| OSD1[worker-01\\nNVMe OSD]\n    RC --&gt;|CephBlockPool| OSD2[worker-02\\nNVMe OSD]\n    RC --&gt;|CephBlockPool| OSD3[worker-03\\nNVMe OSD]\n\n    subgraph Backup\n        VS[VolSync] --&gt;|Replicate PVCs| SYN\n        SNAP[Snapshot Controller] --&gt;|CSI Snapshots| RC\n    end</code></pre>"},{"location":"storage/#storage-classes","title":"Storage Classes","text":"StorageClass Provider Replicated Use Case <code>ceph-block</code> Rook Ceph Yes (3x) General-purpose workloads requiring high availability <code>openebs-hostpath</code> OpenEBS No Databases and workloads needing low-latency local disk NFS (manual) Synology NAS RAID Bulk media storage, shared datasets, backup targets"},{"location":"storage/#when-to-use-each","title":"When to Use Each","text":"<p>Choosing a StorageClass</p> <ul> <li> <p>Rook Ceph -- Default choice for most workloads. Data is replicated across three NVMe drives on separate nodes, surviving single-node failures. Use for application databases, config volumes, and anything that needs to survive rescheduling.</p> </li> <li> <p>OpenEBS -- Best for workloads that manage their own replication (e.g., PostgreSQL with CloudNativePG, etcd) or need the lowest possible latency. Data lives on a single node and is not replicated by the storage layer.</p> </li> <li> <p>Synology NFS -- Ideal for large media libraries, bulk file storage, and backup destinations. Mounted via NFS from the 4-bay Synology NAS with 8 TB of usable storage.</p> </li> </ul>"},{"location":"storage/#components","title":"Components","text":"Component Namespace Purpose Rook Ceph <code>rook-ceph</code> Distributed block storage on NVMe drives OpenEBS <code>openebs</code> Local PV provisioner for node-local storage VolSync <code>system</code> PVC backup and replication Snapshot Controller <code>system</code> CSI volume snapshots"},{"location":"storage/#sections","title":"Sections","text":"Page Description Rook Ceph Distributed Ceph cluster on NVMe -- operator, cluster config, storage classes OpenEBS Local PV provisioner for node-local volumes Backup &amp; Restore VolSync replication, CSI snapshots, Synology backup targets, restore procedures"},{"location":"storage/backup-restore/","title":"Backup &amp; Restore","text":"<p>The cluster uses a combination of VolSync for PVC replication, the CSI Snapshot Controller for point-in-time snapshots, and the Synology NAS as a backup target. Together, these provide data protection across multiple failure scenarios.</p>"},{"location":"storage/backup-restore/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    subgraph Cluster\n        PVC[Application PVC]\n        SNAP[CSI Snapshot]\n        VS[VolSync\\nReplicationSource]\n    end\n\n    subgraph Backup Targets\n        SYN[(Synology NAS\\nNFS / S3)]\n    end\n\n    PVC --&gt;|VolumeSnapshot| SNAP\n    PVC --&gt;|Restic / Rclone| VS\n    VS --&gt;|Backup| SYN\n    SNAP --&gt;|Restore| PVC\n    SYN --&gt;|Restore| PVC</code></pre>"},{"location":"storage/backup-restore/#volsync","title":"VolSync","text":"<p>VolSync is a Kubernetes operator that replicates persistent volume data using Restic, Rclone, or Rsync. It runs in the <code>system</code> namespace and provides scheduled backups of PVCs to external storage.</p>"},{"location":"storage/backup-restore/#deployment","title":"Deployment","text":"<p>VolSync is deployed via Helm (v0.14.0) in the <code>system</code> namespace:</p> kustomization.yaml<pre><code>helmCharts:\n  - name: volsync\n    repo: https://backube.github.io/helm-charts/\n    version: 0.14.0\n    releaseName: volsync\n    includeCRDs: true\n    namespace: system\n</code></pre> values.yaml<pre><code>manageCRDs: true\nmetrics:\n  disableAuth: true\n</code></pre>"},{"location":"storage/backup-restore/#how-it-works","title":"How It Works","text":"<p>VolSync uses two custom resources:</p> Resource Purpose <code>ReplicationSource</code> Defines what to back up, the schedule, and the destination <code>ReplicationDestination</code> Defines where to restore from and how to recreate the PVC <p>A typical <code>ReplicationSource</code> for backing up an application PVC:</p> <pre><code>apiVersion: volsync.backube/v1alpha1\nkind: ReplicationSource\nmetadata:\n  name: my-app-backup\n  namespace: my-app\nspec:\n  sourcePVC: my-app-data\n  trigger:\n    schedule: \"0 */6 * * *\"  # Every 6 hours\n  restic:\n    pruneIntervalDays: 7\n    repository: my-app-restic-secret\n    retain:\n      daily: 7\n      weekly: 4\n      monthly: 6\n    storageClassName: ceph-block\n    copyMethod: Snapshot\n</code></pre>"},{"location":"storage/backup-restore/#monitoring","title":"Monitoring","text":"<p>VolSync includes Prometheus alerting rules for backup health:</p> prometheusrule.yaml<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: volsync\nspec:\n  groups:\n    - name: volsync.rules\n      rules:\n        - alert: VolSyncComponentAbsent\n          expr: |\n            absent(up{job=\"volsync-metrics\"})\n          for: 15m\n          labels:\n            severity: critical\n        - alert: VolSyncVolumeOutOfSync\n          expr: |\n            volsync_volume_out_of_sync == 1\n          for: 15m\n          labels:\n            severity: critical\n</code></pre> <p>Alert on out-of-sync volumes</p> <p>The <code>VolSyncVolumeOutOfSync</code> alert fires when a volume has not been successfully replicated within its expected schedule. Investigate immediately -- this could indicate a failed backup job, connectivity issues to the backup target, or storage capacity problems.</p>"},{"location":"storage/backup-restore/#snapshot-controller","title":"Snapshot Controller","text":"<p>The CSI Snapshot Controller enables point-in-time <code>VolumeSnapshot</code> resources for CSI-backed PVCs. It runs in the <code>system</code> namespace alongside its webhook.</p>"},{"location":"storage/backup-restore/#deployment_1","title":"Deployment","text":"kustomization.yaml<pre><code>helmCharts:\n  - name: snapshot-controller\n    repo: https://piraeus.io/helm-charts/\n    version: 5.0.2\n    releaseName: snapshot-controller\n    includeCRDs: true\n    namespace: system\n</code></pre> values.yaml<pre><code>controller:\n  serviceMonitor:\n    create: true\n</code></pre>"},{"location":"storage/backup-restore/#webhook-pki","title":"Webhook PKI","text":"<p>The snapshot controller webhook uses self-signed certificates managed by cert-manager:</p> pki.yaml<pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: snapshot-controller-webhook-selfsign\nspec:\n  selfSigned: {}\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: snapshot-controller-webhook-ca\nspec:\n  secretName: snapshot-controller-webhook-ca\n  duration: 43800h # 5 years\n  issuerRef:\n    name: snapshot-controller-webhook-selfsign\n    kind: Issuer\n  commonName: \"ca.k8s-ycl.cert-manager\"\n  isCA: true\n---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: snapshot-controller-webhook-ca\nspec:\n  ca:\n    secretName: snapshot-controller-webhook-ca\n</code></pre>"},{"location":"storage/backup-restore/#usage","title":"Usage","text":"<p>Create a point-in-time snapshot of a PVC:</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: my-app-data-snapshot\n  namespace: my-app\nspec:\n  volumeSnapshotClassName: csi-ceph-blockpool  # or appropriate class\n  source:\n    persistentVolumeClaimName: my-app-data\n</code></pre> <p>Restore from a snapshot by referencing it as a PVC data source:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-app-data-restored\n  namespace: my-app\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: ceph-block\n  resources:\n    requests:\n      storage: 10Gi\n  dataSource:\n    name: my-app-data-snapshot\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n</code></pre>"},{"location":"storage/backup-restore/#synology-nas","title":"Synology NAS","text":"<p>The 4-bay Synology NAS (8 TB total) serves as the primary off-cluster backup target and bulk storage endpoint.</p>"},{"location":"storage/backup-restore/#nfs-shares","title":"NFS Shares","text":"<p>The Synology exports NFS shares used for:</p> <ul> <li>Media storage -- large media libraries mounted directly by applications (Jellyfin, *arr stack)</li> <li>Backup destinations -- VolSync Restic repositories for PVC backups</li> <li>Bulk data -- datasets too large for the Ceph cluster</li> </ul> <p>NFS volumes are mounted via standard Kubernetes PVs:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: media-nfs\nspec:\n  capacity:\n    storage: 1Ti\n  accessModes:\n    - ReadWriteMany\n  nfs:\n    server: 192.168.0.x  # Synology IP\n    path: /volume1/media\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: media-nfs\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: \"\"\n  volumeName: media-nfs\n  resources:\n    requests:\n      storage: 1Ti\n</code></pre>"},{"location":"storage/backup-restore/#restore-procedures","title":"Restore Procedures","text":""},{"location":"storage/backup-restore/#restore-from-volsync-backup","title":"Restore from VolSync Backup","text":"<ol> <li> <p>Create a <code>ReplicationDestination</code> pointing to the Restic repository:</p> <pre><code>apiVersion: volsync.backube/v1alpha1\nkind: ReplicationDestination\nmetadata:\n  name: my-app-restore\n  namespace: my-app\nspec:\n  trigger:\n    manual: restore-once\n  restic:\n    repository: my-app-restic-secret\n    destinationPVC: my-app-data\n    storageClassName: ceph-block\n    accessModes:\n      - ReadWriteOnce\n    capacity: 10Gi\n    copyMethod: Direct\n</code></pre> </li> <li> <p>Scale down the application to release the PVC (if it already exists):</p> <pre><code>kubectl -n my-app scale deployment my-app --replicas=0\n</code></pre> </li> <li> <p>Delete the existing PVC (if replacing):</p> <pre><code>kubectl -n my-app delete pvc my-app-data\n</code></pre> </li> <li> <p>Apply the <code>ReplicationDestination</code> to trigger the restore:</p> <pre><code>kubectl apply -f replication-destination.yaml\n</code></pre> </li> <li> <p>Wait for completion, then scale the application back up:</p> <pre><code>kubectl -n my-app get replicationdestination my-app-restore -w\nkubectl -n my-app scale deployment my-app --replicas=1\n</code></pre> </li> </ol>"},{"location":"storage/backup-restore/#restore-from-csi-snapshot","title":"Restore from CSI Snapshot","text":"<ol> <li> <p>List available snapshots:</p> <pre><code>kubectl -n my-app get volumesnapshots\n</code></pre> </li> <li> <p>Create a new PVC from the snapshot (see the Usage section above)</p> </li> <li> <p>Update the application to reference the restored PVC name, or delete the old PVC and rename the restored one.</p> </li> </ol> <p>Test restores regularly</p> <p>Schedule periodic restore tests to verify that backups are valid and the restore process works as expected. A backup that has never been tested is not a backup.</p>"},{"location":"storage/openebs/","title":"OpenEBS","text":"<p>OpenEBS provides a local PV (Persistent Volume) provisioner for the cluster. It creates hostpath-backed persistent volumes that store data directly on the node's local filesystem -- no replication, no network overhead.</p>"},{"location":"storage/openebs/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    PVC[PersistentVolumeClaim\\nstorageClassName: openebs-hostpath] --&gt; SC[StorageClass\\nopenebs-hostpath]\n    SC --&gt; PROV[OpenEBS\\nLocal PV Provisioner]\n    PROV --&gt; HP[Host Path\\n/var/mnt/extra/openebs/local]</code></pre>"},{"location":"storage/openebs/#when-to-use-openebs","title":"When to Use OpenEBS","text":"<p>OpenEBS local PVs are best suited for workloads that:</p> <ul> <li>Manage their own replication -- databases like PostgreSQL (via CloudNativePG) or Redis that handle data replication at the application level</li> <li>Need low-latency local I/O -- workloads where network storage overhead is unacceptable</li> <li>Are tolerant of node affinity -- pods using local PVs are bound to the node where the volume was created</li> </ul> <p>No replication</p> <p>Data stored on OpenEBS local PVs exists only on a single node. If that node fails, the data is unavailable until the node recovers. Always ensure application-level replication or backups for critical data.</p>"},{"location":"storage/openebs/#configuration","title":"Configuration","text":""},{"location":"storage/openebs/#namespace","title":"Namespace","text":"<p>The OpenEBS namespace requires privileged Pod Security Standards since the provisioner needs host filesystem access:</p> namespace.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: openebs\n  labels:\n    pod-security.kubernetes.io/enforce: privileged\n    pod-security.kubernetes.io/audit: privileged\n    pod-security.kubernetes.io/warn: privileged\n</code></pre>"},{"location":"storage/openebs/#helm-chart","title":"Helm Chart","text":"<p>OpenEBS is deployed via the official Helm chart (v4.4.0) with only the local PV provisioner enabled:</p> values.yaml<pre><code>localpv-provisioner:\n  rbac:\n    create: true\n  localpv:\n    image:\n      registry: quay.io/\n      repository: openebs/provisioner-localpv\n    basePath: &amp;hostPath /var/mnt/extra/openebs/local\n  hostpathClass:\n    enabled: true\n    name: openebs-hostpath\n    isDefaultClass: false\n    basePath: *hostPath\n\n# All other engines are disabled\nopenebs-crds:\n  csi:\n    volumeSnapshots:\n      enabled: false\n      keep: false\nzfs-localpv:\n  enabled: false\nlvm-localpv:\n  enabled: false\nmayastor:\n  enabled: false\nengines:\n  local:\n    lvm:\n      enabled: false\n    zfs:\n      enabled: false\n  replicated:\n    mayastor:\n      enabled: false\n</code></pre> <p>Key settings:</p> Setting Value Purpose <code>basePath</code> <code>/var/mnt/extra/openebs/local</code> Host directory where PVs are stored <code>hostpathClass.name</code> <code>openebs-hostpath</code> StorageClass name for PVC requests <code>hostpathClass.isDefaultClass</code> <code>false</code> Not the cluster default (Ceph is preferred) <code>mayastor.enabled</code> <code>false</code> Replicated engine not needed (Rook Ceph fills this role) <code>zfs-localpv.enabled</code> <code>false</code> No ZFS volumes on these nodes <code>lvm-localpv.enabled</code> <code>false</code> No LVM volumes on these nodes <p>Minimal footprint</p> <p>Only the <code>localpv-provisioner</code> component is active. All other OpenEBS engines (Mayastor, ZFS, LVM) are disabled to minimize resource usage and complexity.</p>"},{"location":"storage/openebs/#usage","title":"Usage","text":"<p>To request a local PV, set the <code>storageClassName</code> to <code>openebs-hostpath</code>:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-database-data\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: openebs-hostpath\n  resources:\n    requests:\n      storage: 20Gi\n</code></pre> <p>Node affinity</p> <p>Once a pod is scheduled and a local PV is provisioned, the volume is tied to that specific node. The pod will always be scheduled on the same node as its volume. If the node is unavailable, the pod cannot start elsewhere unless the PV is manually recreated.</p>"},{"location":"storage/openebs/#comparison-with-rook-ceph","title":"Comparison with Rook Ceph","text":"Feature OpenEBS (Local PV) Rook Ceph Replication None 3-way across nodes Latency Lowest (local disk) Higher (network + replication) Node failure tolerance No (data on single node) Yes (data survives 1 node loss) Pod scheduling Pinned to volume node Can reschedule freely Best for Databases with app-level replication General workloads StorageClass <code>openebs-hostpath</code> <code>ceph-block</code>"},{"location":"storage/rook-ceph/","title":"Rook Ceph","text":"<p>Rook Ceph provides distributed block storage for the cluster. Three 512 GB NVMe drives on the Acemagician AM06 nodes form a Ceph cluster managed by the Rook operator, delivering replicated, high-availability persistent volumes.</p>"},{"location":"storage/rook-ceph/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    subgraph Rook Operator\n        OP[rook-ceph-operator\\nManages Ceph lifecycle]\n    end\n\n    subgraph Ceph Cluster\n        MON1[MON\\nworker-01]\n        MON2[MON\\nworker-02]\n        MON3[MON\\nworker-03]\n        MGR[MGR\\nCluster manager]\n        OSD1[OSD\\nworker-01\\n512GB NVMe]\n        OSD2[OSD\\nworker-02\\n512GB NVMe]\n        OSD3[OSD\\nworker-03\\n512GB NVMe]\n    end\n\n    subgraph Resources\n        BP[CephBlockPool\\nReplicated x3]\n        SC[StorageClass\\nceph-block]\n        DASH[Dashboard\\nrook.example.com]\n    end\n\n    OP --&gt; MON1 &amp; MON2 &amp; MON3\n    OP --&gt; MGR\n    OP --&gt; OSD1 &amp; OSD2 &amp; OSD3\n    BP --&gt; OSD1 &amp; OSD2 &amp; OSD3\n    SC --&gt; BP\n    MGR --&gt; DASH</code></pre>"},{"location":"storage/rook-ceph/#repository-layout","title":"Repository Layout","text":"<p>The Rook Ceph deployment is split into three kustomization directories:</p> <pre><code>pitower/kubernetes/apps/rook-ceph/\n\u251c\u2500\u2500 operator/           # Rook operator Helm chart + CRDs\n\u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u251c\u2500\u2500 namespace.yaml\n\u2502   \u2514\u2500\u2500 values.yaml\n\u251c\u2500\u2500 cluster/            # CephCluster CR, dashboard HTTPRoute\n\u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u251c\u2500\u2500 values.yaml\n\u2502   \u2514\u2500\u2500 httproute.yaml\n\u2514\u2500\u2500 add-ons/            # Grafana dashboards for Ceph monitoring\n    \u251c\u2500\u2500 kustomization.yaml\n    \u2514\u2500\u2500 dashboard/\n        \u251c\u2500\u2500 kustomization.yaml\n        \u251c\u2500\u2500 ceph-cluster-dashboard.json\n        \u251c\u2500\u2500 ceph-osd-dashboard.json\n        \u2514\u2500\u2500 ceph-pools-dashboard.json\n</code></pre> <p>Separation of concerns</p> <p>The operator and cluster are deployed as separate ArgoCD applications. This allows the operator to be upgraded independently of the cluster, and prevents accidental cluster disruption during operator updates.</p>"},{"location":"storage/rook-ceph/#operator","title":"Operator","text":"<p>The Rook operator is deployed via the <code>rook-ceph</code> Helm chart (v1.17.9) into the <code>rook-ceph</code> namespace:</p> operator/values.yaml<pre><code>crds:\n  enabled: true\ncsi:\n  enableCephfsDriver: false\nmonitoring:\n  enabled: false\nresources:\n  requests:\n    memory: 128Mi\n    cpu: 100m\n  limits: {}\n</code></pre> <p>Key decisions:</p> <ul> <li>CephFS driver disabled -- the cluster uses block storage only (<code>cephFileSystems: []</code>)</li> <li>CRDs managed by the chart -- <code>crds.enabled: true</code> ensures CRDs are installed and upgraded with the operator</li> </ul>"},{"location":"storage/rook-ceph/#cluster-configuration","title":"Cluster Configuration","text":"<p>The <code>rook-ceph-cluster</code> Helm chart deploys the <code>CephCluster</code> custom resource:</p>"},{"location":"storage/rook-ceph/#monitors-and-managers","title":"Monitors and Managers","text":"Component Count Purpose MON 3 Maintain cluster map consensus (one per node) MGR 1 Cluster management, dashboard, metrics OSD 3 One per NVMe drive, stores actual data"},{"location":"storage/rook-ceph/#storage-nodes","title":"Storage Nodes","text":"<p>Each OSD is pinned to a specific NVMe device by disk ID to prevent accidental data loss:</p> cluster/values.yaml (storage section)<pre><code>cephClusterSpec:\n  storage:\n    useAllNodes: false\n    useAllDevices: false\n    config:\n      osdsPerDevice: \"1\"\n    nodes:\n      - name: \"worker-01\"\n        devices:\n          - name: \"/dev/disk/by-id/nvme-AirDisk_512GB_SSD_NFQ0044006866P70GX\"\n      - name: \"worker-02\"\n        devices:\n          - name: \"/dev/disk/by-id/nvme-AirDisk_512GB_SSD_NFQ0044007344P70GX\"\n      - name: \"worker-03\"\n        devices:\n          - name: \"/dev/disk/by-id/nvme-AirDisk_512GB_SSD_NFQ0044010702P70GX\"\n</code></pre> <p>Device selection</p> <p><code>useAllNodes</code> and <code>useAllDevices</code> are both set to <code>false</code>. Each node and device is explicitly listed to prevent Ceph from consuming unintended disks. Devices are referenced by <code>/dev/disk/by-id/</code> paths for stability across reboots.</p>"},{"location":"storage/rook-ceph/#network","title":"Network","text":"<pre><code>cephClusterSpec:\n  network:\n    provider: host\n</code></pre> <p>Host networking is used for Ceph daemons to maximize throughput and minimize latency between OSDs and monitors.</p>"},{"location":"storage/rook-ceph/#resource-limits","title":"Resource Limits","text":"Daemon CPU Request Memory Request Memory Limit MGR 125m 512Mi 2Gi MON 49m 512Mi 1Gi OSD 442m 1Gi 6Gi MGR Sidecar 49m 128Mi 256Mi Crash Collector 15m 64Mi 64Mi Log Collector 100m 100Mi 1Gi"},{"location":"storage/rook-ceph/#storage-resources","title":"Storage Resources","text":""},{"location":"storage/rook-ceph/#cephblockpool","title":"CephBlockPool","text":"<p>The default block pool provides three-way replication across the three OSD nodes. This is configured through the <code>rook-ceph-cluster</code> Helm chart's defaults.</p> <p>Current configuration</p> <ul> <li><code>cephFileSystems: []</code> -- No CephFS filesystems are deployed</li> <li><code>cephObjectStores: []</code> -- No S3-compatible object stores are deployed</li> <li><code>cephBlockPoolsVolumeSnapshotClass.enabled: false</code> -- Volume snapshot class for block pools is not yet enabled</li> </ul>"},{"location":"storage/rook-ceph/#storageclass","title":"StorageClass","text":"<p>The Helm chart creates a <code>ceph-block</code> StorageClass that provisions RBD (RADOS Block Device) volumes from the block pool. Applications request storage through PVCs referencing this class:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-app-data\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: ceph-block\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"storage/rook-ceph/#dashboard","title":"Dashboard","text":"<p>The Ceph dashboard is enabled and accessible via the internal gateway:</p> cluster/httproute.yaml<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: rook-ceph-dashboard\n  namespace: rook-ceph\nspec:\n  hostnames:\n    - rook.example.com\n  parentRefs:\n    - name: envoy-internal\n      namespace: networking\n      sectionName: https\n  rules:\n    - backendRefs:\n        - name: rook-ceph-mgr-dashboard\n          port: 7000\n</code></pre> <p>Access the dashboard at <code>https://rook.example.com</code> from the internal network or via Tailscale VPN.</p> <p>Dashboard credentials</p> <p>The dashboard admin password is stored in the <code>rook-ceph-dashboard-password</code> secret in the <code>rook-ceph</code> namespace:</p> <pre><code>kubectl -n rook-ceph get secret rook-ceph-dashboard-password \\\n  -o jsonpath='{.data.password}' | base64 -d\n</code></pre>"},{"location":"storage/rook-ceph/#monitoring","title":"Monitoring","text":""},{"location":"storage/rook-ceph/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Three Grafana dashboards are deployed as ConfigMaps with the <code>grafana_dashboard: \"true\"</code> label, automatically discovered by the Grafana sidecar:</p> Dashboard Grafana ID Purpose Ceph Cluster 2842 Overall cluster health, IOPS, throughput Ceph OSD 5336 Per-OSD performance and utilization Ceph Pools 5342 Pool-level statistics and capacity"},{"location":"storage/rook-ceph/#toolbox","title":"Toolbox","text":"<p>The Rook toolbox pod is enabled (<code>toolbox.enabled: true</code>) for interactive Ceph CLI troubleshooting:</p> <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash\n\n# Inside the toolbox\nceph status\nceph osd status\nceph df\nrados df\n</code></pre>"},{"location":"storage/rook-ceph/#health-checks","title":"Health Checks","text":"<p>Common commands to verify Ceph cluster health:</p> Quick StatusOSD TreePool UsagePG Status <pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph status\n</code></pre> <pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd tree\n</code></pre> <pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph df\n</code></pre> <pre><code>kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph pg stat\n</code></pre> <p>Data safety</p> <p>The <code>cleanupPolicy.confirmation</code> field is left empty (<code>\"\"</code>). Setting it to <code>\"yes-really-destroy-data\"</code> would allow the cleanup job to wipe all Ceph data when the CephCluster resource is deleted. Never change this unless you are intentionally decommissioning the cluster.</p>"}]}